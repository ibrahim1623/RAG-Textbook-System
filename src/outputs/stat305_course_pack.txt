Statistical Inference: A Primer on
Likelihood and Bayesian Methods

5
−354. 305
STAT
−354
Introduction to Statistical
Inference

60

5

−353.

2

s) )
m
k
n(
2 (millio
50
σ
40
30

−352.5

Course Notes Prepared by
William J. Welch
Department of Statistics
University of British Columbia
3182 Earth Sciences Building
−352
2207 Main Mall
Vancouver BC, Canada−V6T
353 1Z4
−355

66

20

Version: August 14, 2019

56

58
β (1

64

62
18 s))
0
6
0
1
57 ×
8
.
0
(3

© Copyright William J. Welch 2009–2019

All rights reserved.

Contents
1 Probability Tools
1.1

1.2

1.3

1.4

1.5

1-1

Discrete and Continuous Random Variables . . . . . . . . . . . . . .

1-1

1.1.1

Probability mass function and probability density function . .

1-2

1.1.2

Cumulative distribution function . . . . . . . . . . . . . . . .

1-3

Mean, Median, and Mode . . . . . . . . . . . . . . . . . . . . . . . .

1-4

1.2.1

Mean or expectation . . . . . . . . . . . . . . . . . . . . . . .

1-4

1.2.2

Median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-6

1.2.3

Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-6

Variance

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-7

1.3.1

Computation . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-7

1.3.2

Standard deviation . . . . . . . . . . . . . . . . . . . . . . . .

1-9

1.3.3

Chebyshev’s inequality . . . . . . . . . . . . . . . . . . . . . .

1-9

Commonly Used Discrete Distributions

. . . . . . . . . . . . . . . .

1-9

1.4.1

Bernoulli distribution . . . . . . . . . . . . . . . . . . . . . . 1-11

1.4.2

Binomial distribution . . . . . . . . . . . . . . . . . . . . . . . 1-11

1.4.3

Geometric distribution . . . . . . . . . . . . . . . . . . . . . . 1-11

1.4.4

Negative-binomial distribution

1.4.5

Poisson distribution . . . . . . . . . . . . . . . . . . . . . . . 1-12

. . . . . . . . . . . . . . . . . 1-12

Commonly Used Continuous Distributions . . . . . . . . . . . . . . . 1-12
1.5.1

Beta distribution . . . . . . . . . . . . . . . . . . . . . . . . . 1-12

1.5.2

Exponential distribution . . . . . . . . . . . . . . . . . . . . . 1-14

1.5.3

Gamma distribution . . . . . . . . . . . . . . . . . . . . . . . 1-14

1.5.4

Laplace distribution . . . . . . . . . . . . . . . . . . . . . . . 1-16

1.5.5

Normal distribution . . . . . . . . . . . . . . . . . . . . . . . 1-16

1.5.6

Log-normal distribution . . . . . . . . . . . . . . . . . . . . . 1-16

i

ii

CONTENTS

1.6

1.7

1.8

1.9

1.5.7

χ2 , F , and t distributions . . . . . . . . . . . . . . . . . . . . 1-17

1.5.8

Uniform distribution . . . . . . . . . . . . . . . . . . . . . . . 1-17

Function of a Random Variable . . . . . . . . . . . . . . . . . . . . . 1-17
1.6.1

PDF of a function of a continuous random variable . . . . . . 1-17

1.6.2

Expectation of a function of a random variable . . . . . . . . 1-18

Several Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-20
1.7.1

Joint and marginal distributions . . . . . . . . . . . . . . . . 1-20

1.7.2

Conditional distributions

1.7.3

Statistical independence . . . . . . . . . . . . . . . . . . . . . 1-23

1.7.4

Random sample . . . . . . . . . . . . . . . . . . . . . . . . . . 1-24

1.7.5

Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-25

1.7.6

Expectation of a linear combination of random variables . . . 1-26

1.7.7

Variance of a linear combination of random variables . . . . . 1-28

1.7.8

Covariance between linear functions or combinations of random
variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-29

1.7.9

Bivariate normal distribution . . . . . . . . . . . . . . . . . . 1-30

Moment Generating Functions

. . . . . . . . . . . . . . . . . . . . 1-22

. . . . . . . . . . . . . . . . . . . . . 1-31

1.8.1

Uses of moment generating functions

. . . . . . . . . . . . . 1-31

1.8.2

Definition of the moment generating function . . . . . . . . . 1-32

1.8.3

Finding moments from the MGF . . . . . . . . . . . . . . . . 1-35

1.8.4

MGF of a linear function or a sum . . . . . . . . . . . . . . . 1-38

1.8.5

The MGF identifies a distribution . . . . . . . . . . . . . . . . 1-38

Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-40

1.10 Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-43
1.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-45
2 The Normal Distribution in Statistics

2-1

2.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2-1

2.2

Some Properties of the Normal Distribution . . . . . . . . . . . . . .

2-2

2.3

Distributions Derived From the Normal

. . . . . . . . . . . . . . . .

2-2

2.4

2

2.3.1

The χ distribution

. . . . . . . . . . . . . . . . . . . . . . .

2-2

2.3.2

The t distribution . . . . . . . . . . . . . . . . . . . . . . . .

2-4

2.3.3

The F distribution . . . . . . . . . . . . . . . . . . . . . . . .

2-7

Estimating the Parameters of the Normal . . . . . . . . . . . . . . .

2-8

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

CONTENTS

2.5

2.6

iii

2.4.1

Distribution of the sample mean (known variance) . . . . . .

2-9

2.4.2

Distribution of the sample variance

2.4.3

Distribution of the standardized sample mean (unknown variance) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-11

. . . . . . . . . . . . . . 2-10

Limiting Normal Distributions . . . . . . . . . . . . . . . . . . . . . . 2-14
2.5.1

Convergence in distribution . . . . . . . . . . . . . . . . . . . 2-14

2.5.2

Limiting distributions and large-sample approximations in
statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-16

2.5.3

Central limit theorem . . . . . . . . . . . . . . . . . . . . . . 2-18

Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-19
2.6.1

Sample mean, standard deviation, and variance . . . . . . . . 2-19

2.6.2

Quantiles of the t distribution . . . . . . . . . . . . . . . . . . 2-19

2.6.3

Limiting normal distributions . . . . . . . . . . . . . . . . . . 2-20

2.7

Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-20

2.8

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2-22

2.9

Appendix: Proof of Lemma 2.2 . . . . . . . . . . . . . . . . . . . . . 2-31

2.10 Appendix: Proof of the Central Limit Theorem . . . . . . . . . . . . 2-32
3 Statistical Estimation

3-1

3.1

Statistical Models: The Role of Probability . . . . . . . . . . . . . . .

3-1

3.2

The Frequentist Philosophy . . . . . . . . . . . . . . . . . . . . . . .

3-2

3.3

Properties of an Estimator

. . . . . . . . . . . . . . . . . . . . . . .

3-7

3.3.1

Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3-7

3.3.2

Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3-7

3.3.3

Mean squared error . . . . . . . . . . . . . . . . . . . . . . . .

3-7

3.3.4

Practical perspective . . . . . . . . . . . . . . . . . . . . . . .

3-9

3.3.5

Consistency

3.3.6

Relative Error . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-12

. . . . . . . . . . . . . . . . . . . . . . . . . . . 3-10

3.4

Comparing Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . 3-14

3.5

Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-14

3.6

Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-15

3.7

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-15

4 Maximum Likelihood Estimation
4.1

4-1

Maximum Likelihood Estimation: Basic Ideas . . . . . . . . . . . . .

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

4-1

2019.8.14

iv

CONTENTS
4.1.1

What is a likelihood function and why maximize it? . . . . . .

4-1

4.1.2

Maximum likelihood estimates in general . . . . . . . . . . . .

4-8

4.2

Properties of Maximum Likelihood Estimators

4.3

Consistency of the ML Estimator . . . . . . . . . . . . . . . . . . . . 4-11

4.4

Regularity conditions

4.5

Large-Sample Variance of the ML Estimator

4.6

. . . . . . . . . . . .

4-9

. . . . . . . . . . . . . . . . . . . . . . . . . . 4-13
. . . . . . . . . . . . . 4-14

4.5.1

Observed information . . . . . . . . . . . . . . . . . . . . . . 4-15

4.5.2

Fisher information . . . . . . . . . . . . . . . . . . . . . . . . 4-20

4.5.3

Observed versus Fisher information

4.5.4

Large-sample normality of the maximum likelihood estimator

Confidence Intervals From the ML Estimator

. . . . . . . . . . . . . . 4-24
4-25

. . . . . . . . . . . . . 4-28

4.6.1

Large-sample approximations . . . . . . . . . . . . . . . . . . 4-28

4.6.2

Parameter transformation for better approximation . . . . . . 4-32

4.7

Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 4-34

4.8

Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4-35

4.9

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4-36

4.A Appendix: Equivalence of Observed and Fisher Information . . . . . 4-46
5 Maximum Likelihood Estimation: Several Parameters

5-1

5.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5-1

5.2

Maximum likelihood estimates

. . . . . . . . . . . . . . . . . . . . .

5-1

5.3

Large-sample unbiasedness of ML estimators . . . . . . . . . . . . . .

5-3

5.4

Large-Sample Variances and Covariances of ML Estimators . . . . . .

5-4

5.5

Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . .

5-6

5.6

Censored Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5-8

5.7

Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5-11

5.8

Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 5-12

5.9

Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5-13

5.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5-14
6 Bayesian Estimation

6-1

6.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6-1

6.2

Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6-2

6.3

Bayesian Posterior Distribution of a Parameter . . . . . . . . . . . .

6-6

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

CONTENTS

v

6.4

Bayesian Credible Intervals . . . . . . . . . . . . . . . . . . . . . . . 6-14

6.5

Normal Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 6-16

6.6

Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6-24

6.7

Bayesian Predictive Distributions . . . . . . . . . . . . . . . . . . . . 6-25

6.8

Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6-27

6.9

Bayesian Versus Frequentist Paradigms . . . . . . . . . . . . . . . . . 6-30

6.10 Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 6-31
6.10.1 Monte Carlo predictive distribution . . . . . . . . . . . . . . . 6-31
6.10.2 Gibbs sampling from the posterior distribution . . . . . . . . 6-31
6.11 Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6-32
6.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6-33
7 Hypothesis Testing

7-1

7.1

Introduction

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7-1

7.2

What is a Hypothesis Test? . . . . . . . . . . . . . . . . . . . . . . .

7-1

7.3

Formulation of a Hypothesis Test . . . . . . . . . . . . . . . . . . . .

7-4

7.4

Tests Based on the Likelihood Ratio . . . . . . . . . . . . . . . . . . 7-10
7.4.1

Neyman-Pearson Lemma . . . . . . . . . . . . . . . . . . . . . 7-10

7.4.2

Composite hypotheses . . . . . . . . . . . . . . . . . . . . . . 7-17

7.5

Generalized likelihood ratio tests . . . . . . . . . . . . . . . . . . . . 7-20

7.6

Normal Distribution: Testing µ With σ 2 Unknown . . . . . . . . . . 7-25

7.7

p-values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-30

7.8

Practical Significance Versus Statistical Significance

7.9

Connection With Confidence Intervals . . . . . . . . . . . . . . . . . 7-34

. . . . . . . . . 7-33

7.10 Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-37
7.11 Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-38
7.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-40
7.13 Appendix: Sketch proof of Wilks’ Theorem

. . . . . . . . . . . . . . 7-52
8-1

8 Analysis of Categorical Data
8.1

The Multinomial Distribution . . . . . . . . . . . . . . . . . . . . . .

8-1

8.2

Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . . .

8-3

8.3

Hypothesis Tests for the Multinomial . . . . . . . . . . . . . . . . . .

8-5

8.3.1

8-5

Generalized likelihood ratio tests . . . . . . . . . . . . . . . .

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

vi

CONTENTS
8.3.2

Pearson’s statistic . . . . . . . . . . . . . . . . . . . . . . . .

8-9

8.4

Goodness of Fit Tests . . . . . . . . . . . . . . . . . . . . . . . . . . 8-10

8.5

Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 8-18

8.6

Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8-19

8.7

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8-20

9 Comparative Studies

9-1

9.1

Independent Versus Paired Samples . . . . . . . . . . . . . . . . . . .

9-1

9.2

Two Independent Samples . . . . . . . . . . . . . . . . . . . . . . . .

9-2

9.2.1

Likelihood methods

. . . . . . . . . . . . . . . . . . . . . . .

9-2

9.2.2

Methods for the normal distribution . . . . . . . . . . . . . .

9-9

9.3

Several Independent Multinomial Samples . . . . . . . . . . . . . . . 9-12

9.4

Two-Way Contingency Tables . . . . . . . . . . . . . . . . . . . . . . 9-15

9.5

Paired Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-17

9.6

9.5.1

Paired data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-17

9.5.2

Model for difference data

9.5.3

Estimation and hypothesis testing . . . . . . . . . . . . . . . 9-19

9.5.4

Statistical advantages of paired data . . . . . . . . . . . . . . 9-22

. . . . . . . . . . . . . . . . . . . . 9-18

Getting It Done in R . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-24
9.6.1

Several multinomial samples or a contingency table . . . . . . 9-24

9.6.2

Paired data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-25

9.7

Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-25

9.8

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-27

10 Solutions

10-1

Bibliography

Bib-1

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

List of Tables
1.1

Probability mass function for the final-exam grade . . . . . . . . . . .

1-4

1.2

Binomial PMF and CDF for n = 3 trials and probability of success
π = 1/4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-6

1.3

Some commonly used discrete distributions . . . . . . . . . . . . . . . 1-10

1.4

Some commonly used continuous distributions . . . . . . . . . . . . . 1-13

1.5

HIV vaccine: two-way frequency table by treatment and HIV-infection
status . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-21

1.6

Joint probability function for the final-exam and quiz grades . . . . . 1-27

1.7

Probability mass function for the course grade . . . . . . . . . . . . . 1-28

1.8

PMF of a negative-binomial random variable with n = 2 and π = 0.1

1.9

R functions for the PMF or PDF of some common distributions . . . 1-41

3.1

Faults on data lines . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2

Sample size to estimate the binomial parameter π: nabs achieves
sd(π̃) ≤ 0.015 and nrel achieves sd(π̃)/π ≤ 0.03 . . . . . . . . . . . . . 3-13

3.3

R functions to return the PDF, CDF, quantile, or random numbers for
the normal distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 3-15

4.1

Exponential distribution: approximate variance of λ̃ from observed
information compared with the variance over repeated samples . . . . 4-18

4.2

Faults on data lines of length about 22 km . . . . . . . . . . . . . . . 4-37

4.3

Number of expression events for a sample of 298 cell cycles . . . . . . 4-38

5.1

Lung function: exact and ML confidence intervals for the normal mean 5-7

6.1

Conjugate priors for some distributions . . . . . . . . . . . . . . . . . 6-25

7.1

Definitions of Type I and Type II errors . . . . . . . . . . . . . . . .

7.2

Normal distribution: rejection regions for testing H0 : µ = µ0 . . . . . 7-26
vii

1-41

3-3

7-7

viii

LIST OF TABLES
7.3

Data summaries of a measure of depression for four groups of patients
in a smoking-cessation study . . . . . . . . . . . . . . . . . . . . . . . 7-48

8.1

Frequencies of XX, XY, and YY genotypes . . . . . . . . . . . . . . .

8-3

8.2

Wilks’ and Pearson’s statistics to test the 9:3:3:1 Mendelian ratio . .

8-7

8.3

Wilks’ and Pearson’s statistics to test the Hardy-Weinberg principle .

8-9

8.4

Faults on data lines of length about 90 km . . . . . . . . . . . . . . . 8-11

8.5

Faults on data lines of length about 90 km and Pearson’s goodness of
fit statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8-12

8.6

Observed and expected frequencies of differences in grapefruit solids . 8-17

8.7

Number of expression events for a sample of 298 cell cycles . . . . . . 8-22

9.1

Data summaries for two samples of data-transmission lines . . . . . .

9-3

9.2

Data summaries of smoking-cessation rates for two groups of patients

9-7

9.3

Data on average recall index for an advertisement . . . . . . . . . . . 9-11

9.4

Observed and expected frequencies in a study of sugar-intake and diabetes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-13

9.5

Observed frequencies in I categories for J independent samples . . . 9-14

9.6

Biological activities of 10 samples . . . . . . . . . . . . . . . . . . . . 9-21

9.7

Frequencies of people with and without diabetes in three independent
samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-32

9.8

Frequencies of not smoking versus smoking after one year in four independent samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-34

9.9

Frequency data on flower colour and shape . . . . . . . . . . . . . . . 9-35

9.10 Frequencies of breast cancer, all other cancers, or no cancer in two
independent samples . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-37
9.11 International roughness index (IRI) measurements . . . . . . . . . . . 9-40

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

List of Figures
1.1

PDF of the exponential distribution . . . . . . . . . . . . . . . . . . . 1-14

1.2

PDF of the gamma distribution . . . . . . . . . . . . . . . . . . . . . 1-15

1.3

PDFs of the Laplace and normal distributions . . . . . . . . . . . . . 1-16

1.4

PDF of the t distribution with 10 degrees of freedom . . . . . . . . . 1-43

2.1

Relationships between distributions derived from the normal . . . . .

2-3

2.2

PDF of the χ2 distribution . . . . . . . . . . . . . . . . . . . . . . . .

2-4

2.3

PDF of the t distribution

. . . . . . . . . . . . . . . . . . . . . . . .

2-5

2.4

χ23 PDF and t3 PDF as a mixture of normals . . . . . . . . . . . . . .

2-7

2.5

PDF of the F distribution . . . . . . . . . . . . . . . . . . . . . . . .

2-8

2.6

Quantiles of the t distribution . . . . . . . . . . . . . . . . . . . . . . 2-14

3.1

Histograms of the faults data and Poisson samples (µ = 2.41) . . . .

3-4

3.2

Histograms of the faults data and Poisson samples (µ = 5) . . . . . .

3-6

3.3

R code for Exercise 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . 3-17

4.1

Binomial PMF for various values of π . . . . . . . . . . . . . . . . . .

4-3

4.2

Likelihood plotted against the binomial parameter π . . . . . . . . .

4-4

4.3

Log likelihood plotted against the binomial parameter π . . . . . . .

4-5

4.4

Exponential distribution: likelihood and log likelihood functions . . .

4-7

4.5

Log likelihood plotted against the binomial parameter π . . . . . . . 4-10

4.6

Exponential distribution: ML estimate of λ versus sample size . . . . 4-12

4.7

Exponential distribution: estimated distribution of λ̃ . . . . . . . . . 4-18

4.8

Exponential distribution: distribution of λ̃ and normal approximation 4-27

4.9

Quantiles of the standard normal distribution . . . . . . . . . . . . . 4-29

5.1

Censored insurance claims: Histograms of amount paid . . . . . . . .

ix

5-9

x

LIST OF FIGURES
5.2

Censored insurance claims: contour plots of the log likelihood . . . . 5-11

6.1

Probability tree diagram for Example 6.1 . . . . . . . . . . . . . . . .

6-5

6.2

Beta priors, pΠ (π), for Π for various values of the shape parameters, a
and b . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6-9

6.3

Beta (4, 8) posterior distribution for Π . . . . . . . . . . . . . . . . . . 6-11

6.4

Bayesian analysis of the vaccine treatment in the HIV example . . . . 6-13

6.5

Faults on data lines: likelihood and posterior . . . . . . . . . . . . . . 6-15

6.6

Prior for faults on data lines . . . . . . . . . . . . . . . . . . . . . . . 6-16

6.7

Normal distribution: likelihood as a function of µ and τ

6.8

Normal prior for the mean of the normal distribution . . . . . . . . . 6-23

6.9

Gamma prior for the precision parameter of the normal distribution . 6-24

. . . . . . . 6-22

6.10 Faults on data lines: posterior . . . . . . . . . . . . . . . . . . . . . . 6-27
7.1

Coin tossing: properties of a hypothesis test . . . . . . . . . . . . . .

7-9

7.2

Testing the parameter λ of the exponential distribution: distribution
of the test statistic under null and alternative hypotheses . . . . . . . 7-14

7.3

Exponential distribution: power curve . . . . . . . . . . . . . . . . . 7-18

7.4

Histogram of differences in rainfall . . . . . . . . . . . . . . . . . . . 7-27

7.5

Rainfall example: Rejection regions testing µ against 1-sided and 2sided alternatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-27

7.6

Lung-function example: rejection region testing µ against a 1-sided
alternative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-29

7.7

Lung-function example: p-value for testing µ against a 1-sided alternative7-31

8.1

Data from a TaqMan assay . . . . . . . . . . . . . . . . . . . . . . .

8.2

Difference in grapefruit solids: histogram and normal PDF . . . . . . 8-14

8.3

Difference in grapefruit solids: histogram and normal PDF (5 bins) . 8-15

8.4

Difference in grapefruit solids: histogram and normal PDF (overlaid)

8-16

9.1

Faults on data lines for two samples . . . . . . . . . . . . . . . . . . .

9-3

9.2

Rainfall data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-20

9.3

Grapefruit data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9-23

8-2

10.1 Log-likelihood function for Exercise 4.2 . . . . . . . . . . . . . . . . . 10-13
10.2 R code for Exercise 4.12 . . . . . . . . . . . . . . . . . . . . . . . . . 10-16

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

LIST OF FIGURES

xi

List of Definitions
1.1

Expectation (mean) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-4

1.2

Median of a distribution . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-6

1.3

Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-7

1.4

Expectation of a function of random variable

1.5

Statistical independence . . . . . . . . . . . . . . . . . . . . . . . . . . . 1-24

1.6

Moments of a random variable . . . . . . . . . . . . . . . . . . . . . . . . 1-32

1.7

Moment generating function . . . . . . . . . . . . . . . . . . . . . . . . . 1-32

2.1

Convergence in distribution . . . . . . . . . . . . . . . . . . . . . . . . . 2-14

3.1

Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3-7

3.2

Mean squared error (MSE) . . . . . . . . . . . . . . . . . . . . . . . . . .

3-8

3.3

Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3-10

4.1

Observed information

4.2

Fisher information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4-21

7.1

p-value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7-30

. . . . . . . . . . . . . . . 1-19

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 4-15

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

xii

LIST OF FIGURES

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

LIST OF FIGURES

xiii

List of Lemmas and Theorems
Lemma 1.1

Bivariate normal: covariance of 0 implies independence . . . . . 1-30

Lemma 1.2

MGF of a linear function of a random variable . . . . . . . . . . 1-38

Lemma 1.3

MGF of a sum of independent random variables . . . . . . . . . 1-38

Lemma 2.1

χ2 distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2-4

Lemma 2.2

t distribution (Student, 1908) . . . . . . . . . . . . . . . . . . . .

2-5

Lemma 2.3

F distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2-8

Lemma 7.1

Neyman-Pearson Lemma . . . . . . . . . . . . . . . . . . . . . . 7-11

Theorem 1.1

Chebyshev’s inequality . . . . . . . . . . . . . . . . . . . . . .

Theorem 1.2

Law of total probability . . . . . . . . . . . . . . . . . . . . . . 1-23

Theorem 1.3

The MGF identifies a distribution . . . . . . . . . . . . . . . . 1-38

Theorem 2.1

χ2 corrected degrees of freedom . . . . . . . . . . . . . . . . . . 2-10

Theorem 2.2

Central limit theorem (CLT) . . . . . . . . . . . . . . . . . . . 2-18

Theorem 3.1

Weak law of large numbers (WLLN) . . . . . . . . . . . . . . . 3-11

Theorem 4.1

Consistency of the ML estimator . . . . . . . . . . . . . . . . . 4-13

Theorem 4.2

Asymptotic normality of the ML estimator . . . . . . . . . . . 4-26

Theorem 6.1

Bayes’ Rule (conditional probability) . . . . . . . . . . . . . . .

6-2

Theorem 6.2

Bayes’ Rule (statistical inference) . . . . . . . . . . . . . . . . .

6-7

Theorem 7.1

Distribution of Wilks’ statistic . . . . . . . . . . . . . . . . . . 7-23

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

1-9

2019.8.14

xiv

LIST OF FIGURES

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

LIST OF FIGURES

xv

List of Examples
Example 1.1

Poisson PMF . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1-2

Example 1.2

Exponential distribution: PDF . . . . . . . . . . . . . . . . . .

1-2

Example 1.3

Normal distribution: symmetry . . . . . . . . . . . . . . . . . .

1-3

Example 1.4

Exponential distribution: CDF . . . . . . . . . . . . . . . . . .

1-3

Example 1.5

Final-exam grade: expectation . . . . . . . . . . . . . . . . . .

1-4

Example 1.6

Uniform distribution: expectation . . . . . . . . . . . . . . . .

1-4

Example 1.7

Exponential distribution: median . . . . . . . . . . . . . . . . .

1-6

Example 1.8

Binomial distribution: median . . . . . . . . . . . . . . . . . .

1-6

Example 1.9

Binomial distribution: mode . . . . . . . . . . . . . . . . . . .

1-7

Example 1.10 Final-exam grade: variance . . . . . . . . . . . . . . . . . . . .

1-7

Example 1.11 Uniform distribution: variance . . . . . . . . . . . . . . . . . .

1-8

Example 1.12 PDF of a scaled exponential random variable . . . . . . . . . . 1-17
Example 1.13 PDF of the log-normal distribution from the normal . . . . . . 1-18
Example 1.14 Expectation of log uniform . . . . . . . . . . . . . . . . . . . . 1-19
Example 1.15 HIV vaccination trial: joint and marginal probabilities . . . . . 1-20
Example 1.16 HIV vaccination trial: conditional probability . . . . . . . . . . 1-22
Example 1.17 Final-exam and quiz grades: covariance . . . . . . . . . . . . . 1-25
Example 1.18 Covariance of zero does not imply independence . . . . . . . . . 1-26
Example 1.19 Course grade: expectation of a linear combination . . . . . . . 1-27
Example 1.20 Course grade: variance of a linear combination . . . . . . . . . 1-28
Example 1.21 Gamma distribution: mean and variance . . . . . . . . . . . . . 1-29
Example 1.22 Exponential distribution: MGF . . . . . . . . . . . . . . . . . . 1-32
Example 1.23 Gamma distribution: MGF . . . . . . . . . . . . . . . . . . . . 1-33
Example 1.24 Standard normal distribution: MGF . . . . . . . . . . . . . . . 1-34
Example 1.25 Binomial distribution: MGF . . . . . . . . . . . . . . . . . . . 1-34
Example 1.26 Exponential distribution: mean and variance via the MGF . . . 1-36
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

xvi

LIST OF FIGURES

Example 1.27 Gamma distribution: mean and variance via the MGF . . . . . 1-36
Example 1.28 Binomial distribution: mean and variance via the MGF . . . . 1-37
Example 1.29 Uniform distribution: existence of the MGF . . . . . . . . . . . 1-37
Example 1.30 Normal distribution: linear function . . . . . . . . . . . . . . . 1-39
Example 1.31 Exponential distribution: sum of IID random variables . . . . . 1-39
Example 1.32 Normal distribution: sum of independent random variables . . . 1-39
Example 1.33 Casualty insurance: sum of IID geometric random variables . . 1-40
Example 2.1

Lung function: confidence interval for the normal mean . . . . 2-12

Example 2.2

Binomial distribution: normal approximation . . . . . . . . . . 2-14

Example 2.3

Opinion polls: margin of error (confidence interval) . . . . . . . 2-17

Example 2.4

Binomial distribution: normal approximation via CLT . . . . . 2-19

Example 3.1

Faults on data lines: estimating the Poisson mean . . . . . . .

3-3

Example 3.2

Faults on data lines: properties of the estimator of µ . . . . . .

3-8

Example 3.3

Sample variance: divisor of n − 1 or n?

3-9

Example 3.4

Opinion polls: weak law of large numbers . . . . . . . . . . . . 3-11

Example 3.5

Binomial distribution: sample size determination . . . . . . . . 3-13

Example 4.1

HIV vaccine: ML estimate of the binomial π parameter . . . .

4-1

Example 4.2

Binomial distribution: ML estimate of the parameter π . . . . .

4-4

Example 4.3

Exponential distribution: ML estimate of the rate

. . . . . . .

4-6

Example 4.4

Poisson distribution: ML estimate of the mean . . . . . . . . .

4-7

Example 4.5

HIV vaccine: sampling variance of the ML estimator . . . . . .

4-9

Example 4.6

HIV vaccine: consistency of the ML estimator . . . . . . . . . . 4-11

Example 4.7

Exponential distribution: consistency (simulation) . . . . . . . 4-11

Example 4.8

Exponential distribution: consistency (mathematical) . . . . . . 4-12

Example 4.9

Binomial distribution: observed information . . . . . . . . . . . 4-15

. . . . . . . . . . . . .

Example 4.10 Exponential distribution: observed information . . . . . . . . . 4-16
Example 4.11 Exponential distribution: Var(λ̃) justification (simulation) . . . 4-17
Example 4.12 Exponential distribution: Var(λ̃) justification (mathematical) . 4-19
Example 4.13 Geometric distribution: Fisher information . . . . . . . . . . . 4-21
Example 4.14 Binomial distribution: Fisher information . . . . . . . . . . . . 4-23
Example 4.15 Exponential distribution: Fisher information . . . . . . . . . . 4-24
Example 4.16 Exponential distribution: asymptotic normality of λ̃ (simulation) 4-27
Example 4.17 Exponential distribution: asymptotic normality of λ̃ (mathematics)4-27

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

LIST OF FIGURES

xvii

Example 4.18 Faults on data lines: confidence interval for the mean . . . . . . 4-31
Example 4.19 Faults on data lines: confidence interval for Pr(Y = 0) . . . . . 4-31
Example 4.20 HIV vaccine: two confidence intervals for π . . . . . . . . . . . 4-32
Example 5.1

Normal distribution: ML estimates . . . . . . . . . . . . . . . .

5-1

Example 5.2

Normal distribution: unbiasedness of ML estimators . . . . . .

5-3

Example 5.3

Normal distribution: covariance matrix of ML estimators . . . .

5-5

Example 5.4

Lung function: ML confidence interval for the normal mean . .

5-7

Example 5.5

Censored insurance claims . . . . . . . . . . . . . . . . . . . . .

5-8

Example 5.6

Censored insurance claims: maximum likelihood . . . . . . . . 5-10

Example 6.1

Quality control: Bayesian estimation of (discrete) π . . . . . . .

6-3

Example 6.2

Binomial distribution: Bayesian estimation of π . . . . . . . . .

6-7

Example 6.3

Quality control: Bayesian estimation of (continuous) π . . . . . 6-10

Example 6.4

HIV vaccine: Bayesian estimation of π . . . . . . . . . . . . . . 6-11

Example 6.5

Poisson distribution: Bayesian estimation of µ . . . . . . . . . . 6-12

Example 6.6

HIV vaccine: Bayesian credible interval for Π . . . . . . . . . . 6-14

Example 6.7

Faults on data lines: credible interval for the mean . . . . . . . 6-14

Example 6.8

Normal distribution: Bayesian estimation of µ (known σ 2 ) . . . 6-17

Example 6.9

Normal distribution: Bayesian estimation of σ 2 (known µ) . . . 6-19

Example 6.10 Normal distribution: estimation of µ (unknown τ = 1/σ 2 ) . . . 6-20
Example 6.11 Lung function: Bayesian credible interval for the mean . . . . . 6-21
Example 6.12 Faults on data lines: posterior Pr(Y = 0) . . . . . . . . . . . . 6-26
Example 6.13 Lung function: Credible interval (Gibbs sampling) . . . . . . . 6-28
Example 7.1

Quality control: Bayesian hypothesis test of π . . . . . . . . . .

7-2

Example 7.2

Is coin tossing fair?

7-7

Example 7.3

Exponential distribution: test of simple hypotheses . . . . . . . 7-12

Example 7.4

Normal distribution: test of the mean with known variance . . 7-15

Example 7.5

Exponential distribution: 1-sided test . . . . . . . . . . . . . . 7-17

Example 7.6

Exponential distribution: 2-sided test . . . . . . . . . . . . . . 7-19

Example 7.7

Lung function: formulation of a generalized LR test . . . . . . 7-20

Example 7.8

Normal distribution: GLR justification of the t statistic . . . . 7-22

Example 7.9

Lung function: Wilks’ approximate test . . . . . . . . . . . . . 7-24

. . . . . . . . . . . . . . . . . . . . . . . .

Example 7.10 Rainfall: hypothesis test of the normal mean . . . . . . . . . . 7-26
Example 7.11 Lung function: t test of the normal mean . . . . . . . . . . . . 7-28

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

xviii

LIST OF FIGURES

Example 7.12 Lung function: p-value of test of the normal mean . . . . . . . 7-30
Example 7.13 Rainfall: p-value of test of the normal mean . . . . . . . . . . . 7-30
Example 7.14 Lung function: p-value of test of the mean continued . . . . . . 7-32
Example 7.15 Rainfall: practical significance

. . . . . . . . . . . . . . . . . . 7-33

Example 7.16 Rainfall: hypothesis test versus confidence interval . . . . . . . 7-34
Example 7.17 Anaesthesia: binomial with no failures observed . . . . . . . . . 7-36
Example 8.1

Genotyping: multinomial distribution . . . . . . . . . . . . . .

8-2

Example 8.2

Genotyping: ML estimates of the multiomial parameters . . . .

8-4

Example 8.3

Inheritance: test of Mendel’s ratios . . . . . . . . . . . . . . . .

8-6

Example 8.4

Genotyping: test of Hardy-Weinberg hypothesis . . . . . . . . .

8-8

Example 8.5

Faults on data lines: goodness of fit test . . . . . . . . . . . . . 8-10

Example 8.6

Solids in grapefruit: goodness of fit test . . . . . . . . . . . . . 8-13

Example 9.1

Faults on data lines: comparison of Poisson means . . . . . . .

9-2

Example 9.2

Faults on data lines: comparison of means per kilometre . . . .

9-5

Example 9.3

Smoking cessation: comparison of binomial π parameters . . . .

9-7

Example 9.4

TV advertisements: comparison of normal means . . . . . . . . 9-11

Example 9.5

Rainfall: data before and after differencing . . . . . . . . . . . 9-19

Example 9.6

Protein constructs: estimation of mean difference . . . . . . . . 9-21

Example 9.7

Solids in grapefruit: data . . . . . . . . . . . . . . . . . . . . . 9-22

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

Abbreviations
Abbreviation
CLT
GLR
IID
LR
ML
MSE
PDF
PMF

Description
Central limit theorem
Generalized likelihood ratio
Independent and identically distributed
Likelihood ratio
Maximum likelihood
Mean squared error
Probability density function
Probability mass function

xix

xx

LIST OF FIGURES

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

Greek Symbols
The following Greek letters are used in the book. Pronunciations by statisticians vary
but are often close to those given here.
Case
Lower Upper
α
A
β
B
γ
Γ
δ
∆
ϵ
E
ζ
Z
η
H
θ
Θ
ι
I
κ
K
λ
Λ
µ
M

Pronunciation
al-fah
bay-tah
gam-ah
del-tah
ep-si-lon
zay-tah
ay-tah
thay-tah
eye-oh-tah
kap-ah
lam-dah
mew

Case
Lower Upper
ν
N
ξ
Ξ
o
O
π
Π
ρ
R
σ
Σ
τ
T
υ
Υ
ϕ
Φ
χ
X
ψ
Ψ
ω
Ω

xxi

Pronunciation
new
zie (rhymes with pie)
oh-my-kron
pie
roe
sig-mah
tow (rhymes with now)
up-sigh-lon
fie (rhymes with pie)
kie (rhymes with pie)
sigh
oh-me-gah

xxii

LIST OF FIGURES

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-1

Chapter 1
Probability Tools
Statistical methods are strongly dependent on probability tools. Indeed, a statistical
method typically starts and ends with probability models. The first step is to specify a
probability model for the way the data were generated, and the last step often involves
a calculation such as looking up a probability to compute a confidence interval or a
Bayesian credible interval. In between, much of statistical inference is concerned with
the unknown parameters of the probability model, which has possibly been refined
along the way.
Thus, statistics and probability are intertwined, and this chapter reviews the probability tools we will need for statistical inference. It starts with one random variable
and general properties like expectation and variance. The specific properties of some
common probability models—those we will use frequently in later chapters—are collected together as a resource. Most statistical work involves samples of more than one
observation, and hence we also need to review results for several random variables,
including their joint distribution and properties of their sum or arithmetic mean.
Finally, the chapter outlines the use of moment generating functions as a relatively
simple tool for obtaining properties, particularly those of sums and linear functions
of random variables, as needed for statistical work involving sample totals or sample
means.

1.1

Discrete and Continuous Random Variables

In our journey through this book we will meet random variables that take either
discrete values (e.g., integers) or continuous values (e.g., positive real numbers). In
both instances we denote the random variable by an upper case letter like Y and its
values by the corresponding lower case letter, y.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-2

CHAPTER 1. PROBABILITY TOOLS

1.1.1

Probability mass function and probability density function

The distribution of Y over its possible values is denoted by fY (y).
For a discrete random variable, fY (y) can be interpreted as Pr(Y = y), the probability
that Y takes the value y, and fY (y) is called a probability mass function (PMF). The
mass function is positive and sums to 1 over the possible y values.
Example 1.1 (Poisson PMF)
If Y has a Poisson distribution, it has possible values y = 0, 1, . . . , ∞ and PMF
fY (y) =

e−µ µy
.
y!

The Poisson distribution is actually a family of distributions depending on the
value of the parameter µ > 0, and we will use the notation Pois (µ) to denote
the family. (The properties of the Poisson and other commonly used distributions
will be summarized in Sections 1.4 and 1.5.) In practice, the value of µ is usually
unknown for a specific application, and much of our statistical work will be about
how to estimate the values of parameters like µ from a sample of data.
The Poisson PMF sums to 1, as required:
∞
∑

fY (y) =

y=0

∞
∑
e−µ µy
y=0

y!

−µ

=e

∞
∑
µy
y=0

y!

= e−µ eµ = 1,

because of the series representation 1 + µ + µ2 /2! + · · · for eµ .

♢♢♢

(The end of an example is marked by a ♢♢♢ symbol.)
For a continuous random variable, fY (y) is called a probability density function
(PDF), and fY (y) cannot be interpreted as a probability. It is, however, proportional to the probability that Y falls in a small interval around y (Exercise 1.1). The
density function is positive and integrates to 1 over the range of possible y values.
Example 1.2 (Exponential distribution: PDF)
If Y has an exponential distribution, it has possible values 0 < y < ∞ and PDF
fY (y) = λe−λy

(0 < y < ∞; λ > 0).

The distribution, denoted Expon (λ), depends on the parameter λ > 0.
The exponential PDF integrates to 1, as required:
∫ ∞
∫ ∞
y=∞
fY (y) dy =
λe−λy dy = −e−λy y=0 = 0 − (−1) = 1.
0

♢♢♢

0

A distribution is symmetric if there exists µ such that its PMF or PDF can be written
fY (µ − x) = fY (µ + x),
for values x that generate all possible values y.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.1. DISCRETE AND CONTINUOUS RANDOM VARIABLES

1-3

Example 1.3 (Normal distribution: symmetry)
A random variable with a normal distribution has PDF
1
2
1
e− 2σ2 (y−µ)
2πσ
over possible values −∞ < y < ∞, for given constants µ and σ 2 > 0. The PDF
satisfies
fY (µ − x) = fY (µ + x) (0 ≤ x < ∞)

fY (y) = √

and hence is symmetric around µ for any value of σ 2 .

1.1.2

Cumulative distribution function

For either a continuous or a discrete random variable, Y , the cumulative distribution
function (CDF) is defined as
FY (y) = Pr(Y ≤ y).
For a particular value y, the probability will be evaluated by summation (discrete Y )
or integration (continuous Y ) over values up to y. For a continuous random variable,
it does not matter whether the CDF is defined as Pr(Y ≤ y) or Pr(Y < y).
For a discrete random variable, there is usually little choice but to sum the PDF
explicitly to compute the CDF. For instance, the Poisson CDF evaluated at, say,
y = 3 is
3
∑
e−µ µy
,
FY (3) = Pr(Y ≤ 3) =
y!
y=0
and not much simplification is possible.
For some commonly met continuous distributions, however, simple expressions for
the CDF are available by integrating the PDF. Conversely, the PDF is obtained by
differentiating the CDF.
Example 1.4 (Exponential distribution: CDF)
Let Y have an Expon (λ) distribution. From the definition of the CDF,
∫ y
∫ y
t=y
λe−λt dt = −e−λt t=0 = −e−λy −(−1) = 1−e−λy .
FY (y) = Pr(Y ≤ y) =
fY (t) dt =
0

0

(Here, t is a dummy variable as we want to integrate over all values t of Y up to
y.)
Similarly, we can go from the CDF to the PDF:
dFY (y)
d
= (1 − e−λy ) = λe−λy = fY (y).
dy
dy

♢♢♢

Statisticians sometimes find it convenient to work in terms of the survival function
or survivor function,
SY (y) = Pr(Y > y) = 1 − FY (y).
It is just the complement of the CDF.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-4

CHAPTER 1. PROBABILITY TOOLS
Grade on
final (y)
60
90

fY (y)
0.2
0.8

Table 1.1: Probability mass function for the final-exam grade of a randomly chosen
student in a given section of a statistics course

1.2

Mean, Median, and Mode

Much statistical analysis is concerned with estimating an average or typical value to
represent a distribution of possible values. There are several definitions of “average”.

1.2.1

Mean or expectation

The mean or expected value of a random variable Y is just a weighted average over
the possible values, y, with the weights given by fY (y).
Definition 1.1 (Expectation (mean))
The expected value or mean of a random variable Y is given by the sum
E(Y ) =

∑

yfY (y)

y

if Y takes discrete values, or by the integral
∫
E(Y ) = yfY (y) dy
if Y takes continuous values. The integral or sum is over all possible values
y.
Example 1.5 (Final-exam grade: expectation)
As a simple illustration of expectation of a discrete random variable, let Y represent the grade on the final exam of a randomly chosen student from a given section
of a statistics course. For simplicity, let us say Y can take only two values, 60%
and 90%. The probability mass function, fY (y), for Y is given in Table 1.1.
Definition 1.1 immediately gives
E(Y ) = 60(.2) + 90(.8) = 84,
i.e., the mean grade of students is 84%. This example shows that the so-called
expected value does not have to be a possible value of the random variable.♢♢♢

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.2. MEAN, MEDIAN, AND MODE

1-5

Example 1.6 (Uniform distribution: expectation)
If Y has a uniform distribution, it has possible values a < y < b, for given
constants a and b, and PDF
fY (y) =

1
b−a

(a < y < b; a < b).

The distribution is denoted by Unif (a, b).
From Definition 1.1, the expectation or mean of Y is
∫ b
∫ b
y=b
1
y2
b 2 − a2
a+b
E(Y ) =
yfY (y) dy =
y
dy =
=
=
.
b−a
2(b − a) y=a 2(b − a)
2
a
a
♢♢♢
In later probability and statistical results we will often have a condition that a property, like expectation, of a random variable has to exist. The∑condition is just requiring that the expectation is defined, that is, if and only if y |y|fY (y) (discrete)
∫
or |y|fY (y) dy (continuous) is finite. To illustrate this technicality, consider the
Poisson distribution,
fY (y) =

e−µ µy
y!

(y = 0, 1, . . . , ∞; µ > 0),

where µ is a parameter controlling the shape of the distribution. The expectation is
∞
∑
e−µ µy
E(Y ) =
y
.
y!
y=0

It may look like this sum diverges, because the infinite sum averages y values tending
to infinity. But the growth in y (and possibly µy ) is dominated by 1/y!, which
decreases much more rapidly. Thus, the sum converges to a finite quantity, and the
expectation is µ (Exercise 1.4). The notation µ is often used for the mean of a random
variable in general.
In contrast, take the distribution
fY (y) =

6 1
π2 y2

for y = 1, 2, . . . , ∞,

where π ≃ 3.14159 (not a parameter). This is a valid PMF, because its values are
positive and sum to 1. If we try to calculate
E(Y ) =

∞
∑

∑∞

y=1

y

6 1
,
π2 y2

however, the sum does not converge ( y=1 1/y is divergent). Here, the PMF does not
decay fast enough to offset the growth in the value of y; the expectation is infinite.
This simple illustration shows that not every PMF or PDF yields an expected value.
A constant a has expectation a. This is seen by applying Definition 1.1 to the degenerate discrete random variable A that takes value a with probability 1.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-6

CHAPTER 1. PROBABILITY TOOLS
y
PMF
CDF

0
1
0.421875 0.421875
0.421875 0.843750

2
0.140625
0.984375

3
0.015625
1.000000

Table 1.2: Binomial PMF and CDF for n = 3 trials and probability of success π = 1/4

1.2.2

Median

The median m of a random variable Y essentially divides its range such that the
total probability of 1 is divided equally left and right of m. Thus, from the CDF,
m satisfies FY (m) = Pr(Y ≤ m) = 1/2 or FY (m) ≃ 1/2. The latter approximation
arises because there may be no solution m exactly satisfying FY (m) = 1/2 when Y
is discrete. The definition of the median has to accommodate such cases.
Definition 1.2 (Median of a distribution)
The median m of a random variable Y is the smallest possible value of Y such
that FY (m) = Pr(Y ≤ m) ≥ 1/2. For a continuous random variable with
continuous CDF, m is the solution of FY (m) = 1/2.
The definition is thus straightforward for continuous distributions.
Example 1.7 (Exponential distribution: median)
As FY (y) = 1 − e−λy , the median m satisfies 1 − e−λm = 1/2. Rearrangement
gives e−λm = 1/2, then −λm = − ln(2), and finally m = ln(2)/λ.
♢♢♢
For a discrete random variable, there are rules for some special cases, but m is usually
found by enumeration.
Example 1.8 (Binomial distribution: median)
The binomial distribution with n trials and probability of “success” π has PMF
( )
n y
fY (y) =
π (1 − π)n−y (y = 0, 1, . . . , n).
y
Suppose n = 3 and π = 1/4, for which the PMF and CDF are given computed in
Table 1.2. It is seen that the y = 1 is the smallest value such that FY (y) ≥ 1/2,
and the median is m = 1. Also note that FY (1) = Pr(Y ≤ 1) ≥ 1/2 and
Pr(Y ≥ 1) = 1 − 0.421875 ≥ 1/2, and in this sense m = 1 divides the total
probability of 1 into 2 halves.
♢♢♢

1.2.3

Mode

The mode of a distribution is a value maximizing the PMF or PDF. It may not be
unique.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.3. VARIANCE

1-7

Example 1.9 (Binomial distribution: mode)
The binomial distribution with n = 3 trials and probability of success π = 1/4
has the PMF computed in Table 1.2. We see that the PMF is maximized by both
y = 0 and y = 1. Hence, there are two modal values.
♢♢♢

1.3

Variance

1.3.1

Computation

The variance of Y is the expected (mean) of the squared deviation of Y around its
mean.
Definition 1.3 (Variance)
The variance of Y is
Var(Y ) = E (Y − E(Y ))2 ,
where the expectation on the right is with respect to the distribution of Y . An
equivalent definition, often used, is
Var(Y ) = E(Y 2 ) − (E(Y ))2 .
The definition of variance requires computation of expectations, which are handled
by referring back to Definition 1.1.
For a discrete random variable, expectation and hence variance are computed by
summation over all the possible values y:
∑
Var(Y ) = E (Y − E(Y ))2 = E (Y − µ)2 =
(y − µ)2 fY (y)
y

or, equivalently,
Var(Y ) = E(Y 2 ) − (E(Y ))2 = E(Y 2 ) − µ2 =

∑

y 2 fY (y) − µ2 ,

y

where µ = E(Y ).
Example 1.10 (Final-exam grade: variance)
For the distribution fY (y) of final grades in Table 1.1, we already computed
E(Y ) = µ = 84. Hence,
Var(Y ) = E (Y − µ)2 = (60 − 84)2 (.2) + (90 − 84)2 (.8) = 144.
Alternatively, using the second definition of variance,
Var(Y ) = E(Y 2 ) − µ2 = (60)2 (.2) + (90)2 (.8) − (84)2 = 144.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-8

CHAPTER 1. PROBABILITY TOOLS

To see the equivalence of the definition in general for discrete random variables, we just
expand the square in the first definition and rearrange the sum for the expectation:
∑
∑
E(Y − µ)2 =
(y − µ)2 fY (y) =
(y 2 − 2µy + µ2 )fY (y)
y

=

∑

y 2 fY (y) − 2µ

∑

y

y

yfY (y) + µ2 = E(Y 2 ) − 2µE(Y ) + µ2

y

= E(Y ) − 2µµ + µ = E(Y 2 ) − µ2 .
2

2

For a continuous random variable, summation is again replaced by integration, and
∫
2
Var(Y ) = E (Y − µ) = (y − µ)2 fY (y) dy
or, equivalently,
∫
Var(Y ) = E(Y ) − (E(Y )) =
2

2

y 2 fY (y) dy − µ2 .

The equivalence is shown in the same way as for a discrete random variable.
Example 1.11 (Uniform distribution: variance)
Let Y have a Unif (a, b) distribution, i.e., it has PDF
fY (y) =

1
b−a

(a < y < b; a < b).

From Example 1.6 we already know that E(Y ) = (a + b)/2.
To use the second expression for the variance in Definition 1.3, we also need
∫ b

∫ b

y=b

y3
a2 + b2 + ab
b 3 − a3
1
dy =
=
.
=
E(Y ) =
y fY (y) dy =
y
b−a
3(b − a) y=a 3(b − a)
3
a
a
2

2

2

Hence,
a2 + b2 + ab
Var(Y ) = E(Y ) − (E(Y )) =
−
3
2

2

(

a+b
2

)2
=

(b − a)2
.
12

♢♢♢

The variance exists only if the sum or integral converges. The expectation must exist
for the variance to exist.
A constant a has variance 0. This is seen by applying Definition 1.3 to the degenerate
discrete random variable A that takes value a with probability 1:
Var(A) = E(A − E(A))2 = (a − a)2 × 1 = 0.
Often, σ 2 is used as notation for a variance.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.4. COMMONLY USED DISCRETE DISTRIBUTIONS

1.3.2

1-9

Standard deviation

The standard deviation, often denoted by σ, is
√
sd(Y ) = Var(Y ).
As the variance and standard deviation of a random variable are trivially related,
we can use either. For mathematical manipulation, it is often easier to work with
variances. For example, variances, not standard deviations, add for independent
random variables (Section 1.7.7). On the other hand, when reporting results the
standard deviation is easier to interpret because it has the same physical units as Y
and not the square of the original units. For instance, the variance of the exam grade
in Example 1.10 is 144%2 and having units of %2 is bizarre. We could also say that
the standard deviation of Y is
√
sd(Y ) = Var(Y ) = 12%,
which is much easier to interpret. Hence, we will switch back and forth between
variance and standard deviation.

1.3.3

Chebyshev’s inequality

Chebyshev’s inequality uses the variance to bound how far a random variable, Y , can
deviate from its mean in the following probabilistic sense.
Theorem 1.1 (Chebyshev’s inequality)
Let the random variable Y have a distribution such that the mean and variance,
µ and σ 2 , exist. Then
σ2
Pr(|Y − µ| > t) ≤ 2 ,
t
for any t > 0.
The result holds for any distribution for Y , and hence the probability bound on the
right can be weak. Nonetheless, if Y has a small enough variance then there is only
a small probability that Y is more than an arbitrary distance from its mean, an
argument used to prove the law of large numbers in Theorem 3.1, for instance.

1.4

Commonly Used Discrete Distributions

Table 1.3 summarizes some commonly used discrete distributions, along with their
expectations and variances. It also gives their moment generating functions (to be
developed in Section 1.8). In the table, parameters of the distributions (e.g., the
parameter π of the Bernouilli distribution) are denoted by lower-case Greek letters
if they are usually unknown in practice (and hence estimated in statistical inference)
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-10

CHAPTER 1. PROBABILITY TOOLS

Distribution
and

PMF, fY (y)

E(Y )

Var(Y )

MGF, MY (t)

Bernoulli

fY (0) = 1 − π,

π

π(1 − π)

1 − π + πet

Bern (π)

fY (1) = π

Binomial

(y = 0, 1; 0 < π < 1)
(n ) y
π (1 − π)n−y
y

Bin (n, π)

(y = 0, 1, . . . , n;

notation

(−∞ < t < ∞)

nπ

nπ(1−π)

(1 − π + πet )n
(−∞ < t < ∞)

n = 1, 2, . . .;
0 < π < 1)
Geometric

(1 − π)y π

Geom0 (π)

(y = 0, 1, . . . , ∞;

1−π
π

1−π
π2

π
1 − (1 − π)et
(−∞ < t < − ln(1−π))

1
π

1−π
π2

et π
1 − (1 − π)et
(−∞ < t < − ln(1−π))

n
π

n(1 − π)
π2

µ

µ

0 < π < 1)
Geometric

(1 − π)y−1 π

Geom1 (π)

(y = 1, 2, . . . , ∞;
0 < π < 1)

Negative
binomial

(y−1 )
n−1

(1 − π)

y−n n

π

(y = n, n + 1, . . . , ∞;

(

)n
et π
1 − (1 − π)et
(−∞ < t < − ln(1−π))

NegBin (n, π) n = 1, 2, . . . , ∞;
0 < π < 1)
Poisson
Pois (µ)

e−µ µy
(y =
y!
0, 1, . . . , ∞; µ > 0)

eµ(e −1) (−∞ < t < ∞)
t

Table 1.3: Some commonly used discrete distributions, along with their expectations,
variances, and moment generating functions (MGFs)

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.4. COMMONLY USED DISCRETE DISTRIBUTIONS

1-11

or by Roman lower-case letters if they are usually known quantities. (The Greek
alphabet, with pronunciations, is given on page xxi.)
The distributions in Table 1.3 are now briefly described.

1.4.1

Bernoulli distribution

A Bernoulli random variable has only two possible outcomes, coded as 0 (“no”, “absent”, “failure”, etc.) or 1 (“yes”, “present”, “success”, etc.), with probabilities 1 − π
and π, respectively. Thus, the PMF can be represented as
fB (b) = π b (1 − π)1−b

(b = 0, 1; 0 < π < 1).

The Bernoulli distribution Bern (π) is the building block for the remaining discrete
distributions, which can all be thought of as counting the number of “successes”
(B = 1) observed from independent Bernoulli events. (We will refer to the event
B = 1 generically as a “success” when outlining the remaining distributions.)

1.4.2

Binomial distribution

The binomial distribution counts the number of “successes” among a fixed number,
n, of independent and identically distributed∑(IID) Bernoulli trials, each of which is
a success or not. Thus, Y ∼ Bin (n, π) is ni=1 Bi , where the Bi are independent
Bern (π). The binomial distribution is perhaps the most important discrete distribution, because Y /n is the sample proportion, of interest in numerous applications. For
instance, the efficacy of an experimental drug might be assessed by the proportion of
patients in a clinical trial of n patients who respond positively to the drug.

1.4.3

Geometric distribution

A random variable with a geometric distribution arises from a sequence of IID
Bernoulli trials. It counts the number of trials until one success is observed.
There are two equivalent versions of the geometric distribution; the one used is just a
matter of convenience for the application. The difference is whether the terminating
trial with a success outcome is counted. A Geom0 (π) random variable does not count
the terminating successful trial, and hence takes values 0, 1, 2, . . . for the number of
failures observed. The Geom1 (π) version does count the terminating trial, so there
must be at least one trial, and the random variable takes values 1, 2, . . .. Thus, the
Geom0 (π) versus Geom1 (π) notation indicates whether the support of the random
variable starts at 0 or 1.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-12

1.4.4

CHAPTER 1. PROBABILITY TOOLS

Negative-binomial distribution

A negative-binomial random variable Y ∼ NegBin (n, π) arises as the sum of n independent Geom1 (π) random variables. Thus it counts the number of Bernoulli trials
until n successes have occurred. The Geom1 (π) distribution is the special case of the
NegBin (n, π) distribution with n = 1.
The negative-binomial distribution is also related to the binomial in the following
sense. If Y ∼ NegBin (n, π), then Y represents a random sample size to achieve a
fixed number, n, of successes. The binomial switches what is random and what is
fixed: Y ∼ Bin (n, π) represents a random number of successes for a fixed sample size,
n.

1.4.5

Poisson distribution

A Poisson random variable can be thought of as a limiting case of the binomial. If
Y ∼ Bin (n, π), and we take the limits n → ∞ and π → 0 such that µ = nπ tends
to a constant, then Y ∼ Pois (µ) is the limiting distribution. Thus, the Poisson
distribution is called the law of rare events: the probability of a success is vanishingly
small, but a proper distribution arises because there are many such potential events.
The parameter µ is the mean and variance, which can be restrictive for applications.
Often empirical data suggest that the variance is larger than the mean, so-called
“over-dispersion”. Thus, even when first principles suggest a Poisson probability
model, a distribution with more flexibility in the mean-variance relationship, such as
the negative-binomial, might be substituted.

1.5

Commonly Used Continuous Distributions

Table 1.4 summarizes some commonly used continuous distributions, which we now
describe briefly.

1.5.1

Beta distribution

The beta distribution takes values on (0, 1) and hence is useful for modelling quantities
that must lie on that interval. It finds particular utility in Chapter 6 on Bayesian
inference, where uncertainty about a parameter representing a probability is often
treated as a beta random variable. In that chapter we shall see that the parameters
a and b of the Beta (a, b) distribution make the shape of its PDF fairly flexible.
The beta function,

∫ 1
y a−1 (1 − y)b−1 dy,

B(a, b) =

(1.1)

0

is the normalizing factor of the beta distribution.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.5. COMMONLY USED CONTINUOUS DISTRIBUTIONS

1-13

Distribution
and notation

PDF, fY (y)

E(Y )

Var(Y )

MGF, MY (t)

Beta

a
a+b

ab
(a + b)2 (a + b + 1)

Not useful

d

2d

χ2d

1 y a−1 (1 − y)b−1
B(a, b)
(0 < y < 1; a > 0; b > 0)
1
y d/2−1 e−y/2
2d/2 Γ(d/2)
(y > 0; d = 1, 2, . . .)

Exponential

λe−λy (y > 0; λ > 0)

1
λ

1
λ2

Beta (a, b)
Chi-squared

1
(1 − 2t)d/2
(−∞ < t < 21 )
λ
λ−t
(−∞ < t < λ)

Expon (λ)
Fisher’s F
Fd1 ,d2

(d1 /d2 )d1 /2 y d1 /2−1
) d1 +d
2
(
)(
2
B d21 , d22 1 + dd12 y
(y > 0; d1 , d2 = 1, 2, . . .)

1 λ(λy)ν−1 e−λy
Gamma
Γ(ν)
Gamma (ν, λ) (y > 0; ν > 0; λ > 0)
1 e− |y−µ|
ϕ
Laplace
2ϕ
(−∞ < y < ∞; −∞ <
Lap (µ, ϕ)
Log-normal
logN (µ, σ 2 )
Normal
N (µ, σ 2 )

µ < ∞; ϕ > 0)
1
2
√ 1 e− 2σ2 (ln(y)−µ)
2πσy
(y > 0; µ > 0; σ 2 > 0)
1
2
√ 1 e− 2σ2 (y−µ)
2πσ
(−∞ < y < ∞;

d2
d2 − 2
(d2 >

2d22 (d1 + d2 − 2)
Does not exist
d1 (d2 − 2)2 (d2 − 4)
(d2 > 4)

2)
ν
λ

ν
λ2

µ

2ϕ2

2

eµ+σ /2

(eσ − 1)e2µ+σ
2

)
λ ν
λ−t
(−∞ < t < λ)
eµt
1 − ϕ2 t2
(|t| < 1/ϕ)
(

2

Does not exist
at t = 0

µ

σ

2

1

2 2

eµt+ 2 σ t

(−∞ < t <

−∞ < µ < ∞; σ 2 > 0)
(
) d+1
2 − 2
( 1 1d ) √ 1 + yd
0
B 2, 2
d
(−∞ < y < ∞;
(d > 1)

∞)
d (d > 2)
d−2

Does not exist

(b − a)2
12

(rectangu-

ebt − eat
(b − a)t
(−∞ < t <

lar)

∞)

Student’s t
td
Uniform

d = 1, 2, . . .)
1 (a < y < b; a < b)
b−a

a+b
2

Unif (a, b)
Table 1.4: Some commonly used continuous distributions, along with their expectations, variances, and moment generating functions (MGFs)
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

CHAPTER 1. PROBABILITY TOOLS
5

1-14

λ

0

1

2

fY(y)

3

4

1
2
5

0

1

2
y

3

4

Figure 1.1: PDF of the exponential distribution with rate parameter λ taking values
1, 2, or 5

1.5.2

Exponential distribution

The PDF of the exponential distribution has a parameter λ called the rate, and
we denote the PDF by Expon (λ). As its name suggests, the PDF is exponentially
decreasing, as illustrated in Figure 1.1. As the rate increases, the distribution has
more mass to the left. For instance, if Y is an exponential distribution representing
the time to occurrence of an event, then a larger value of λ says that the rate at
which the event occurs is faster and Y tends to take smaller values. Mathematically,
E(Y ) = 1/λ for the exponential distribution, i.e., the mean decreases with increasing
rate (Exercise 1.7), which is summarized in Table 1.4 along with other properties.
Hence, if Y is measured in days say, E(Y ) also has units of days, and λ = 1/E(Y )
has units 1/day, a rate per day. That is why λ is often called the “rate” parameter.

1.5.3 Gamma distribution
The gamma PDF is a generalization of the exponential PDF: putting ν = 1 in
Gamma (ν, λ) gives Expon (λ). As with the exponential distribution, λ is interpreted
as a rate, but the extra parameter ν controls the shape of the distribution. Figure 1.2 shows that the gamma PDF is skewed like the exponential but approaches a
symmetric, bell-shape as ν increases.
A further connection between the exponential and gamma distributions is that a
sum of ν IID Expon (λ) random variables has a Gamma (ν, λ) distribution, a result
demonstrated in Example 1.31.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.5. COMMONLY USED CONTINUOUS DISTRIBUTIONS

λ
1
2
5

0.2
0.0

0.0

0.5

fY(y)
0.4

0.6

λ
1
2
5

fY(y)
1.0

1.5

0.8

(b) ν = 10

2.0

(a) ν = 2

1-15

0

1

2

3

4

5

0

y

5

10
y

15

20

Figure 1.2: PDF of the gamma distribution with rate parameter λ taking values 1,
2, or 5: (a) shape parameter ν = 2 and (b) shape parameter ν = 10
The normalizing factor in the denominator of the gamma PDF is the gamma function,
∫ ∞
y ν−1 e−y dy (ν > 0).
(1.2)
Γ(ν) =
0

It has the following properties.
• Γ(1) = 1 and Γ( 12 ) =

√
π (Exercise 1.8).

• Γ(ν + 1) = νΓ(ν) (by integration by parts).
• For integer ν > 0, from the previous result and Γ(1) = 1, we have Γ(ν + 1) = ν!.
The gamma and beta functions are related via B(a, b) = Γ(a)Γ(b)/Γ(a + b).
If Y has a Gamma (ν, λ) distribution, then Z = 1/Y has an inverse-gamma distribution with PDF
( )ν
1 1 λ
fZ (z) =
e−λ/z (0 < z < ∞; ν > 0; λ > 0)
Γ(ν) z z
(Exercise 1.9). Here ν is the shape parameter of the gamma distribution, but λ is
now called the scale (not rate).
The gamma and inverse-gamma distributions are much used in Bayesian estimation
of the parameters of other distributions (Chapter 6).

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-16

CHAPTER 1. PROBABILITY TOOLS
(b) Log base 10 of PDF

0.0

−3.0

0.1

−2.5

0.2

fY(y)
0.3
0.4

0.5

Log base 10 of fY(y)
−2.0
−1.5
−1.0

0.6

Laplace
Standard normal

−0.5

0.7

(a) PDF

−4

−2

0
y

2

4

−4

−2

0
y

2

4

√ )
(
Figure 1.3: PDFs of the Laplace and normal distributions: Lap µ = 0, ϕ = 1/ 2
(solid line) versus N (µ = 0, σ 2 = 1) (dashed line). The two distributions have the
same variance: 2ϕ2 = σ 2 = 1. (a) PDF and (b) log base 10 of the PDF

1.5.4

Laplace distribution

The Laplace distribution is also known as the double-exponential, because the PDF
decays exponentially on the left and on the right of the location parameter µ. Note
that the decay, exp(−|y − µ|/ϕ), is a function of the absolute distance from µ, in
contrast to the normal distribution’s decay with squared distance, exp(− 2σ1 2 (y − µ)2 ).
Thus, even if the two distributions have the same variance (2ϕ2 = σ 2 ), the Laplace
distribution has fatter tails, as illustrated in Figure 1.3. The use of log scale for the
PDFs in Figure 1.3(b) emphasizes that the Laplace PDF is relatively much larger in
the tails. The Laplace distribution is therefore useful in statistics for modelling data
with outlying observations.

1.5.5

Normal distribution

The normal distribution has great importance in statistical work, and Chapter 2 is
devoted to it.

1.5.6

Log-normal distribution

By definition, Y has a log-normal distribution denoted logN (µ, σ 2 ) distribution if
Z = ln(Y ) has a N (µ, σ 2 ) distribution. Note that in the definition, µ and σ 2 are

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.6. FUNCTION OF A RANDOM VARIABLE

1-17

the mean and variance after applying the log transformation, and not the mean and
variance of Y .
Having positive support, the log-normal distribution is useful for modelling quantities
such as losses in actuarial science, precipitation over a period of time, etc.

1.5.7

χ2 , F , and t distributions

These distributions arise from IID normal random variables, particularly through
their sample mean and sample variance. Properties of the χ2 , F , and t distributions
are developed in Chapter 2.

1.5.8

Uniform distribution

The uniform distribution is a special case of the beta distribution: a Beta (0, 0) random
variable has a Unif (0, 1) distribution, and the latter can easily be rescaled to have
range (a, b). Like the beta, the uniform finds most utility in this book for Bayesian
inference (Chapter 6), where a uniform distribution on a parameter is often taken to
represent no prior information about the value of the parameter.

1.6

Function of a Random Variable

1.6.1

PDF of a function of a continuous random variable

Suppose the PDF of a random variable Y is known, but we are interested in the
function g(Y ). It is easy to write down the PDF of the new random variable g(Y ), if
g(·) is a monotonic function.
Example 1.12 (PDF of a scaled exponential random variable)
Let Y ∼ Expon (λ), where the notation “∼” stands for “is distributed as”. For
instance, suppose Y is the time in years between earthquakes in a region. Then,
as described in Section 1.5, λ is interpreted as the rate of occurrences per year. If
we change the time scale to months, then Y becomes 12Y , a simple function of
Y . What is the PDF of Z = 12Y ?
From the definition of the CDF,
FZ (z) = Pr(Z < z) = Pr(Z/12 < z/12) = Pr(Y < z/12) = FY (z/12) = 1−e−λz/12 ,
where the last equality is obtained by evaluating the exponential CDF for Y at
y = z/12. Then differentiating,
fZ (z) =

d
dFZ (z)
= (1 − e−λz/12 ) = (λ/12)e−(λ/12)z .
dz
dz

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-18

CHAPTER 1. PROBABILITY TOOLS

This is seen to be the PDF of an exponential random variable, except that the
original rate of occurrence λ per year becomes λ/12 per month, which is intuitive.
♢♢♢
The derivation in Example 1.12 used an explicit expression for the exponential CDF,
and it may be easier to see the argument that way, but closer inspection shows
that the CDF could be used implicitly. Writing Z = g(Y ), where g(Y ) = 12Y ,
and Y = g −1 (Z), where g −1 (Z) = Z/12, we see that the argument boils down to
differentiating the CDF of Y as a function of z and applying the chain rule:
fZ (z) =

dFZ (z)
dFY (g −1 (z))
dg −1 (z)
=
=
fY (g −1 (z)).
dz
dz
dz

The CDF of Y is not explicitly required, as differentiating the CDF returns to the
PDF, a handy feature for distributions like the normal with no closed form for the
CDF.
This type of computation can be done in general for transformations Z = g(Y ), where
g(·) is a differentiable, monotonic function:
fZ (z) =

dg −1 (z)
fY (g −1 (z)),
dz

(1.3)

where the absolute value takes care of monotonic decreasing functions.
Example 1.13 (PDF of the log-normal distribution from the normal)
By definition, Z has a log-normal distribution denoted logN (µ, σ 2 ) if Y = ln(Z)
has a N (µ, σ 2 ) distribution.
Thus, Y and Z are related by the monotonic functions Z = g(Y ) = exp(Y ) and
Y = g −1 (Z) = ln(Z), and the log-normal PDF of Z may be obtained via (1.3)
from the normal PDF of Y in Table 1.4:
fZ (z) =

1.6.2

1
2
dg −1 (z)
d ln(z)
1 1
fY (g −1 (z)) =
fY (ln(z)) = √
e− 2σ2 (ln(z)−µ) .
dz
dz
z 2πσ
♢♢♢

Expectation of a function of a random variable

Suppose the random variable Y is transformed to
Z = g(Y ),
where g(·) is a known function. Because Y is random, so is Z, and the distribution of
Z has properties such as expectation. An extension of Definition 1.1 gives E(g(Y )).

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.6. FUNCTION OF A RANDOM VARIABLE

1-19

Definition 1.4 (Expectation of a function of random variable)
The expected value of g(Y ) is given by the sum
E(g(Y )) =

∑

g(y)fY (y)

y

if Y takes discrete values, or by the integral
∫
E(g(Y )) = g(y)fY (y) dy
if Y takes continuous values. The integral or sum is over all possible values
y.
∫
∑
Again, the expectation is defined only if y |g(y)|fY (y) or |g(y)|fY (y) dy, respectively, is finite. Thus, depending on the function g(·), Z could have a well-defined
(finite) expectation whether Y does or not.
Example 1.14 (Expectation of log uniform)
A chemist represents her uncertainty about the concentration of a chemical species
by thinking of it as a random variable, Z. Chemists often work on log scales for
concentrations, and she thinks Y = ln(Z) has a continuous uniform distribution,
1
(a < y < b),
b−a
where a and b are known bounds. (Log base 10 would be used in practice by
chemists, but it’s easier mathematically to work with natural logs.) But what is
the expected value of the unlogged concentration, Z = g(Y ) = eY ? We have
∫ b
∫ b
∫ b
1
1
y 1
e
g(y)fY (y) dy =
E(Z) =
dy =
ey dy =
(eb − ea ).
b−a
b−a a
b−a
a
a
fY (y) =

Suppose a = ln(10−4 ) and b = ln(10−2 ), for example, which correspond to unlogged concentrations from 10−4 M to 10−2 M (M = “mole”). Then the expected
concentration is
10−2 − 10−4
.0099
E(Z) =
=
= 0.0021M.
−2
−4
ln(10 ) − ln(10 )
4.605
Note there is no need to compute the PDF of Z to obtain its mean here.
Alternatively, if we do the work to find the PDF of Z first, applying the result (1.3)
with g −1 (Z) = ln(Z) we have
fZ (z) =

d ln(z)
1 1
fY (ln(z)) =
dz
zb−a

(ea < z < eb ).

Then we can find E(Z) from its PDF:
∫ eb
∫ eb
∫ eb
1 1
1
1
z
zfZ (z)d z =
E(Z) =
dz =
dz =
(eb − ea ).
z
b
−
a
b
−
a
b
−
a
a
a
a
e
e
e
This is the same result as before.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

♢♢♢
2019.8.14

1-20

CHAPTER 1. PROBABILITY TOOLS

A well-known use of the expectation of a function of a random variable is computing
the random variable’s variance. From Definition 1.3, we can write
Var(Y ) = E(Y 2 ) − (E(Y ))2 .
The term E(Y 2 ) is the expectation of the function g(Y ) = Y 2 , which is computed
just as in Definition 1.4: see Example 1.11 for instance.

1.7

Several Variables

1.7.1

Joint and marginal distributions

Data for statistical models are usually multiple observations, which are considered
realizations of random variables for deriving statistical properties of quantities such as
sample means and proportions. Thus, probability results for several random variables
are of interest. For simplicity, we concentrate here mainly on properties of two random
variables; extensions to more than two are fairly immediate.
Suppose the two random variables are Y and Z. If both are discrete, the joint PMF,
written fY,Z (y, z), is the probability that Y takes the value y and Z takes the value
z. We could also write
fY,Z (y, z) = Pr(Y = y ∩ Z = z),
where the intersection symbol “∩” denotes “and”. The joint PMF sums to 1 over
all possible y and z values. For continuous random random variables, the joint PDF
fY,Z (y, z) integrates to 1 over the possible y and z values. The following rules also apply to one discrete and one continuous random variable with appropriate summation
or integration.
The marginal distribution of one of the variables is given by summing or integrating
over the other variable. For example, the marginal distribution of Y is
∑

(Z is discrete)

 fY,Z (y, z)
∫z
fY (y) =
(1.4)


 fY,Z (y, z) dz (Z is continuous),
where the summation or integration is over all possible values z. The result here is
just a version of the law of total probability, which is discussed further in Section 1.7.2.
As one and only value of Z must occur, the marginal PMF or PDF of Y for any value
y is obtained by totalling the joint distribution over all possible values of z. Similarly,
fZ (z) is obtained by summing or integrating over the y values.
Example 1.15 (HIV vaccination trial: joint and marginal probabilities)
In recent years, studies like the one examined here have suggested that the search
for an effective vaccine against HIV will eventually pay off. A trial in Thailand
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.7. SEVERAL VARIABLES

Treatment (x)
Placebo (x = 0)
Vaccine (x = 1)

1-21
HIV infected?
No (y = 0) Yes (y = 1) Total
8124
74
8198
8146
51
8197
16 270
125 16 395

Table 1.5: HIV vaccine: two-way frequency table by treatment and HIV-infection
status
reported by Rerks-Ngarm et al. (2009) compared vaccination with ALVAC and
AIDSVAX against a placebo (no vaccination) in a double-blind, randomized clinical trial involving about 16 000 volunteers.
We will focus on the “modified intention to treat” data presented by the authors
and summarized in Table 1.5. The data are arranged by two variables: whether
a subject received a placebo (no treatment) or the vaccine, coded by x = 0, 1,
respectively, and whether the subject is HIV positive at the end of the trial,
coded by y = 0, 1 for no and yes, respectively.
Thus, random variables X and Y take the values x = 0, 1 and y = 0, 1, respectively. As this chapter reviews probability, we think of the 16 395 subjects in
Table 1.5 as the population of interest, from which the probabilities of various
events involving X and Y can be calculated. Of course, the real problem, the
statistical problem from Chapter 2 on, is to estimate such probabilities, regarding
the data as a random sample from a bigger population.
Considering the 16 395 subjects in the trial as the population of interest, suppose
a subject is sampled at random from the 16 395. From the observed frequencies
in Table 1.5, we can compute, for instance, the joint probability that X takes the
value 0 and Y takes the value 1:
fX,Y (0, 1) = Pr(X = 0 ∩ Y = 1) =

74
.
16 395

The probabilities for the other values of X and Y are analogous.
Marginal probabilities, i.e., probabilities relating to only one variable, can be
calculated directly or via (1.4). For a randomly chosen subject, for instance,
fY (1) = Pr(Y = 1) =

125
16 395

or equivalently by summing joint probabilities over the two possible values of X,
fY (1) = fX,Y (0, 1) + fX,Y (1, 1) =

51
125
74
+
=
.
16 395 16 395
16 395

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

♢♢♢

2019.8.14

1-22

1.7.2

CHAPTER 1. PROBABILITY TOOLS

Conditional distributions

Conditioning allows the PMF or PDF of one or more random variables to depend
on the value(s) of one or more other variables. For simplicity, we will again consider
just two random variables, Y and Z say. Their joint PMF or PDF is related to their
marginal and conditional distributions via
fY,Z (y, z) = fY (y)fZ|Y (z | y) = fZ (z)fY |Z (y | z)

(1.5)

for all y such that fY (y) > 0 and all z such that fZ (z) > 0. The symbol “|” is read
as “given” or “conditional on”. This result builds the joint distribution in two steps:
the distribution of Y , then the conditional distribution of Z given the value of Y ;
or conversely the distribution of Z, then the conditional distribution of Y given the
value of Z. The requirement “for all y such that fY (y) > 0” is there as conditioning
on the value y implies that the value has occurred, which in turn implies the value is
possible, and similarly the condition fZ (z) > 0.
Hence, if Y and Z are continuous or discrete random variables with joint PMF or
PDF fY,Z (y, z), by simple rearrangement the conditional distribution of Z given the
value of Y is
fY,Z (y, z)
fZ|Y (z | y) =
.
(1.6)
fY (y)
Here we assume fY (y) > 0, which is computed as in (1.4). The other conditional
distribution, fY |Z (y | z), is analogous.
Example 1.16 (HIV vaccination trial: conditional probability)
In the context of Example 1.15, the random variable of interest is Y , the HIVinfection status, particularly how it depends on X. Technically, we will consider
the probability that Y = 1 (HIV positive) conditional on or given the value of
X. For instance, “Is the probability of HIV infection smaller for the vaccine
treatment?”
We can compute directly from Table 1.5, for example, the probability that Y takes
the value 1 conditional on X taking the value 0 (no treatment):
fY |X (1 | 0) = Pr(Y = 1 | X = 0) =

74
≃ 0.0090.
8198

Alternatively, to demonstrate the result in (1.6),
fY |X (1 | 0) =

Pr(X = 0 ∩ Y = 1)
74/16 395
74
fY,Z (y, z)
=
=
=
.
fY (y)
Pr(X = 0)
8198/16 395
8198

A similar calculation shows that fY |X (1 | 1) = Pr(Y = 1 | X = 1) ≃ 0.0062.
So, based on these calculations, the treatment reduces the probability of being
HIV positive for these 16 395 subjects. The statistical question to be addressed
in later chapters is whether the apparent efficacy of the vaccine can be explained
by chance variation.
♢♢♢
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.7. SEVERAL VARIABLES

1-23

As already noted, the rule in (1.4) for obtaining the marginal distribution of one
random variable from its joint distribution with another is a version of the law of total
probability. Another version follows by rewriting fY,Z (y, z) according to marginal and
conditional distributions, as in (1.5). Thus, we have two ways of writing the law of
total probability for two random variables. (The law is often written in terms of
probabilities of events, which carries over immediately to discrete random variables.
For continuous random variables, “probability” is interpreted as a PDF.)
Theorem 1.2 (Law of total probability)
Let fY,Z (y, z) be the joint PMF or PDF of the random variables Y and Z with
values y and z, respectively. The marginal distribution of Y is given by
∑

(Z is discrete)

 fY,Z (y, z)
z
∫
fY (y) =


 fY,Z (y, z) dz (Z is continuous),
or equivalently by
∑

(Z is discrete)

 fZ (z)fY |Z (y | z)
∫z
fY (y) =


 fZ (z)fY |Z (y | z) dz (Z is continuous),
where the summation or integration is over all values z with fZ (z) > 0.

1.7.3

Statistical independence

Independence of random variables is an assumption we will often, indeed nearly always, be making for the statistical models in later chapters.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-24

CHAPTER 1. PROBABILITY TOOLS
Definition 1.5 (Statistical independence)
Two random variables Y and Z with joint PDF or PMF fY,Z (y, z) are statistically independent if and only if the following equivalent conditions hold.
1. The joint distribution factorizes as the product of the two marginal
distributions:
fY,Z (y, z) = fY (y)fZ (z)

(for all y, z).

2. The conditional distribution of Y does not depend on the value of Z:
fY |Z (y | z) = fY (y) (for all y and z such that fZ (z) > 0).
3. The conditional distribution of Z does not depend on the value of Y :
fZ|Y (z | y) = fZ (z)

(for all y and z such that fY (y) > 0).

As the conditions are equivalent, to demonstrate independence it is sufficient to verify
just one of them; note it has to hold for all possible values y and z. To show that
two variables are not independent, i.e., dependent, it is sufficient to find one counterexample pair of y, z values in one condition.
The conditions could also be equivalently expressed in terms of CDFs. For example,
the first condition becomes
FY,Z (y, z) = FY (y)FZ (z) (for all y, z).
In later chapters, however, we work more with PMFs and PDFs, hence the use of
them in the above definition.
With more than two random variables, they are pairwise independent if all pairs
of them satisfy the above conditions. They are mutually independent if their joint
distribution factorizes as a product of all their marginal distributions, with similar
definitions for the other equivalent conditions. When authors say just “independent”,
then “mutually independent” is usually assumed by default.

1.7.4

Random sample

Mutual independence is also usually implied for a “random sample” of size n from
some distribution. The sample of size n comprises n random variables over possible
samples, and the random variables are independent in the sense that the distribution
of the second draw from the distribution does not depend on the value of the first draw,
etc. As the random variables are drawn from the same distribution, we also have the
“identically distributed” part of “IID”. Hence, “random sample from a distribution”
and “IID random variables” are usually taken as synonymous.
In contrast, random sampling from a finite population without replacement would
give at best approximate independence: the first draw changes the membership of
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.7. SEVERAL VARIABLES

1-25

the finite population, affecting the population available for the second draw, and so
on.

1.7.5

Covariance

Two random variables have a covariance, and several random variables have pairwise
covariances. As well as being useful in their own right, covariances are sometimes
necessary to compute the variance of a linear combination of random variables.
In general, the covariance between two random variables Y and Z—discrete, continuous, or a mixture thereof—is defined as
Cov(Y, Z) = E((Y − µY )(Z − µZ )) = E(Y Z) − µY µZ .

(1.7)

where µY and µZ are E(Y ) and E(Z), respectively. The equivalence of the definitions
is easily verified by multiplying out the product and applying expectation of a linear
combination. The computation of E(Y Z) is again via a weighted average of possible
values, with the weights now given by the joint distribution fY,Z (y, z). If Y and Z
both take discrete values, for example, then
∑∑
yzfY,Z (y, z).
E(Y Z) =
y

z

Here the double sum is over the possible combinations of y and z values. If one or
both of the random variables are continuous, then one or both of the sums becomes
an integral.
From the definition of covariance, we immediately have
Cov(Y, Z) = Cov(Z, Y )
and
Cov(Y, Y ) = Var(Y ).
These identities are much used in mathematical manipulations.
Often, the correlation between Y and Z,
Cov(Y, Z)
√
,
ρ(Y, Z) = √
Var(Y ) Var(Z)

(1.8)

is easier to interpret. It is on the scale −1 ≤ ρ(Y, Z) ≤ 1, and measures the strength
of the linear relationship (negative or positive) between Y and Z.
Example 1.17 (Final-exam and quiz grades: covariance)
For the joint distribution in Table 1.1, we have already computed µY = 84, µZ =
71, and Var(Y ) = 144. Similarly, Var(Z) = 189. Using (1.7) we find that
Cov(Y, Z) = (60 − 84)(50 − 71)(0.1) + · · · = 36.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-26

CHAPTER 1. PROBABILITY TOOLS

Hence, from (1.8),
ρ(Y, Z) = √

36
√
= 0.218.
144 189

(What features of Table 1.6 lead to a mildly positive correlation here?)

♢♢♢

If Y and Z are independent, then E(Y Z) = E(Y )E(Z) and Cov(Y, Z) = 0. The
converse, that covariance of zero implies independence, is not true in general.
Example 1.18 (Covariance of zero does not imply independence)
As a simple counter-example to a claim that zero covariance always implies independence, let Y ∼ Unif (−1, 1) and Z = Y 2 . Clearly, the distribution of Z depends
on the value taken by Y , and from Definition 1.5 they are not independent. Their
covariance is
Cov(Y, Z) = E(Y Z) − E(Y )E(Z) = E(Y 3 ) − E(Y )E(Z) = 0 − 0 × E(Z) = 0,
where E(Y ) = 0 and E(Y 3 ) = 0 follow because Y is symmetric around 0. Thus,
the covariance between Y and Z is zero but they are not independent.
♢♢♢
A covariance of zero does imply independence in the special case of the bivariate
normal distribution. If Y and Z are bivariate normal with Cov(Y, Z) = 0, then Y
and Z are independent normal random variables, a result shown in Section 1.7.9.

1.7.6

Expectation of a linear combination of random variables

Linear combinations of random variables arise very frequently throughout statistical
science. In general, suppose we have a linear combination,
a0 +

n
∑

ai Y i ,

i=1

of n random variables, Y1 , . . . , Yn . Here, a0 , . . . , an are constants (not random). Then
(
)
n
n
∑
∑
E a0 +
ai Y i = a0 +
ai E(Yi ).
(1.9)
i=1

i=1

All expectations must exist. Other than that requirement, there are no further conditions. In particular, the result holds whether the Yi are independent or not.
Important special cases of the general result (1.9) include the expectation of a linear
function of a single random variable,
E(a0 + a1 Y ) = a0 + a1 E(Y )

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.7. SEVERAL VARIABLES

Grade on
final (y)
60
90

1-27
Grade from
quizzes (z)
50
80
0.1
0.1
0.2
0.6
0.3
0.7

0.2
0.8
1

Table 1.6: Joint probability function, fY,Z (y, z), for the final exam grade (Y ) and the
grade from the quizzes (Z) of a randomly chosen student in a given STAT 305 section
and the expectation of a sum of random variables,
E(Y1 + Y2 ) = E(Y1 ) + E(Y2 ).
These special cases are proved as Exercise 1.14
Note also that other definitions of “average” or “location of a distribution” such as
the median or mode do not obey such rules. Expectation is a sum or integral and is
therefore a linear operator. If expectation is combined with another linear operator,
like a linear combination, the order of operations can be interchanged.
When using this rule in deriving another result, it is helpful to include a statement
such as “the expectation of a linear combination of random variables is the linear
combination of their expectations” to explain the step in the argument.
Example 1.19 (Course grade: expectation of a linear combination)
Let Y be the final-grade random variable defined in Table 1.1. We now also have
the random variable Z, the grade from the quizzes, taking the value z, which is
either 50% or 80%. A randomly chosen student will have a value for Y and a
value for Z.
Table 1.6 gives the joint distribution, fY,Z (y, z), for this situation. For example,
the probability that Y takes the value 60 and Z takes the value 50 is 0.1. By
summing across the rows of the table we obtain the probability mass function
fY (y) in Table 1.1. Similarly, by summing down the columns we obtain fZ (z). (We
should really write fY (y) and fZ (z), respectively, to distinguish the two functions.
Often, the arguments are used to distinguish the functions, even though this is
sloppy mathematically.) The probability distributions fY (y) and fZ (z) are called
marginal distributions, because they are derived from the margins of the fY,Z (y, z)
table. It is easily verified that E(Z) = 71.
The overall course grade, G, is important. To simplify, let us ignore the lab
component and say
G = 0.55Y + 0.45Z.
Note that the weights in the linear combination are non-random.
From first principles, we could compute E(G) from the PMF, fG (g), for G. Using
the joint distribution in Table 1.6, fG (g) is given in Table 1.7. For example, G
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-28

CHAPTER 1. PROBABILITY TOOLS
Grade
(g)
55.5
69.0
72.0
85.5

fG (g)
0.1
0.1
0.2
0.6

Table 1.7: Probability mass function for the course grade of a randomly chosen student in a given STAT 305 section
takes the value 55.5 if Y is 60 and Z is 50, with joint probability 0.1 from Table 1.6.
From fG (g) it is easily verified that E(G) = 78.15. Computing the expectation of
G by first computing fG (g) would be a very tedious numerical problem, however,
if many random variables were being combined to form G.
Alternatively, applying (1.9) which says that the expectation of a linear combination of random variables is the linear combination of their expectations, we
have
E(G) = E(0.55Y + 0.45Z) = 0.55E(Y ) + 0.45E(Z) = 78.15.
This calculation requires only the expectations of the marginal distributions; it
does not require any other properties of the joint distribution.
♢♢♢

1.7.7

Variance of a linear combination of random variables

Providing all variances exist,
(
)
n
n
n ∑
n
∑
∑
∑
2
Var a0 +
ai Yi =
ai Var(Yi ) + 2
ai aj Cov(Yi , Yj ).
i=1

i=1

(1.10)

i=1 j=i+1

A simpler version of this result with n = 2 random variables is proved in Exercise 1.16.
Special cases of the result include:
Var(Y + Z) = Var(Y ) + Var(Z) + 2Cov(Y, Z)
Var(Y − Z) = Var(Y ) + Var(Z) − 2Cov(Y, Z)
Var(a + bY ) = b2 Var(Y ).
When using the general rule or special cases in deriving another result, it is helpful
to explain the step in the argument by a statement such as “using the result on the
variance of a linear combination of random variables”.
Example 1.20 (Course grade: variance of a linear combination)
The distribution of grades in Table 1.7 gives Var(G) = 99.65. Alternatively,
from (1.10) we find
Var(G) = (.55)2 Var(Y ) + (.45)2 Var(Z) + 2(.55)(.45)Cov(Y, Z) = 99.65.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

♢♢♢
2019.8.14

1.7. SEVERAL VARIABLES

1-29

Example 1.21 (Gamma distribution: mean and variance)
The gamma distribution (see Table 1.4) has PDF
fY (y) =

1
λ(λy)ν−1 e−λy
Γ(ν)

(0 < y < ∞; ν > 0; λ > 0),

which we write as Gamma (ν, λ). It has a similar form to the exponential distribution, for which we have already found the mean and variance, and a similar
approach could be used again.
With a slight loss of generality we can find the gamma distribution’s mean and
variance rather more simply, however. If the parameter ν is an integer greater
than or equal to 1 (this is the loss of generality), then a Gamma (ν, λ) random
variable Y can be generated by:
Y = Y1 + · · · + Yν ,
where the Yi are independent Expon (λ) random variables. (This result will
be proved in Example 1.31.) We know Yi has mean 1/λ and variance 1/λ2 .
Hence it immediately follows that a Gamma (ν, λ) random variable has mean ν/λ
(from (1.9)) and variance ν/λ2 (from (1.10)). Note that because the Yi are independent, all covariance terms in the variance calculation are zero. This result
actually holds for general ν > 0.
♢♢♢
Independence can be an important assumption in formal statistical models and derivations. For instance, the result (1.10) on the variance of a linear combination of random variables is applied to derive the variance of a sample mean or sample proportion
from independent observations Y1 , . . . , Yn (e.g., Exercise 1.17). But simple results are
only obtained when all distinct pairs of observations are independent and hence all
Cov(Yi , Yj ) terms for i ̸= j are all zero. If the assumption of independence is false,
the claimed variance of the sample mean or proportion could be highly misleading.
Furthermore, the assumption of independence is usually made out of necessity. To
take account of covariance terms between any two observations in the calculation of
the variance of a linear combination, one needs some insight into the structure of the
covariance, insight which is often lacking. In practice, appealing to the way the data
were collected—as a random sample or via randomization in an experiment—is the
only feasible justification of an independence assumption.

1.7.8

Covariance between linear functions or combinations of
random variables

There are analogous results for the covariance between linear functions of random
variables:
Cov(a + bY, c + dZ) = bdCov(Y, Z)
(1.11)
This is proved as Exercise 1.15.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-30

CHAPTER 1. PROBABILITY TOOLS

Similarly, for linear combinations of random variables,
( n
)
m
n ∑
m
∑
∑
∑
Cov
ai Y i ,
bj Zj =
ai bj Cov(Yi , Zj ).
i=1

j=1

i=1 j=1

Note that in general the two linear combinations can have different numbers of random
variables, n and m, respectively. When they involve the same random variables, i.e.,
with m = n and Yi = Zi for i = 1, . . . , n, we have
( n
)
n
n ∑
n
∑
∑
∑
Cov
ai Y i ,
bj Y j =
ai bj Cov(Yi , Yj ).
i=1

1.7.9

j=1

i=1 j=1

Bivariate normal distribution

Two continuous random variables Y1 and Y2 with a bivariate normal distribution have
joint PDF given by
(
)
1
1
T −1
exp − (y − µ) Σ (y − µ) ,
fY1 ,Y2 (y1 , y2 ) =
1
2
2π det 2 (Σ)
(

where
y=

y1
y2

)

(
,

µ=

µ1
µ2

)

(
,

Σ=

ρσ1 σ2
σ12
ρσ1 σ2
σ22

)
,

µ1 and µ2 are the means of Y1 and Y2 , respectively, σ1 > 0 and σ2 > 0 are the standard
deviations of Y1 and Y2 , respectively, −1 < ρ < 1 is the correlation between Y1 and
Y2 , and det(Σ) and Σ−1 denote matrix determinant and inverse of Σ, respectively.
The off-diagonal element ρσ1 σ2 in the covariance matrix Σ is the covariance between
Y1 and Y2 . It is zero if Y1 and Y2 are uncorrelated, i.e., if ρ = 0.
The bivariate normal has the special property that a covariance of zero between the
two random variables implies they are independent.
Lemma 1.1 (Bivariate normal: covariance of 0 implies independence)
If Y1 and Y2 have a joint bivariate normal distribution and their covariance
(correlation) is zero, then Y1 and Y2 are independent normal random variables.
To show the result, assume ρ = 0. Independence will follow by showing that the joint
distribution factorizes. First, we have
( 2
)
σ1 0
det(Σ) = det
= σ12 σ22 ,
0 σ22
1

and hence det 2 (Σ) = σ1 σ2 . Second,

(

σ12 0
(y − µ) Σ (y − µ) = (y1 − µ1 , y2 − µ2 )
0 σ22
(y1 − µ1 )2 (y2 − µ2 )2
=
+
.
σ12
σ22
T

−1

)−1 (

y1 − µ1
y2 − µ2

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

)

2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-31

Substituting these two results into the joint distribution gives
(
(
))
1
1 (y1 − µ1 )2 (y2 − µ2 )2
fY1 ,Y2 (y1 , y2 ) =
exp −
+
2πσ1 σ2
2
σ12
σ22
(
)
(
)
1
1
1
1
2
2
=√
exp − 2 (y1 − µ1 ) × √
exp − 2 (y2 − µ2 ) ,
2σ1
2σ2
2πσ1
2πσ2
which is the product of a N (µ1 , σ12 ) PDF for Y1 and a N (µ2 , σ22 ) PDF for Y2 . Hence
Y1 and Y2 are independent by Definition 1.5.
Note that independence implies covariance of zero for any two random variables (see
Section 1.7.5), including the bivariate normal. But the result that covariance of zero
implies independence does not hold in general.
The same arguments apply to the multivariate normal with n variables: if all pairwise
covariances are zero, the n random variables are mutually independent and normal.

1.8

Moment Generating Functions

1.8.1

Uses of moment generating functions

The moment generating function (MGF) is a powerful tool for proving probability
results essential for statistical methods.
• We can sometimes find the distribution of a sum of IID random variables from
the MGF of the underlying distribution. This is clearly useful for statistical
properties of sample totals or sample means, which are sums. For instance:
– Example 1.31 establishes that the sum of IID exponential random variables
has a gamma distribution, a result used for statistical hypothesis testing
in Example 7.3.
– Exercise 1.20 shows that a sum of independent Poisson random variables
has a Poisson distribution. This is again used in hypothesis testing, in
Exercise 7.2.
– The sum of IID geometric random variables has a negative-binomial distribution (Example 1.33).
• If we know the MGF of a random variable, it is easy to write down the MGF
of any linear function of it. This provides an easy proof that a linear function
of a normal random variable also has a normal distribution (Example 1.30), an
important property.
• Using the MGF is a relatively easy way of establishing approximate normality
of a sample mean or sample total under certain conditions (the central limit
theorem of Theorem 2.2) and special cases like the approximation of a binomial
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-32

CHAPTER 1. PROBABILITY TOOLS
distribution by a normal distribution (Example 2.2). Normal approximations
are widely used in statistical inference.

• The properties of the χ2 distribution and hence the sample variance when
sampling from a normal distribution are readily shown using MGFs (in Section 2.4.2).

1.8.2 Definition of the moment generating function
As its name suggests, the moment generating function generates the moments of a
distribution or random variable.
Definition 1.6 (Moments of a random variable)
Let Y be a random variable. Its kth moment for k = 1, 2, . . . is E(Y k ), which
exists if the expectation is finite.
Thus, the first moment with k = 1 is simply E(Y ). The first two moments, E(Y 1 ) and
E(Y 2 ), give the variance from Var(Y ) = E(Y 2 ) − (E(Y ))2 . The MGF, once found,
can generate all the moments of a random variable, including these two.
The MGF is found by computing an expectation.
Definition 1.7 (Moment generating function)
Let Y be a random variable. The moment generating function (MGF) for Y
is defined as
MY (t) = EY (etY ),
if it exists for t in a neighbourhood of 0, i.e., for t in the open interval
−T < t < T , where T > 0.
Note that the expectation is with respect to the distribution of Y , and is just the
expectation of a function of Y , namely etY . The parameter t is a dummy variable.
The MGF has to exist in an interval around t = 0 because manipulations of it will
involve the derivatives at t = 0 and Taylor series approximation at t = 0.
Example 1.22 (Exponential distribution: MGF)
Let Y be distributed Expon (λ). As this is a continuous random variable, we
compute the expectation in the MGF via integration:
∫ ∞
∫ ∞
∫ ∞
tY
ty
ty
−λy
MY (t) = E(e ) =
e fY (y) dy =
e λe
dy =
λe−(λ−t)y dy.
0

0

0

The integrand converges and the MGF exists if λ − t > 0.
Carrying out the integration is straightforward here, but this simple example is
an opportunity to show a method that avoids explicit integration in more difficult
cases. With the condition λ − t > 0, we can rewrite the integral as
∫ ∞
λ
(λ − t)e−(λ−t)y dy.
λ−t 0
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-33

The integrand is now the PDF of an exponential random variable with parameter
λ − t, and like any PDF it must integrate to 1. Thus, the MGF of the Expon (λ)
distribution is
λ
MY (t) =
,
(1.12)
λ−t
which exists for t < λ. We also note that λ > 0 (see Table 1.4), so the interval
t < λ includes an open interval around t = 0, as required by Definition 1.7. ♢♢♢
Example 1.23 (Gamma distribution: MGF)
From Table 1.4, the PDF of Y ∼ Gamma (ν, λ) is
fY (y) =

1
λ(λy)ν−1 e−λy .
Γ(ν)

First, we apply the definition of the MGF and simplify a little:
∫ ∞
∫ ∞
1
tY
ty
λ(λy)ν−1 e−λy dy
MY (t) = E(e ) =
e fY (y) dy =
ety
Γ(ν)
0
0
∫ ∞
1
=
λ(λy)ν−1 e−(λ−t)y dy,
Γ(ν)
0
which exists if t < λ. The integrand is of the form y a eby , and we note that a is
not necessarily an integer. (From Table 1.4, the parameter ν takes values ν > 0
and hence a = ν − 1 > −1.) There are many ways to proceed:
• Use standard methods of calculus.
• Look up a table of integrals.
• Use software such as Mathematica or Maple.
• Note that the integrand is very similar to the form of the original gamma
PDF and again use the fact that a PDF integrates to 1.
The last route turns out to be easy. All we need to do is take out a factor:
(
)ν ∫ ∞
∫ ∞
1
λ
1
ν−1 −(λ−t)y
λ(λy) e
dy =
(λ − t)((λ − t)y)ν−1 e−(λ−t)y dy.
Γ(ν)
λ−t
Γ(ν)
0
0
The integrand is now the PDF of a Gamma (ν, λ − t) random variable, i.e., with
the parameter λ replaced by λ − t everywhere, and the integral is 1. We are
left with just the factor in front of the integral, and the MGF of a Gamma (ν, λ)
random variable is
(
)ν
λ
MY (t) =
.
(1.13)
λ−t
It exists for t < λ and hence in an open interval around t = 0, because again
λ > 0.
♢♢♢
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-34

CHAPTER 1. PROBABILITY TOOLS

Example 1.24 (Standard normal distribution: MGF)
Let Z ∼ N (0, 1), i.e., the standard normal distribution with mean µ = 0 and
variance σ 2 = 1. Substituting these parameter values into the general normal
PDF in Table 1.4 gives
1 2
1
fZ (z) = √ e− 2 z .
2π
Thus,
∫ ∞
∫ ∞
∫ ∞
1 2
1
− 12 z 2
tz
tz 1
√ e− 2 (z −2tz) dz
dz =
MZ (t) =
e fZ (z) dz =
e √ e
2π
2π
−∞
−∞
∫ ∞
∫−∞
∞
1
1 2
1
1 − 1 ((z−t)2 −t2 )
2
√ e 2
√ e− 2 (z−t) dz
=
dz = e 2 t
2π
2π
−∞
−∞
1 2

= e2t .
The last integral is 1, because we see that the integrand is a normal PDF (with
1 2
µ = t and σ 2 = 1). We also note that e 2 t and hence MZ (t) exist for −∞ < t <
∞.
♢♢♢
The method used in Example 1.24 can also be applied to find the MGF of Y ∼
N (µ, σ 2 ), i.e., a normal random variable with arbitrary mean and variance. The
MGF of Y is
1 2 2
(1.14)
MY (t) = eµt+ 2 σ t .
The details are left to Exercise 1.23.
Example 1.25 (Binomial distribution: MGF)
The binomial distribution has PMF
( )
n y
fY (y) =
π (1 − π)n−y (y = 0, 1, . . . , n).
y
Because it takes discrete values, the MGF is found by summation:
( )
n y
MY (t) = E(e ) =
e fY (y) =
e
π (1 − π)n−y .
y
y=0
y=0
tY

n
∑

ty

n
∑

ty

Minor simplification is possible by collecting together the ety and π y terms, and
the task is to evaluate
n ( )
∑
n
MY (t) =
(πet )y (1 − π)n−y .
(1.15)
y
y=0
For this discrete problem we next try to turn the sum into a sum of a PMF over
its possible values. The expression to evaluate in (1.15) looks like the binomial
PMF, but a binomial PMF involves complementary probabilities, π and 1 − π,
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-35

which sum to 1. In (1.15) πet and 1 − π do not sum to 1 for t ̸= 0, but this is
easily fixed by dividing them by their sum to create
π̇ =

πet
1 − π + πet

and 1 − π̇ =

1−π
.
1 − π + πet

The divisor is cancelled by a factor outside the sum when we rewrite (1.15) as
n ( )
∑
n y
t n
MY (t) = (1 − π + πe )
π̇ (1 − π̇)n−y .
y
y=0
Here the sum is over the possible values of a Bin (n, π̇) random variable, and the
sum must be 1. Thus, the MGF of the binomial distribution is
MY (t) = (1 − π + πet )n .
It exists for −∞ < t < ∞.

1.8.3

♢♢♢

Finding moments from the MGF

As its name suggests, the MGF for a distribution generates the moments, E(Y k ). The
first moment is E(Y ), the mean of Y . The second moment is E(Y 2 ); from it and the
first moment, we can compute the variance, Var(Y ) = E(Y 2 ) − (E(Y ))2 . Similarly,
skewness, etc., can be computed from higher-order moments.
We will prove the result relating the MGF to the moments using a Taylor series
expansion around t = 0. This explains the mysterious condition in the definition of
the MGF that it needs to exist for t in an open interval around 0.
Suppose, MY (t) exists in a neighbourhood of t = 0. Then,
(k)

MY (0) = E(Y k ),

(1.16)

(k)

where MY (0) is MY (t) differentiated k times and evaluated at t = 0. To show this
we make a Taylor series expansion of etY in the definition of the MGF:
)
(
t2 Y 2 t3 Y 3
tY
+
+ ···
(1.17)
MY (t) = E(e ) = E 1 + tY +
2!
3!
t2
t3
2
= 1 + tE(Y ) + E(Y ) + E(Y 3 ) + · · · .
(1.18)
2!
3!
Differentiating once with respect to t gives
(1)

MY (t) = E(Y ) +

2t
3t2
E(Y 2 ) +
E(Y 3 ) + · · · ,
2!
3!

and evaluating at t = 0 gives M (1) (0) = E(Y ). Similarly, differentiating twice gives
(2)

MY (t) = E(Y 2 ) +

6tY 3
+ ··· ,
3!

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-36

CHAPTER 1. PROBABILITY TOOLS
(2)

and MY (0) = E(Y 2 ). The general result in (1.16) for E(Y k ) is just a continuation of
this process. The proof given here makes it obvious how the Taylor-series expansion
of etY generates powers of Y and hence the moments after taking expectation.
Example 1.26 (Exponential distribution: mean and variance via the MGF)
The first and second derivatives of the exponential distribution’s MGF in (1.12)
are
(
)2
λ
1
(1)
MY (t) =
λ λ−t
and
(2)
MY (t) =

2
λ2

(

λ
λ−t

)3
.
(1)

Putting t = 0 in the first expression gives E(Y ) = MY (0) = 1/λ. Similarly, t = 0
(2)
in the second expression gives E(Y 2 ) = MY (0) = 2/λ2 , and hence Var(Y ) =
E(Y 2 ) − (E(Y ))2 = 1/λ2 .
♢♢♢
We can compute the mean and variance of the exponential distribution directly (Exercise 1.7), so what is gained by use of the MGF in Example 1.26? The direct attack
on the mean and variance involves moderately complicated integrals. In contrast the
integration to find the MGF of the exponential distribution was straightforward. After some minor algebra, we recognized the integral of a PDF, which we know must be
1. Differentiating the MGF to get the moments was also easy. So we replaced nontrivial integrations with algebra and differentiation. (For a discrete random variable,
potentially nontrivial summations are similarly avoided.)
Example 1.27 (Gamma distribution: mean and variance via the MGF)
First, rewrite the MGF of the gamma distribution in (1.13) as
(
)ν (
)ν
λ
1
MY (t) =
=
.
λ−t
1 − t/λ
(Application of the chain rule is then a little easier.)
The first derivative of MY (t) is
(
(1)
MY (t) = −ν

1
1 − t/λ

)ν+1 (
)
(
)ν+1
1
1
ν
−
=
,
λ
λ 1 − t/λ

which equals ν/λ at t = 0. Therefore,
E(Y ) =

ν
.
λ

The second derivative is
ν
(2)
MY (t) = (−(ν + 1))
λ

(

1
1 − t/λ

)ν+2 (

1
−
λ

)

ν(ν + 1)
=
λ2

(

1
1 − t/λ

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

)ν+2
,
2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-37

which evaluates to ν(ν + 1)/λ2 at t = 0. This is E(Y 2 ). Therefore,
ν
ν(ν + 1) ( ν )2
Var(Y ) = E(Y 2 ) − (E(Y ))2 =
−
= 2.
2
λ
λ
λ
The same expectation and variance were found in Example 1.21, but the result
here holds for any ν > 0 and not just for positive integer values of ν.
♢♢♢
Example 1.28 (Binomial distribution: mean and variance via the MGF)
From Example 1.25 the MGF of the binomial distribution is
MY (t) = (1 − π + πet )n .
The first two derivatives of the MGF are
(1)

MY (t) = nπet (1 − π + πet )n−1
and
(2)

MY (t) = nπet (1 − π + πet )n−1 + nπet (n − 1)πet (1 − π + πet )n−2 .
Evaluating these derivatives at t = 0 gives
E(Y ) = nπ
and
E(Y 2 ) = nπ + n(n − 1)π 2 .
Therefore,
Var(Y ) = E(Y 2 ) − (E(Y ))2 = nπ + n(n − 1)π 2 − (nπ)2 = nπ(1 − π).

♢♢♢

The argument to obtain the moments from the MGF hinges on the Taylor series
expansion (1.18) at t = 0. Thus, the MGF has to exist at t = 0, which was straightforward to demonstrate for the examples up to here. The next example is a little
more subtle.
Example 1.29 (Uniform distribution: existence of the MGF)
If Y ∼ Unif (a, b), Table 1.4 says its MGF is
MY (t) =

ebt − eat
(b − a)t

(−∞ < t < ∞),

i.e., the table claims the MGF exists for all t including t = 0.
The appearance of t in the denominator may create some doubt about the existence, but note that the numerator is also 0 at t = 0. Expanding the exponential
functions, however, shows that all is well, however:
1 + bt + (bt)2 /(2!) + (bt)3 /(3!) + · · · − (1 + at + (at)2 /(2!) + (at)3 /(3!) + · · · )
(b − a)t
2
2 2
3
3 3
(b − a)t + (b − a )t /2 + (b − a )t /6 + · · ·
=
(b − a)t
=1 + (a + b)t/2 + (a2 + b2 + ab)t2 /6 + · · · ,

MY (t) =

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-38

CHAPTER 1. PROBABILITY TOOLS

which equals 1 at t = 0. (Simplifying the t3 term uses (b3 − a3 ) = (b − a)(a2 +
b2 + ab).)
Hence, the first two moments give the expectation and variance of Y in Table 1.4
(Exercise 1.21).
♢♢♢

1.8.4

MGF of a linear function or a sum

Lemma 1.2 (MGF of a linear function of a random variable)
If the MGF of Y is MY (t), then Z = a + bY has MGF
MZ (t) = eat MY (bt).
The result follows from
MZ (t) = EZ (etZ ) = EY (et(a+bY ) ) = eat EY (ebtY ) = eat MY (bt).
We next derive a result useful for a sum of independent random variables.
Lemma 1.3 (MGF of a sum of independent random variables)
Suppose Y1 , . . . , Yn are independent random variables, and Yi has MGF MYi (t)
(which must exist). Then the MGF of X = Y1 + · · · + Yn is
MX (t) =

n
∏

MYi (t).

i=1

The result follows from
tX

t

MX (t) = EX (e ) = EY1 ,...,Yn (e

∑n

i=1 Yi

) = EY1 ,...,Yn

( n
∏
i=1

=

n
∏

)
e

tYi

=

n
∏

EYi (etYi )

i=1

MYi (t).

i=1

Here, we are using the result that the expectation of a product of independent random
variables is the product of expectations.

1.8.5

The MGF identifies a distribution

An important property of the MGF is that it identifies a distribution uniquely.
Theorem 1.3 (The MGF identifies a distribution)
Let Y and Z be two random variables with MGFs MY (t) and MZ (t), respectively. If MY (t) = MZ (t) for all t in an open interval of 0, then
Pr(Y ≤ y) = Pr(Z ≤ y), i.e., Y and Z have the same CDF.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-39

In other words, if we can find the MGF of a random variable, and this MGF is on a
list of MGFs we have computed (as in Tables 1.3 and 1.4), we can use the MGF to
tell us the random variable’s distribution.
The MGF is thus a mathematical “fingerprint” for a distribution. A human fingerprint
can identify a person on a list of suspects already fingerprinted. However, if the
fingerprint belongs to a person not on the list of suspects, identification is not possible.
It is similarly difficult to work backwards from an MGF to its distribution without a
list of suspect distributions with MGFs like those in Tables 1.3 and 1.4. Fortunately,
those tables suffice for most of our needs in this book.
Billingsley (2012, Sections 9 and 30) gives several proofs of Theorem 1.3, for discrete
random variables and more generally. The essence of the general argument is that,
if the open-interval condition of the lemma is satisfied, a probability distribution is
determined by its moments, which are in turn determined by the MGF.
We use Theorem 1.3 in the following examples to find various distributions of random
variables derived from a linear function of a random variable or a sum of independent
random variables.
Example 1.30 (Normal distribution: linear function)
Let Y have a N (µ, σ 2 ) distribution, and let Z = a + bY . From the MGF of Y
in (1.14), the MGF of Z is
1

2

2

1 2 2 2

MZ (t) = eat MY (bt) = eat eµ(bt)+ 2 σ (bt) = e(a+bµ)t+ 2 b σ t .
This has the form of the MGF in (1.14) for a normal random variable, except that
the mean µ is replaced by a + bµ, and the variance σ 2 is replaced by b2 σ 2 . Thus,
Z is identified to have a N (a + bµ, b2 σ 2 ) distribution.
♢♢♢
Example 1.31 (Exponential distribution: sum of IID random variables)
Let Y1 , . . . , Yν be ν IID Expon (λ) random variables. Note they have the same
value of the parameter λ. Thus, from (1.12) each Yi has MGF λ/(λ − t). Their
sum, Y , has MGF
(
)ν
ν
∏
λ
λ
MY (t) =
=
.
λ−t
λ−t
i=1
This was shown to be the MGF of a Gamma (ν, λ) random variable in Example 1.23. Thus, the sum of independent exponential random variables with the
same value of λ follows a gamma distribution.
♢♢♢
Example 1.32 (Normal distribution: sum of independent random variables)
Let Y1 and Y2 be independent normal random variables. Y1 has mean µ1 and
variance σ12 ; similarly, Y2 has mean µ2 and variance σ22 . The MGFs of Y1 and Y2
are available from (1.14). The sum X = Y1 + Y2 has MGF
1

2 2

1

2 2

1

2

2

2

MX (t) = MY1 (t)MY2 (t) = eµ1 t+ 2 σ1 t eµ2 t+ 2 σ2 t = e(µ1 +µ2 )t+ 2 (σ1 +σ2 )t .
This is seen to be the MGF of a normal random variable with mean µ1 + µ2 and
variance σ12 + σ22 . Thus, we have shown that the sum of two independent normal
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-40

CHAPTER 1. PROBABILITY TOOLS

random variables is also normal. By repeating the argument, the obvious extension
of the result holds for the sum of n independent normal random variables.
(The result also holds more generally for n random variables following a multivariate normal distribution, a generalization of the bivariate normal in Section 1.7.9,
even if they are not independent. Their sum or a linear combination of them
has a normal distribution. When computing the variance of the sum or linear
combination via (1.10), the covariance terms will have to be included.)
♢♢♢
Example 1.33 (Casualty insurance: sum of IID geometric random variables)
An actuary models the distribution of the number of months to the first claim
for drivers insured in a particular high-risk rating class. She uses a geometric
distribution, i.e., Geom1 (π), where π is the probability of at least one claim in a
month. A Geom1 (π) random variable, Y , has probability mass function
fY (y) = (1 − π)y−1 π

(y = 1, 2, . . . , ∞; 0 < π < 1),

expectation 1/π, and variance (1 − π)/π 2 . Its MGF is
MY (t) =

et π
1 − (1 − π)et

(see Exercise 1.22 or Table 1.3).
Suppose the actuary is interested in the distribution of the number of months
to the second claim. She assumes that the distribution arises as X = Y1 + Y2 ,
where Y1 and Y2 are independent Geom1 (π) random variables. (This ignores the
possibility of more than one claim in a month.) What is the distribution of X?
From Lemma 1.3,
(
MX (t) = MY1 (t)MY2 (t) =

et π
1 − (1 − π)et

)2
.

This is the MGF of a negative-binomial random variable (again see Exercise 1.22
or Table 1.3) with parameters n = 2 and π, i.e., X ∼ NegBin (2, π).
Table 1.3 gives the PMF of the negative-binomial distribution in general. With
n = 2 and, say, π = 0.1, the first few numerical values of the PMF are given in
Table 1.8.

1.9

Getting It Done in R

In later chapters of this book we have to compute PDFs, PMFs, and CDFs for a
variety of distributions, such as those in Tables 1.3 and 1.4. R can compute these
quantities for all commonly used distributions.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.9. GETTING IT DONE IN R

1-41

( )
x
fX (x) = x−1
(1 − π)x−2 π 2
2−1
2
(2 − 1)(1 − 0.1)0 (0.1)2 = 0.01
3
(3 − 1)(1 − 0.1)1 (0.1)2 = 0.018
4
(4 − 1)(1 − 0.1)2 (0.1)2 = 0.0243
etc.
Table 1.8: Probability mass function of a negative-binomial random variable with
n = 2 and π = 0.1

R function with
Distribution
arguments and defaults
Discrete distributions
Binomial
dbinom(x, size, prob)
Geometric
dgeom(x, prob)
Negative binomial dnbinom(x, size, prob)
Poisson
dpois(x, lambda)
Continuous distributions
Beta
dbeta(x, shape1, shape2)
2
χ
dchisq(x, df)
Exponential
dexp(x, rate = 1)
Fisher’s F
df(x, df1, df2)
Gamma
dgamma(x, shape, rate = 1)
Normal
dnorm(x, mean = 1, sd = 1)
Student’s t
dt(x, df)
Uniform
dunif(x, min = 0, max = 1)

Translation into
our notation
Bin (n = size, π = prob)
Geom0 (π = prob)
See text
Pois (µ = lambda)
Beta (a = shape1, b = shape2)
χ2d=df
Expon (λ = rate)
Fd1 =df1,d2 =df2
Gamma (ν = shape, λ = rate)
N (µ = mean, σ 2 = sd2 )
td=df
Unif (a = min, b = max)

Table 1.9: R functions for the PMF or PDF of some common distributions

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-42

CHAPTER 1. PROBABILITY TOOLS

Table 1.9 lists the R functions to compute the PMF or PDF of the distributions in
Tables 1.3 and 1.4. In all cases, the argument x is the value of a random variable
X. The table also translates the R arguments into our notation for a particular
distribution and its parameter(s). In most cases, R’s syntax and our notation agree
well, and X in Table 1.9 has the same distribution as the random variable Y in
Table 1.3 or 1.4. There are a few exceptions, however.
1. R has no function corresponding to the Geom1 (π) distribution, the version of
the geometric distribution with range y = 1, 2, . . . , ∞. If X is a Geom0 (π)
random variable, then Y = X + 1 is Geom1 (π) random variable, however, and
hence the PMF of Y is given by dgeom(x = y - 1, prob), which could be
called for values y ≥ 1.
2. R’s negative-binomial distribution for X is defined by the PMF
(
)
x+n−1
fX (x) =
(1 − π)x π n (x = 0, 1, . . . , ∞),
x
whereas Y in Table 1.3 has PMF
(
)
y−1
fY (y) =
(1 − π)y−n π n
n−1

(y = n, n + 1, . . . , ∞).

Again, there is a simple relationship, and Y and n + X have the same distribution. Thus, dnbinom(x = y - n, size, prob) for y ≥ n gives our negativebinomial PMF for Y .
In addition, there is no R function for the Bernoulli distribution; it is a special case
of the binomial with n = 1 (or size = 1).
The functions in Table 1.9 are useful for sketching a PMF or PDF. For instance, the
following R code plots fY (y) against y when Y has the t distribution with d = 10
degrees of freedom, to produce Figure 1.4.
# Values of y at which to plot the PDF , namely -5, -4.99 , ... , 5
y <- seq(-5, 5, by = 0.01)
# PDF for t distribution with 10 degrees of freedom
fy <- dt(y, df = 10)
# Plot fy against y using type = "l" to join the points with lines
plot(y, fy , xlab = "y", ylab = expression (f[Y](y)), type = "l")

The last line is perhaps more complicated than necessary, with expression and [Y]
generating the subscript Y in the y-axis label. The simpler syntax
plot(y, fy , xlab = "y", ylab = "f(y)", type = "l")

would often suffice. (Type demo(plotmath) to demonstrate R’s capabilities to format
mathematical expressions in text of plots.)
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-43

0.2
0.0

0.1

fY(y)

0.3

0.4

1.10. LEARNING OUTCOMES

−4

−2

0
y

2

4

Figure 1.4: PDF of the t distribution with 10 degrees of freedom
The functions in Table 1.9 give a PMF or PDF, fY (y), but statistical calculations
often require the CDF, FY (y) = Pr(Y ≤ y). For each PMF or PDF function, which
has a name starting with d, there is a corresponding CDF function, which has a name
starting with p. For instance, ppois(y, lambda) computes the CDF, Pr(Y ≤ y), of
the Poisson distribution.
> ppois (1, lambda = 2)
[1] 0.4060058

Here, > is the R prompt, and recall that lambda is µ in the Pois (µ) distribution of
Table 1.3. The calculation is easily verified:
e−µ µ0 e−µ µ1
+
0!
1!
= e−µ (1 + µ) = e−2 (1 + 2) = 0.4060058.

Pr(Y ≤ 1) = Pr(Y = 0) + Pr(Y = 1) =

Similarly, analogous to dnorm(x, mean, sd), there is pnorm(x, mean, sd), etc.

1.10

Learning Outcomes

On completion of this chapter you should be able to demonstrate the following skills.
They relate to the probability mass function (PMF) or probability density function
(PDF), fY (y), of a random variable, Y .
1. Understand the relationship between a PMF or PDF and its CDF, including
the interpretation of a PDF fY (y) as proportional to the probability that Y is
in a small interval around y.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-44

CHAPTER 1. PROBABILITY TOOLS

2. From a given PMF or PDF of the random variable Y , write down the definition of E(Y ) and Var(Y ). Simplify to a closed-form expression when readily
available.
3. From the PMF or PDF of Y , write down the definition of the expectation of
g(Y ). Simplify to a closed-form expression when readily available.
4. From the mean and variance of Y , apply Chebyshev’s inequality to bound how
far Y can deviate from its mean in a probabilistic sense.
5. From the PDF of a random variable Y , find the PDF of a monotonic function
(transformation) of Y .
6. From the PMF or PDF of a random variable Y , find the expectation of a
function g(Y ).
7. From the joint distribution of two random variables, find their marginal and
conditional distributions.
8. Check whether two random variables are independent or not from their joint
distribution.
9. Find the covariance and correlation between two random variables. Interpret
the correlation.
10. Understand the relationship between covariance and independence.
11. Find the expectation of a linear combination of several random variables from
their individual expectations. As a special case, find the expectation of a linear
function of a random variable from its expectation.
12. Find the variance of a linear combination of several random variables from their
individual variances and their pair-wise covariances. As a special case, find the
variance of a linear function of a random variable from its variance.
13. From the PMF or PDF of Y , find its moment generating function (MGF).
14. Check whether the MGF exists.
15. From the MGF of a random variable, find the mean and variance.
16. From the MGF of a random variable, Y , find the MGF of a + bY .
17. From the (common) MGF of several independent random variables, find the
MGF of their sum.
18. Use the MGF of a random variable to identify its distribution.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.11. EXERCISES

1-45

19. Explain your reasoning. When using a result such as the expectation or variance
of a linear function of a random variable or a linear combination of random
variables as part of a longer derivation, briefly state the result you are using. If
the result depends on an assumption such as statistical independence of random
variables, remind the reader that you are using the assumption.
Perhaps just as important in practice is a list of the tasks that would not be tested
on the quizzes and final exam, or at least will receive less emphasis in grading.
1. There is no doubt that facility with calculus, summation, and algebra is helpful
for the mathematical manipulations in STAT 305. Nonetheless, STAT 305 is a
course in probability and statistical inference, not mathematical manipulation.
Thus, the formulation of, say, an expectation, is at least as important as the
subsequent calculations to implement the final answer.
2. Long proofs involving many steps will not be tested. On the other hand, simple
derivations that can be done in a few lines may appear. Again, it is the demonstration of the use of appropriate probability and statistics tools to carry out
the derivation that is most important.
3. You are not expected to memorize specific PMFs, PDFs, and MGFs. You may
put them on your formula sheet. If a PMF, PDF, or MGF is required to compute
properties stemming from it, it will be given in the question. Similarly, you will
not be asked, e.g., “find the mean of an exponential random variable” if the
mean is required for subsequent calculations. You will be asked “to show that
the mean is 1/λ”.

1.11

Exercises

Exercise 1.1
Let Y be a normal random variable with mean 100 and variance 52 (i.e., standard
deviation 5).
1. Use pnorm in R to compute the following probabilities: Pr(90.0 < Y < 90.1),
Pr(95.0 < Y < 95.1), and Pr(100.0 < Y < 100.1).
2. Use dnorm in R to compute the PDF of Y at the following values: y = 90.05,
y = 95.05, and y = 100.05. (Note that these values are the midpoints of the
intervals in part 1.)
3. Multiply each of the PDF values in part 2 by the width of the intervals used in
part 1. What do you get? Hence, draw a picture illustrating the assertion, “For
a given value of y, the probability that Y falls in a small interval around y is
approximately the PDF computed at y multiplied by the width of the interval.”

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-46

CHAPTER 1. PROBABILITY TOOLS

Exercise 1.2
Let Y be an indicator random variable, i.e., with possible values 0 and 1. Show
E(Y ) = Pr(Y = 1). (Interchanging expectation and probability like this is a frequently used trick for indicator variables.)
Exercise 1.3
In Section 1.3, two definitions were given for the variance of a random variable Y :
Var(Y ) = E(Y − E(Y ))2
and
Var(Y ) = E(Y 2 ) − (E(Y ))2 .
Show that these two expressions are equivalent.
Exercise 1.4
Let Y ∼ Pois (µ).
1. Show that E(Y ) = µ. Hint: You can rewrite the sum in the expectation to
include the factor
∞
∑
µy
= eµ .
y!
y=0
This factor will cancel.
2. Show that Var(Y ) = µ.
Exercise 1.5
Let Y ∼ Geom1 (π).
1. From the definition of expectation, show that E(Y ) = 1/π.
2. From the definition of variance, show that Var(Y ) = (1 − π)/π 2 .
3. Show that the CDF of Y is
Pr(Y ≤ y) = 1 − (1 − π)y .
4. Hence, find the survival function, Pr(Y > y), i.e., the probability that Y exceeds
or “survives” y trials.
5. Suppose it is given that Y exceeds a value y0 . Show that
Pr(Y > y0 + y | Y > y0 ) = Pr(Y > y),
i.e., the probability of surviving at least another y trials does not depend on
the number of trials already survived. (Such a random variable is said to have
a “memoryless” property.)

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.11. EXERCISES

1-47

Exercise 1.6
Let Y ∼ Geom0 (π).
1. Find Pr(Y ≥ 1).
2. Suppose it is given that Y ≥ 1. Show that
Pr(Y = y | Y ≥ 1) = (1 − π)y−1 π

(y = 1, 2, . . .).

3. What is the distribution of Y conditional on Y ≥ 1?
Exercise 1.7
Let Y have an Expon (λ) distribution.
1. What is E(Y )?
2. What is E(Y 2 ) and hence what is Var(Y )?
Exercise 1.8
Show the following properties of the normalizing factor (1.2) of the gamma PDF.
1. Γ(1) = 1.
√
2. Γ( 12 ) = π.
(Manipulation of an integrand to make it have same form as a well-known PDF is a
tactic much used in Section 1.8 to evaluate moment generating functions.)
Exercise 1.9
Let Y have a Gamma (ν, λ) distribution, i.e.,
fY (y) =

1
λ(λy)ν−1 e−λy
Γ(ν)

(0 < y < ∞; ν > 0; λ > 0),

where ν and λ are shape and rate parameters.
Use the result in (1.3) to show that the PDF of Z = 1/Y is
( )ν
1 1 λ
fZ (z) =
e−λ/z (0 < z < ∞; ν > 0; λ > 0).
Γ(ν) z z
This is called an inverse-gamma distribution with shape parameter ν and scale parameter λ.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-48

CHAPTER 1. PROBABILITY TOOLS

Exercise 1.10
Let Y be the lifetime of an item. It has an Expon (λ) distribution.
1. Find the survival function, SY (y) = Pr(Y > y).
2. Suppose at time y0 the item is still alive, and we want to condition on the fact
that Y > y0 . Find Pr(Y > y0 + y | Y > y0 ), the probability of surviving an
additional time y given survival to time y0 .
3. Hence, given that Y > y0 , what is the distribution of the additional lifetime?
Exercise 1.11
Let Y ∼ logN (µ, σ 2 ), i.e., Z = ln(Y ) has a N (µ, σ 2 ) distribution. Using properties of
the normal distribution, show that the the median of Y is eµ .
Exercise 1.12
A mail-order company sends an offer to its population of customers. Let B1 be
a random variable taking the values 0 (does not buy) or 1 (buys) for a randomly
chosen customer. At a later date the company sends out another offer; let B2 be the
analogous 0/1 random variable for the second offering.
The probabilities for the joint distribution of B1 and B2 are given in the following
table.

B1

0
1

B2
0
1
0.3 0.0
0.1 0.6

1. Compute E(B1 ) and E(B2 ).
2. Compute Var(B1 ) and Var(B2 ).
3. Compute Cov(B1 , B2 ).
4. Let B = B1 + B2 be the total number of purchases by a randomly selected
customer. Compute E(B) and Var(B):
(a) by first enumerating the distribution of B (i.e., computing fB (b) for all
values b of B); and
(b) by using results on linear combinations of random variables.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.11. EXERCISES

1-49

Exercise 1.13
For a specific type of property insurance claim, an actuary models the customer’s loss
by the random variable X ∼ Expon (λ). But the particular policy has a limit k on
the amount that the insurance company has to pay. Thus, when a claim is made the
company pays out Y = min(X, k). What is E(Y )?
Exercise 1.14
Let X and Y be continuous random variables with finite expectations, and let a, b,
and c be finite constants. From the definition of expectation, prove the following
results.
1. E(a + X) = a + E(X).
2. E(bX) = bE(X).
3. E(a + bX) = a + bE(X).
4. E(X + Y ) = E(X) + E(Y ).
5. E(a + bX + cY ) = a + bE(X) + cE(Y ).
6. If X and Y are discrete random variables, how are these proofs changed?
Exercise 1.15
Let X and Y be random variables with finite covariance, and let a and b be finite
constants. From the definition of covariance, prove the result
Cov(a + bY, c + dZ) = bdCov(Y, Z)
in (1.11).
Exercise 1.16
Let X and Y be random variables with finite variances, and let a, b, and c be finite
constants. Starting from the definition of variance, i.e., Var(X) = E(X 2 ) − (E(X))2 ,
prove the following results. (Hint: The definition of variance is in terms of expectations; use the results of Exercise 1.14.)
1. Var(a + X) = Var(X).
2. Var(bX) = b2 Var(X).
3. Var(a + bX) = b2 Var(X).
4. Var(X + Y ) = Var(X) + Var(Y ) + 2Cov(X, Y ).
5. Var(a + bX + cY ) = b2 Var(X) + c2 Var(Y ) + 2bcCov(X, Y ). (This is special case
of the more general result in Section 1.7.7.)

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-50

CHAPTER 1. PROBABILITY TOOLS

Exercise 1.17
Let Y1 , . . . , Yn be independent random variables, each taking the values 0 or 1 with
probabilities 1 − π and π, respectively. Here π, the probability that Y = 1, is an
unknown parameter to be estimated.
1. Show that E(Yi ) = π and Var(Yi ) = π(1 − π).
∑
2. Consider the estimator π̃ = n1 ni=1 Yi of π. (This is simply the proportion of
1’s amongst Y1 , . . . , Yn . It is a random variable because the Yi are random.)
(a) Show that E(π̃) = π, i.e., π̃ is an unbiased estimator of π.
(b) Show that Var(π̃) = π(1 − π)/n.
Exercise 1.18
[Quiz #1, 2009-10, Term 1] Let B be a Bernoulli random variable taking values
b = 0, 1. Its PMF is given by fB (0) = Pr(B = 0) = 1−π and fB (1) = Pr(B = 1) = π.
Thus, B ∼ Bern (π).
Show each of the following results. For full marks you need to be explicit about the
mathematical definition of the quantity involved (E(), Var() or MGF) and how the
definition is used for this specific problem.
1. Show E(B) = π.
2. Find E(10B ).
3. Show Var(B) = π(1 − π).
4. Show that the moment generating function (MGF) of B is MB (t) = 1 − π + πet .
5. Check that the MGF exists for t in an open neighbourhood of zero.
6. Use the MGF to find E(B).
Exercise 1.19
[Quiz #1, 2009-10, Term 1] Let Y = B1 + · · · + Bn , where the random variables
B1 , . . . , Bn are independent and each has a Bern (π) distribution. You may use the
results in Exercise 1.18 that Bi has mean π, variance π(1 − π), and MGF 1 − π + πet .
Also n is some fixed number.
1. Find E(Y ).
2. Find Var(Y ).
3. Find the moment generating function of Y .
4. Hence, what is the distribution of Y ?

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1.11. EXERCISES

1-51

Exercise 1.20
Let Y ∼ Pois (µ). Thus, the PMF of Y is
fY (y) =

e−µ µy
y!

(y = 0, 1, . . . , ∞; µ > 0).

1. Show that Y has the MGF
MY (t) = eµ(e −1) .
t

2. Let Y1 , . . . , Yn be independent Poisson random variables, where Yi has mean µi ,
i.e., Yi ∼ Pois (µi ). Thus, the random variables may have different
∑nmeans and
are not necessarily identically distributed. What is the MGF of i=1 Yi ?
∑
3. Hence, what is the distribution of ni=1 Yi ?
Exercise 1.21
Let Y ∼ Unif (a, b). Use the expansion of its MGF in Example 1.29 to show the
following properties:
1. E(Y ) = (a + b)/2; and
2. Var(Y ) = (b − a)2 /12.
Exercise 1.22
Let Y ∼ Geom1 (π). Thus, the PMF of Y is
fY (y) = (1 − π)y−1 π

(y = 1, 2, . . . , ∞; 0 < π < 1).

1. Show that Y has the MGF
MY (t) =

et π
.
1 − (1 − π)et

2. From the MGF show that
E(Y ) =

1
π

and Var(Y ) =

1−π
.
π2

3. ∑
Let Y1 , . . . , Yn be IID Geom1 (π) random variables.
n
i=1 Yi ?
∑
4. Hence, what is the distribution of ni=1 Yi ?

What is the MGF of

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

1-52

CHAPTER 1. PROBABILITY TOOLS

Exercise 1.23
Let Y ∼ N (µ, σ 2 ), i.e., the normal distribution with mean µ and variance σ 2 . This
exercise shows in two ways that the MGF of Y is
1

2 2

MY (t) = eµt+ 2 σ t .
1. Apply the definition of the MGF in Definition 1.7 directly to the N (µ, σ 2 ) PDF
to find the MGF of Y .
2. Let Z have a standard normal distribution, i.e., N (0, 1). Its MGF is
1 2

MZ (t) = e 2 t
(see Example 1.24). Now let Y = µ + σZ.

(a) Verify that E(Y ) = µ and Var(Y ) = σ 2 as required.
(b) Find the MGF of Y from the MGF of Z.
Exercise 1.24
Let Y ∼ N (µ, σ 2 ). Starting from the MGF of Y , i.e.,
1

2 2

MY (t) = eµt+ 2 σ t ,
this exercise verifies the first two moments.
1. Use the MGF to show that E(Y ) = µ.
2. Use the MGF to show that E(Y 2 ) = µ2 + σ 2 , and hence that Var(Y ) = σ 2 .
Exercise 1.25
Let Y ∼ Expon (λ). Consider multiplying Y by a constant to give a new random
variable, Z = bY , where b > 0.
1. Table 1.4 says the MGF of Y is λ/(λ − t). What is the MGF of Z?
2. What is the distribution of Z?
3. Apply the same argument to the gamma distribution to show that if Y ∼
Gamma (ν, λ), then Z = bY ∼ Gamma (ν, λ/b).
Exercise 1.26
A random variable, Y , taking positive values is said to have a log-normal distribution
if Z = ln(Y ) has a N (µ, σ 2 ) distribution. This exercise finds the expectation of Y
from the MGF of Z.
1. The definition of the MGF of Z is E(etZ ). What expression do we get if we put
t = 1 in this definition?
2. Look up the MGF of Z (see Table 1.4 or Exercise 1.23), and put t = 1 in it.
Hence, what is E(Y )?

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-1

Chapter 2
The Normal Distribution in
Statistics
2.1

Introduction

The normal distribution, sometimes called the Gaussian distribution after Gauss, is
ubiquitous in statistical analysis. It will arise in this book in several ways, including
the following.
• Based on a random sample from a N (µ, σ 2 ) distribution, the sample mean is
often used to estimate µ. Exact properties of the normal distribution lead
to a confidence interval for µ that accounts for the uncertainty in estimation,
even if σ 2 is unknown. This analysis based on the t distribution is common in
applications, as typified by Example 2.1.
• If a random sample is taken from a distribution with mean µ and variance σ 2 ,
but the distribution is only approximately normal, use of the t distribution to
provide a confidence interval for µ is often still approximately valid.
• For a large sample size, the normal distribution serves as an approximation to
other distributions. For instance, the binomial distribution is a commonly used
model for applications where estimating a population proportion is the focus.
The error in using the sample proportion as an estimate is again quantified in a
confidence interval, this time based on a normal approximation to the binomial
distribution. More generally, under certain conditions, sample means, proportions, and totals have approximate normal distributions for a large enough
sample size via the central limit theorem (Section 2.5.3).
• The method of maximum likelihood in Chapter 4 is a powerful generic method
to estimate parameters for a wide range of probability models. An approximate
confidence interval for the parameter is often available from an approximate
normal distribution.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-2

2.2

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

Some Properties of the Normal Distribution

Let Y be a normal random variable with mean µ and variance σ 2 . We write Y ∼
N (µ, σ 2 ). The following properties were established using MGFs in Section 1.8 or in
related exercises.
• The expected value (mean) of Y is E(Y ) = µ, and the variance is Var(Y ) = σ 2
(Exercise 1.24).
• A linear function of Y is also normal: a+bY ∼ N (a + bµ, b2 σ 2 ) (Example 1.30).
In particular,
Y −µ
µ 1
Z=
= − + Y ∼ N (0, 1)
σ
σ σ
has the standard normal distribution N (0, 1), i.e., with mean 0 and variance 1
(Exercise 2.1).
Also, for n normally distributed random variables, we have the following properties.
• If the n random variables are independent (but not necessarily identically distributed), Example 1.32 showed that a linear combination of them also has a
normal distribution.
• If the n random variables are correlated but follow a multivariate normal distribution, a linear combination of them still has a normal distribution. The
covariances between the n variables affect only the variance of the linear combination, via (1.10).
• If the n random variables follow a multivariate normal distribution and all the
pairwise covariances between them are zero, then they are mutually independent. This result is a generalization of Lemma 1.1, which said that if Y and Z
are bivariate normal with Cov(Y, Z) = 0, then Y and Z are independent.

2.3

Distributions Derived From the Normal

The normal distribution is important in itself and because other important distributions arise from it. The relationships among these distributions are summarized in
Figure 2.1. We next give some details about these distributions.

2.3.1

The χ2 distribution

A χ2 (“chi-squared”) random variable is generated by the sum of squares of independent standard normal random variables.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.3. DISTRIBUTIONS DERIVED FROM THE NORMAL

2-3

Standard normal distribution
Z ∼ N (0, 1)
Z1 , . . . , Zd ∼ independent N (0, 1)

SSS
SSS
SSS
SSS
SS)

t distribution
If Z and Xd are independent
√ Z ∼ td



5
lll
lll
l
l
lll
lll

Xd /d

χ2 (Chi-squared) distribution
Z 2 ∼ χ21
2
Xd = Z1 + · · · + Zd2 ∼ χ2d


F distribution
Xd1 ∼ χ2d1
Xd2 ∼ χ2d2
If Xd1 and Xd2 are independent
Xd1 /d1
∼ Fd1 ,d2
Xd2 /d2

Figure 2.1: Relationships between distributions derived from the normal

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS
1.2

2-4

0.0

0.2

0.4

f(y)
0.6

0.8

1.0

1 df
3 df
5 df
10 df

0

5

10
y

15

20

Figure 2.2: PDF of the χ2 distribution with 1, 3, 5, and 10 degrees of freedom
Lemma 2.1 (χ2 distribution)
If Z1 , . . . , Zd are independent N (0, 1) random variables, then their sum of
squares,
Xd = Z12 + · · · + Zd2 ∼ χ2d ,
has a χ2 distribution with d degrees of freedom, which we write as χ2d .
A proof is developed in Exercise 2.6.
Figure 2.2 plots the PDF of the χ2d distribution for d = 1, 3, 5, and 10. With d = 1
or d = 2 degrees of freedom, the PDF is monotonic decreasing. For d ≥ 3, the PDF
increases then decreases. For large d the shape of the distribution is approximately
normal; this is starting to be evident for d = 10 in Figure 2.2. The limiting normality
stems from the central limit theorem (Theorem 2.2), because of the sum in Lemma 2.1.
The χ2 distribution is related to the gamma distribution.
Let G be a
Gamma (ν = d/2, λ = 1) random variable (see Table 1.4), where d > 0 is an integer. Then X = 2G is a χ2d random variable.
The χ2 distribution arises in inference about the variance of a normal distribution in
Section 2.4.2. It is also heavily used for hypothesis testing in Chapters 7–9.

2.3.2

The t distribution

The following lemma is due to W.S. Gosset, who wrote under the pseudonym “Student”.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-5

0.4

2.3. DISTRIBUTIONS DERIVED FROM THE NORMAL

0.0

0.1

f(y)
0.2

0.3

1 df
3 df
10 df
Normal

−6

−4

−2

0
y

2

4

6

Figure 2.3: PDF of the t distribution with 1, 3, 10, and ∞ degrees of freedom; ∞
degrees of freedom gives the standard normal
Lemma 2.2 (t distribution (Student, 1908))
If Z ∼ N (0, 1), Xd ∼ χ2d , and Z and Xd are independent, then
Z
√
∼ td ,
Xd /d
where td is Student’s t distribution with d degrees of freedom.
Gosset’s motivation for the lemma was the distribution of the sample mean of n IID
N (µ, σ 2 ) standardized using the sample variance as an estimator of σ 2 , leading to
confidence intervals and hypothesis tests for µ based upon the t distribution (Section 2.4.3). Contrary to Gosset’s modest dismissal of his contribution in comments
made to R.A. Fisher, the result is one of the most commonly applied in all of the
statistical sciences.
Figure 2.3 plots the td PDF for d = 1, 3, and 10. The PDF has a bell shape similar to
the normal, but has wider tails than the normal. As d → ∞, the td PDF approaches
the standard normal PDF. With 1 degree of freedom, the distribution is known as
the Cauchy distribution.
√
An insightful way to prove Lemma 2.2 is to first write Z/ Xd /d in the lemma as a
mixture of normal random variables. Define
Z
Xd
Z
=√ ,
W =
and T = √
d
W
Xd /d
where T is the variable of interest and W appears in its denominator. (We write T
following our convention of using upper case letters for random variables, whereas
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-6

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

many textbooks use t for the random variable and T for Hotelling’s T 2 statistic.)
Their joint distribution can be written in terms of the conditional distribution of T
given W :
fT,W (t, w) = fT |W (t | w)fW (w)
(see Section 1.7.2). The key simplification here is that conditional on W taking the
value w, the variable T of interest becomes
(
)
Z
1
√ ∼ N 0,
,
w
w
i.e., fT |W (t | w) is simply the normal PDF with variance 1/w or √
standard deviation
√
1/ w. Here we are using the assumed independence of Z and W in the lemma,
so that Z still has a normal distribution conditional on W√. Once we condition on
W = w, there is a constant, not random, divisor in Z/ w, i.e., a simple linear
transformation of a normal random variable, which is also normal. Similarly, W is a
trivial rescaling of a χ2d random variable, and fW (w) is easily derived. The marginal
distribution of T then follows by integrating out W from the joint distribution (see
Section 1.7.1):
∫ ∞
∫ ∞
fT (t) =
fT,W (t, w) dw =
fT |W (t | w)fW (w) dw.
0

0

Written this way, the td PDF is an average or mixture of normal PDFs, averaged with
respect to the distribution of W .
The second step is to carry out the integration. Readers interested in the formal details can find them in the Appendix of Section 2.9, but it is perhaps more instructive
to demonstrate the averaging of normal random variables graphically. For definiteness, take d = 3 degrees of freedom. Furthermore, we will approximate the continuous
values of Xd=3 and hence W by just five representative values. Figure 2.4(a) shows
the χ23 PDF of X3 . The distribution is divided into five equal probabilities by the
dotted lines. These sub-intervals in the figure are represented by the numbers 0.6,
1.4, etc. For instance, the value 0.6 cuts off a probability of 0.1 to the left, and in this
sense it is in the centre of the first interval of probability 0.2. The five representative
values are obtained in R using the function qchisq.
> # Quantiles of X_3 cutting off probs 0.1 , 0.3 , etc. to the left
> x3 <- qchisq (c(0.1 , 0.3 , 0.5 , 0.7 , 0.9) , df = 3)
> x3
[1] 0.5843744 1.4236522 2.3659739 3.6648708 6.2513886

As W = X3 /3 is a monotonic increasing function of X3 , we have 0.1 = Pr(X3 <
0.5843744) = Pr(X3 /3 < 0.5843744/3), etc., and the five representative values of W
are found by simple arithmetic.
> # Quantiles of W cutting off probs 0.1 , 0.3 , etc. to the left
> w <- x3 / 3
> w
[1] 0.1947915 0.4745507 0.7886580 1.2216236 2.0837962
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.3. DISTRIBUTIONS DERIVED FROM THE NORMAL

(b) t distribution with 3 df as mixture of normals

f(y)
0.3
0.6

0

1.4

2.4

2

3.7

0.0

0.00

0.1

0.05

0.2

0.10

f(y)

0.15

0.4

0.5

0.20

0.6

0.25

(a) Chi squared distribution with 3 df

2-7

6.3

4

6

8

−4

−2

y

0
y

2

4

Figure 2.4: (a) χ23 PDF, with the distribution divided into five subintervals of probability 0.2 each by the dashed lines. Each subinterval is represented by a single value.
(b) The t3 PDF shown as a solid line is to a good approximation given by the average
of the five normal PDFs shown as dashed lines.
√
Finally, each representative value, w, leads to a standard deviation of 1/ w in the
normal distribution.
> # Standard deviation of conditional normal
> sd.norm <- 1/ sqrt(w)
> sd.norm
[1] 2.2657659 1.4516391 1.1260448 0.9047556 0.6927434

Figure 2.4(b) shows five normal PDFs as dashed lines with these five standard deviations. Note that the conditional normal distributions have considerable variation in
their standard deviations. Visually averaging these five PDFs gives a curve that is
hard to distinguish from the t3 PDF shown by a solid line in the figure.
The t distribution will arise when we make inference about the mean, µ, of a normal
distribution and the variance, σ 2 , also has to be estimated.

2.3.3

The F distribution

The F distribution arises from the ratio of two independent χ2 distributions.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

0.8

1.0

2-8

0.0

0.2

0.4

f(y)

0.6

(2, 2) df
(2, 10) df
(3, 2) df
(3, 10) df

0

1

2

3
y

4

5

6

Figure 2.5: PDF of the F distribution with (2, 2) (2, 10), (3, 2), and (3, 10) degrees
of freedom
Lemma 2.3 (F distribution)
If Xd1 ∼ χ2d1 , Xd2 ∼ χ2d2 , and Xd1 and Xd2 are independent, then
Xd1 /d1
∼ Fd1 ,d2 ,
Xd2 /d2
where Fd1 ,d2 is an F distribution with d1 and d2 degrees of freedom.
Figure 2.5 plots the PDF of Fd1 ,d2 for various values of d1 and d2 . With d1 = 1
or 2, the PDF is monotonic decreasing. For d1 > 2 the distribution increases then
decreases.
The F distribution is used to make inference when comparing two variances. It is
also very important in inference about the parameters of linear regression models
(STAT 306).

2.4

Estimating the Parameters of the Normal Distribution

Let Y1 , . . . , Yn be independent N (µ, σ 2 ) random variables. Such variables would arise
if each is an independent draw from the same normal distribution. Equivalent to this
probability model, a statistician might say Y1 , . . . , Yn are assumed to be a random
sample of size n from an N (µ, σ 2 ) distribution. Here, the statistician has in mind an
infinite—in practice, large—population of values taken to be normal.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.4. ESTIMATING THE PARAMETERS OF THE NORMAL

2-9

Typically, both µ and σ 2 are unknown, and these quantities are estimated by the
sample mean and sample variance, respectively.

2.4.1

Distribution of the sample mean (known variance)

Suppose we use the sample mean to estimate µ. What are its statistical properties?
The sample mean is

1∑
Ȳ =
Yi .
n i=1
n

Clearly, Ȳ is a linear combination of random variables. Hence,
n
n
1∑
1∑
E(Ȳ ) =
E(Yi ) =
µ=µ
n i=1
n i=1

and

1 ∑
1 ∑ 2 σ2
Var(Yi ) = 2
σ = .
Var(Ȳ ) = 2
n i=1
n i=1
n
n

n

The distribution of Ȳ will be exactly normal under the assumptions we are making
or often approximately normal:
• The assumptions at the beginning of Section 2.4 included normality of Y1 , . . . ,
Yn . As a linear combination of normal random variables is normal (Example 1.32), Ȳ is also normal. We can summarize by saying
(
)
Ȳ ∼ N µ, σ 2 /n ,
or equivalently if we standardize Ȳ for its mean and variance,
Ȳ − µ
√
∼ N (0, 1).
σ 2 /n

(2.1)

• Even if Y1 , . . . , Yn are not normal, the CLT will often apply. The left-hand side
of (2.1) is the same as the sample-mean version of the CLT in (2.10). Thus,
if the conditions of the CLT hold, the left-hand side of (2.1) will converge in
distribution to N (0, 1) as n → ∞ even if the Yi are not normal.
If we know σ 2 we are ready for statistical inference (confidence intervals, hypothesis
tests) for µ based on the normal distribution. Unfortunately, in practice, σ 2 is also
usually unknown and we need to estimate it by the sample variance.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-10

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

2.4.2

Distribution of the sample variance

The sample variance is
n
1 ∑
1
2
(Yi − Ȳ )2 =
S =
n − 1 i=1
n−1

( n
∑

)
Yi2 − nȲ 2

.

(2.2)

i=1

The equivalence of the two formulas for computing S 2 is seen by multiplying out the
square in the first formula and collecting terms.
It is fairly straightforward to show that dividing by n − 1 (and not n) makes S 2 an
unbiased estimator of σ 2 , i.e.,
E(S 2 ) = σ 2 .
This is proved as Exercise 2.9. The proof only requires that Y1 , . . . , Yn are independent
with mean µ and variance σ 2 , i.e., it does not depend on having a normal distribution.
Finding the distribution of S 2 is more challenging in general but approachable when
the Yi are IID normal. In the first definition of S 2 in (2.2), write
(
)
Yi − µ Ȳ − µ
Yi − Ȳ = σ
−
= σ(Zi − Z̄),
σ
σ
i.e., standardize Yi to Zi with a standard normal distribution. Then
S2 =

n
n
1 ∑
1 ∑
(Yi − Ȳ )2 = σ 2
(Zi − Z̄)2 ,
n − 1 i=1
n − 1 i=1

or

n
1 ∑
S2
=
(Zi − Z̄)2 .
σ2
n − 1 i=1
∑
Thus, the distribution of S 2 is that of ni=1 (Zi − Z̄)2 up to constants.
∑
The following theorem shows that the distribution of ni=1 (Zi − Z̄)2 is χ2n−1 , i.e., the
degrees of freedom are n−1 and not n. Each observation is “corrected” for the sample
mean, i.e., Yi − Ȳ or Zi − Z̄. This is because the mean, µ, of the normal distribution
is unknown and has to be estimated; the estimation of this one parameter leads to a
correction of the degrees of freedom by 1.

Theorem 2.1 (χ2 corrected degrees of freedom)
Let Z1 , . . . , Zn be independent N (0, 1) random variables. Then
n
∑

(Zi − Z̄)2 ∼ χ2n−1 ,

i=1

i.e., χ2 with n − 1 degrees of freedom.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.4. ESTIMATING THE PARAMETERS OF THE NORMAL
Proof. Write

n
∑
i=1

Zi2 =

n
∑

(Zi − Z̄)2 + nZ̄ 2 .

2-11

(2.3)

i=1

(This is just a restatement of the equivalent formulas in (2.2) with Z instead of Y .)
Then use an argument based on MGFs.
∑
1. On the right of (2.3), denote by M (t) the MGF of ni=1 (Zi − Z̄)2 . This sum
is the random variable of interest in Theorem 2.1. By finding its MGF, we will
identify its distribution.
√
2. In the second term on the right of (2.3), Z̄ ∼ N (0, 1/n), so that nZ̄ ∼ N (0, 1).
Hence, nZ̄ 2 ∼ χ21 with MGF (1 − 2t)−1/2 (see Exercise 2.3).
3. We also note that Zi − Z̄ and Z̄ are independent (use the result of Exercise 2.8
in the special case of Z with µ = 0 and σ 2 = 1 instead of Y ). Thus, the two
terms on the right of (2.3) are independent and by Lemma 1.3 their sum has
MGF M (t) × (1 − 2t)−1/2 .
∑
4. On the left of (2.3), Lemma 2.1 says that ni=1 Zi2 has a χ2n distribution with
MGF (1 − 2t)−n/2 .
5. Hence, equating the MGFs of the left and right of (2.3),
(1 − 2t)−n/2 = M (t) × (1 − 2t)−1/2 ,
and M (t) = (1 − 2t)−(n−1)/2 , which is the MGF of a χ2n−1 random variable.
The consequence of Theorem 2.1 is that S 2 /σ 2 has the same distribution as that of a
χ2n−1 random variable divided by n − 1.

2.4.3

Distribution of the standardized sample mean (unknown variance)

At the end of Section 2.4.1 we looked ahead to making statistical inference about the
mean µ based on the sample mean, Ȳ . We noted that in practice σ 2 will have to be
estimated in the standardized sample mean,
Ȳ − µ
√
,
σ 2 /n
in (2.1). The obvious strategy is to use the sample variance, S 2 , as it is an unbiased
estimator of σ 2 . The standardized mean becomes
Ȳ − µ
√
.
S 2 /n
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-12

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

Replacing σ 2 by S 2 will change the standard normal distribution on the right of (2.1).
It becomes a tn−1 distribution. To see this, we write
√
(Ȳ − µ)/ σ 2 /n
Ȳ − µ
√
√
=
.
(2.4)
S 2 /n
S 2 /σ 2
We then argue as follows about the numerator and denominator of this expression.
√
• The numerator, (Ȳ − µ)/ σ 2 /n is back to the random variable in (2.1) and
therefore has the same distribution as Z ∼ N (0, 1).
√
• The denominator is S 2 /σ 2 . At the end of Section 2.4.2, we concluded that
S 2 /σ 2 has the same distribution as that of a χ2n−1 random variable
divided by
√
n − 1. Thus, the denominator has the same distribution as Xn−1 /(n − 1),
where Xn−1 ∼ χ2n−1 .
• The random variables in the numerator and denominator are Ȳ and S 2 , respectively. Now, S 2 is a function of Yi − Ȳ (i = 1, . . . , n), and Exercise 2.8 shows
that Ȳ and Yi − Ȳ are independent. Therefore, Ȳ and S 2 are independent, and
the numerator and denominator of (2.4) are independent.
These properties of the numerator and denominator of 2.4 are those leading to the t
distribution in Lemma 2.2. Thus,
Ȳ − µ
√
∼ tn−1 .
S 2 /n

(2.5)

In Lemma 2.2, the degrees of freedom are given by d = n − 1, because we related
S 2 /σ 2 to χ2n−1 . Thus, we will use the tn−1 distribution for inference about the mean
of a normal distribution when the variance is unknown. This is the result derived
by Student (1908), which was motivated by his analysis of experimental results at
Guinness Breweries in Dublin.
Example 2.1 (Lung function: confidence interval for the normal mean)
Schlaich et al. (1998) conducted a study on reduced pulmonary (i.e., lung) function
in patients with spinal osteoporosis (“manifest osteoporosis”). Their objective was
to compare pulmonary function between patients with this manifest osteoporosis
and patients without the condition. For now we will consider only the manifest
osteoporosis data here.
The measure of lung function was forced expiratory volume in 1 second (FEV1).
The raw measure is adjusted for sex, age, and body height, leading to a percentage
(FEV1%) relative to a standard, called y below. (The calculations in this example
are based on data adjusted for current body height.) Data for n = 34 patients with
manifest osteoporosis were collected, which we will treat as a random sample from
a larger population of interest. The authors checked that the data are consistent
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.4. ESTIMATING THE PARAMETERS OF THE NORMAL

2-13

with arising from a normal distribution. They were interested in estimating the
mean, µ, of the normal distribution when σ 2 is unknown.
The data summaries for the sample of size 34 are:
ȳ = 94.3

and s = 14.7,

where s is the sample standard deviation.
The estimate of µ is ȳ = 94.3 here. How much error could there be in this estimate
just by chance due to random sampling? The analysis Schlaich et al. conducted
was based on the exact t distribution in (2.5), i.e.,
Ȳ − µ
√ ∼ tn−1 .
S/ n
Given 0 < α < 1, by definition the quantiles tn−1,α/2 and tn−1,1−α/2 cut off a
probability of α/2 in each tail of the tn−1 distribution, and
)
(
Ȳ − µ
√ < tn−1,1−α/2 = 1 − α.
Pr tn−1,α/2 <
S/ n
Rearrangement gives
(
)
S
S
Pr Ȳ + tn−1,α/2 √ < µ < Ȳ + tn−1,1−α/2 √
= 1 − α,
n
n
which is a probabilistic bound on µ. Figure 2.6 illustrates.
Note that the random variables here are the estimators µ̃ = Ȳ and σ̃ = S. When
we replace them by the numerical estimates ȳ and s from the data, we have a
100(1 − α)% confidence interval for µ:
s
s
ȳ + tn−1,α/2 √ < µ < ȳ + tn−1,1−α/2 √ .
n
n
(Chapter 3 develops the distinction between estimators and estimates.) Because
the t distribution is symmetric, tn−1,α/2 = −tn−1,1−α/2 .
With n = 34 as in the Schlaich et al. study, and α = 0.05 for 95% confidence,
qt(0.975, df = 33) in R gives tn−1,1−α/2 = t33,0.975 = 2.035. A 95% confidence
interval for µ is therefore
s
14.7
ȳ ± tn−1,0.975 √ = 94.3 ± 2.035 √ = 94.3 ± 5.1 = [89.2, 99.4]%.
n
34
(The units of the interval, percentage points, are the same as for the FEV1%
measurements.)
♢♢♢

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

0.1

f(t)
0.2

0.3

0.4

2-14

1−α

0.0

α 2
−4

−3

−1
tn−1,α 2

0
t

α 2
1

3

4

tn−1,1−α 2

Figure 2.6: Quantiles of the t distribution with n−1 degrees of freedom. The quantiles
tn−1,α/2 = −tn−1,1−α/2 and tn−1,1−α/2 cut off the probability α/2 in each tail. The
PDF shown has n − 1 = 33 degrees of freedom as in the Schlaich et al. study, and the
quantiles shown are for α = 0.05, leading to a 2-sided 95% confidence interval.

2.5

Limiting Normal Distributions

2.5.1

Convergence in distribution

Definition 2.1 (Convergence in distribution)
Let X1 , X2 , . . . be a sequence of random variables with CDFs FX1 (x),
FX2 (x), . . ., respectively. Suppose there exists a CDF FX (x) such that
lim Pr(Xn ≤ x) = FX (x),

n→∞

for all x where FX (x) is continuous. Then we say that the sequence of random
variables Xn converges in distribution to FX (x).
The random variables X1 , X2 , . . . here are not the individual elements of a sample.
Rather, Xn will be based on a summary like the sample mean of the entire sample.
In the next example, we work with a standardized version of the sample total. The
example also demonstrates that the limiting distribution may be found from the
limiting MGF.
Example 2.2 (Binomial distribution: normal approximation)
Let Xn ∼ Bin (n, π) with PMF
( )
n x
fXn (x) =
π (1 − π)n−x (x = 0, 1, . . . , n).
x
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.5. LIMITING NORMAL DISTRIBUTIONS

2-15

We will show that a standardized version of Xn has a distribution that converges
to the standard normal distribution as n → ∞.
We want the limiting distribution of Xn as n → ∞. But the limit cannot possibly
be a fixed distribution. We know that E(Xn ) = nπ and Var(Xn ) = nπ(1 − π);
both are changing (increasing) with n. But
Xn − E(Xn )
Xn − nπ
Zn = √
=√
Var(Xn )
nπ(1 − π)

(2.6)

has mean 0 and variance 1. It is the distribution of this standardized quantity
that converges to a fixed distribution, namely the standard normal.
The key steps are: (1) to find the MGF of Zn in (2.6); and (2) to show that the
MGF tends to that of the standard normal as n → ∞.
Derivation of the MGF of Zn is based on Lemma 1.2 for the MGF of a linear
function of a random variable. Write Zn in (2.6) as Zn = an + bn Xn , where
√
π n
1
an = −
and bn = √ ,
c
c n
√
and c = π(1 − π) to simplify notation. From Table 1.3 the MGF of the binomial
random variable Xn is
MXn (t) = (1 − π + πet )n .
Applying Lemma 1.2 gives
√

an t

MZn (t) = e

−πcnt

MXn (bn t) = e

(

1 − π + πe

c

1
√
t
n

)n
,

which can be rearranged as
))n (
)n
( π (
(1−π)
1
π
√
√ t
t
t
− √ t
− √
.
= (1 − π)e c n + πe c n
e c n 1 − π + πe c n
This completes the first step.
To find the limiting MGF of Zn as n → ∞, first make Taylor series expansions of
the exponential functions in MZn (t):
(
(
(
))
π2 2
π
1
MZn (t) =
(1 − π) 1 − √ t + 2 t − O
2c n
n3/2
c n
(
)))n
(
(1 − π)2 2
1
1−π
t +O
+ π 1+ √ t+
.
2c2 n
n3/2
c n
The MGF here is a function of t and n, but for any fixed value of t, we are interested
in the behaviour as n → ∞. The notation O(1/n3/2 ) represents further terms that
are decreasing at rate 1/n3/2 or faster. Technically, if m(t, n) is the sum of all
terms after the t2 term, then m(t, n) is O(1/n3/2 ) says that |m(t, n)| < a(t)/n3/2
for some positive constant a(t) and sufficiently large n. Thus, m(t, n) gives a
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-16

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

√
negligible contribution for large n, providing the leading terms in 1/ n and 1/n
do not both cancel. Collecting terms of order t0 , t1 , and t2 gives
(
(
))n
(1 − π)π 2 + π(1 − π)2 2
1
MZn (t) = 1 +
t +O
,
2c2 n
n3/2
which further simplifies to
(
(
))n
1 t2
1
MZn (t) = 1 +
+O
,
n2
n3/2
√
recalling that c = π(1 − π). As n → ∞, the O(1/n3/2 ) term can be ignored,
2
and (1 + (t2 /2)/n)n → et /2 (recall that (1 + x/n)n → ex as n → ∞). Thus,
2

MZn (t) → et /2

as n → ∞,

i.e., the limiting MGF is that of the standard normal (see Table 1.4). Theorem 1.3
says that the MGF uniquely identifies a distribution, and hence the distribution
of Zn converges to the standard normal distribution as n → ∞.
♢♢♢

2.5.2

Limiting distributions and large-sample approximations in statistics

Results on limiting distributions like the result in Example 2.2 show convergence
in distribution for a standardized version of a random variable. The sample size,
n, becomes infinitely large. In statistical practice, however, sample sizes are finite.
Furthermore, to a statistician the statistic of interest is often an unstandardized
random variable. Limiting distributions of standardized random variables justify the
use of approximate distributions for finite samples and unstandardized quantities.
For instance, let Xn be a sample from a Bin (n, π) distribution. The sample proportion, Xn /n, is often be used to estimate π. This is not the standardized random
variable Zn in Example 2.2. We can write Xn /n in terms of Zn , however, by rearranging (2.6):
√
Xn = nπ + nπ(1 − π)Zn ,
and hence

√
π(1 − π)
Xn
=π+
Zn .
n
n
For a finite sample, Zn has an approximately standard normal distribution, and the
theory says that the approximation will become better as the sample size increases.
Furthermore, Xn /n is a linear function of Zn , and using the result that a linear
function of a normal random variable also has a normal distribution (Example 1.30),
Xn /n has the following approximate distribution:
(
)
π(1 − π)
Xn
∼ N π,
.
(2.7)
n
n
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.5. LIMITING NORMAL DISTRIBUTIONS

2-17

This is the argument for quantifying how accurate Xn /n is as an estimator of π in a
statistical sense.
Example 2.3 (Opinion polls: margin of error (confidence interval))
Opinion polls are typically conducted with sample sizes like n = 1000 or n =
3000 from a large population such as all adult Canadians. Often, the results will
be qualified with a statement like, “These results have a margin of error of 3
percentage points 19 times out of 20.” Let’s see how such a statement can be
justified.
For definiteness, take the Nanus poll taken in September 2016 commissioned by
Clean Energy Canada (available at
http://cleanenergycanada.org/wp-content/uploads/2016/09/
Clean-Energy-Canada-Nanos-Climate-Policy-Polling-Report-Oct-2016.
pdf). It asked 1000 randomly selected Canadian adults a number of questions
about climate change. The findings included:
• 48% agreed with the statement, “A changing climate presents a significant
threat to our economic future” (and a further 23% somewhat agreed).
• 33% supported, “Having a price on carbon to reduce the use of fossil fuels
such as coal, oil or natural gas” (and a further 26% somewhat supported
the statement).
In the methodology section of the report, we find, “The margin of error is ±3.1
percentage points 19 times out of 20.” The margin of error is a measure of sampling
variability. Thus, to check the calculation, we treat the sample as a random sample
from a large population. Then, the number of people agreeing with a particular
statement (e.g., the question about significant threat to our economic future) is a
binomial random variable with n = 1000 (the sample size here) and probability π.
The parameter π represents the unknown proportion of people in the population
who agree with this particular statement. The Nanos poll was stratified by age,
gender, and region and not a simple random sample, which implies our binomial
model probability is oversimplified. Nonetheless, the impact on the margin of
error calculation is usually small.
The approximate distribution of Xn /n in (2.7) implies that approximately
(
)
π(1 − π)
Xn
− π ∼ N 0,
.
n
n
On the left is the error in estimating π by Xn /n. The normal distribution has
the property that 95% of the probability (“19 times out of 20”) lies within ±1.96
standard deviations of the mean. Thus, the error is within
√
π(1 − π)
(2.8)
±1.96
n
approximately 19 times out of 20.
There are two further practical difficulties in completing the computation.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-18

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS
• What value should be used for π in the error calculation? Recall that the
purpose of the poll is to estimate π, which is unknown.
• Most polls, like the Nanos poll, ask several questions. Each question could
have a different true value of π. It would be overly complicated to give a
different error calculation for every question.

To overcome both of these difficulties, pollsters will often report (2.8) for π = 0.5.
This is the value of π that maximizes the variance in the approximate normal
distribution of the error and hence gives the widest, worst-case bounds on the
margin of error.
Returning to the Nanos poll, with n = 1000 the worst-case margin of error
from (2.8) is
√
0.5(1 − 0.5)
±1.96
= ±1.96(0.0158) = ±0.031,
1000
i.e., 3.1%, which is the margin of error reported.

♢♢♢

Error bounds like those in Example 2.3 are also called confidence intervals and are
tackled more generally in Section 4.6.

2.5.3

Central limit theorem

The normal approximation to the binomial distribution is just a special case of the
central limit theorem (CLT).
Theorem 2.2 (Central limit theorem (CLT))
Let Y1 , Y2 , . . . be a sequence of IID random variables with mean µ, variance
σ 2 , and MGF defined in a neighbourhood of zero. Define the sum
Xn =

n
∑

Yi .

i=1

The standardized sum,
Zn =

Xn − nµ
√
,
σ n

(2.9)

converges in distribution to N (0, 1).
Referring back to Definition 2.1 for convergence in distribution, the CLT says that
the CDF of Zn approaches that of the standard normal as n → ∞, i.e., Pr(Zn <
z) → Pr(Z < z), where Z ∼ N (0, 1). This is sufficient for statistical purposes: in
Example 2.3, for instance, the limits for the 95% confidence interval use z0.975 = 1.96,
a quantile of the standard normal defined by the CDF. Note, however, that the speed
of convergence will be slower in the extreme tails of the Zn distribution.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.6. GETTING IT DONE IN R

2-19

We can also express the CLT in terms of the arithmetic mean, Ȳn = Xn /n. Dividing
the top and bottom of (2.9) by n, we have
Zn =

Ȳn − µ
√
σ/ n

(2.10)

converges in distribution to N (0, 1). We can use either form depending on whether
its easier to work with a sum or mean of random variables.
The proof of the CLT, given in Section 2.10, is based on a generalization of the
strategy in Example 2.2 for the binomial distribution.
Example 2.4 (Binomial distribution: normal approximation via CLT)
In Example 2.2 the normal approximation to the binomial distribution was developed by working directly with the binomial MGF. The CLT, a more general
result, gives the same limiting normal approximation almost immediately.
Let B1 , . . . , Bn be n independent Bern (π) random variables. Then
Xn =

n
∑

Bi

i=1

has the distribution Xn ∼ Bin (n, π) (Exercise 1.19). We have expressed the
binomial random variable as a sum, so the sum version of the CLT in (2.9) is
more convenient. As E(Bi ) = π and Var(Bi ) = π(1 − π), the CLT says that
X − nπ
√ n
nπ(1 − π)
converges in distribution to the standard normal. This is the result obtained in
Example 2.2.
♢♢♢

2.6

Getting It Done in R

2.6.1

Sample mean, standard deviation, and variance

The functions mean, sd, and var are useful for obtaining the sample mean, standard
deviation, and variance, respectively.

2.6.2

Quantiles of the t distribution

R has functions to compute quantiles for commonly used distributions. Corresponding
to the functions in Table 1.9 with names starting with d for PMFs and PDFs, names
starting with q compute quantiles.
For instance, to compute a confidence interval in the lung-function study of Example 2.1, quantiles of the t distribution are computed by qt.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-20

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

> qt(p = 0.025 , df = 33)
[1] -2.034515
> qt(p = 0.975 , df = 33)
[1] 2.034515

Here, p is the probability to the left of the quantile, and there are 33 degrees of
freedom because n = 34 in the particular study.
Whereas pt(y, ...) returns a CDF probability for a given value or quantile of a
random variable with a t distribution, the function qt(p, ...) returns a quantile for
a given probability. Thus, the quantile function is the inverse cumulative distribution
function. In general, for given probability p, the quantile function returns y such
that Pr(Y ≤ y) = FY (y) = p, or equivalently y = F −1 (p), where F −1 is the inverse
function of the CDF.

2.6.3

Limiting normal distributions

Similarly, in Example 2.3 we needed quantiles of the standard normal distribution,
which are available from qnorm.
> qnorm (p = 0.025)
[1] -1.959964
> qnorm (p = 0.975)
[1] 1.959964

Quantiles of the normal distribution with non-standard mean and variance are obtained via the arguments mean and sd of qnorm.

2.7

Learning Outcomes

On completion of this chapter you should be able to demonstrate the following skills.
1. Normal distribution
(a) Use the MGF to derive the mean, variance, and distribution of a linear
function of a normal random variable. (You can interpret “derive” here as
justify via an appropriate result.)
(b) Derive the mean, variance, and distribution of a linear combination of
normal random variable.
(c) Standardize a normal random variable to have a standard normal distribution.
2. χ2 distribution

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.7. LEARNING OUTCOMES

2-21

(a) Derive the MGF of the χ21 distribution.
(b) Show via the MGF that the square of a standard normal random variable
has a χ21 distribution.
(c) Find the MGF of the χ2d distribution.
(d) Show that the sum of squares of d independent standard normal random
variables has a χ2d distribution.
3. Random sample from a normal distribution
Let Y1 , . . . , Yn be a random sample of independent N (µ, σ 2 ) random variables.
(a) Derive the mean and variance of the sample mean, Ȳ .
(b) Write down unstandardized and standardized distributions of Ȳ when σ 2
is known.
(c) Write down the distribution of the sample variance, S 2 .
(d) Show that the sample mean and sample variance are statistically independent.
(e) Write down a standardized version of Ȳ when σ 2 is unknown. Argue
that the standardized random variable has the properties leading to a t
distribution with specified degrees of freedom.
(f) Use the t distribution to compute a confidence interval for µ when σ 2 is
unknown for given data.
4. Central Limit Theorem
(a) Apply the CLT to establish convergence in distribution to the standard
normal for a random variable with a specified distribution (e.g., binomial).
Included here is the formulation of the random variable as a sample mean
or sample sum, standardizing it, and checking the conditions of the CLT.
A detailed proof of the theorem is not expected.
(b) Under a binomial probability model, convert the standard normal distribution of a standardized version of the sample proportion as n → ∞ to an
approximate normal distribution of the unstandardized sample proportion
and a finite sample size.
(c) Compute the “margin of error” in estimating the parameter π in the binomial distribution for given data.
5. Explanation
Explain your reasoning by describing the results you are using, along with any
assumptions that are necessary.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-22

2.8

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

Exercises

Exercise 2.1
Let Y ∼ N (µ, σ 2 ). Show that the standardized random variable Z = (Y − µ)/σ has
a N (0, 1) distribution.
Exercise 2.2
Let Y1 , . . . , Yn be independent N (µ, σ 2 ) random variables.
1. Write down the MGF of Yi .
2. Derive the MGF of Y1 + · · · + Yn .
3. Hence, derive the MGF of Ȳ = (Y1 + · · · + Yn )/n.
√
4. Hence, derive the MGF of Z = (Ȳ − µ)/(σ/ n) and identify its distribution.
In parts 2–4, you may use without proof general properties in Chapter 1 on MGFs.
When you use a property, however, remember to state it, make clear how it is being
applied, and check any assumptions required for the result.
Exercise 2.3
Let Y ∼ χ21 . Show that the MGF of Y is (1 − 2t)−1/2 .
Exercise 2.4
[Quiz #1, 2010-11, Term 2, except that showing “Var(Z 2 ) = 2” was not included]
Let Z have a standard normal distribution with expectation 0 and variance 1, i.e.,
Z ∼ N (0, 1).
1. Write down the definition of MZ 2 (t), the moment generating function (MGF)
of Z 2 .
2. From the definition, show MZ 2 (t) = (1 − 2t)−1/2 .
3. Use MZ 2 (t) to show that E(Z 2 ) = 1 and Var(Z 2 ) = 2.
4. Argue that the distribution of Z 2 is χ21 .
Exercise 2.5
Show that the MGF of a random variable with a χ2d distribution is (1 − 2t)−d/2 .
Exercise 2.6
[Quiz #1, 2011-12, Term 1, except that part 5 was not included] Let Z1 , . . . , Zd be d
independent N (0, 1) random variables, and let
Y = Z12 + · · · + Zd2 .
You may use without proof the result that Zi2 ∼ χ21 for i = 1, . . . , d, and that the
moment generating function (MGF) of the χ21 distribution is (1 − 2t)−1/2 (see Exercise 2.4).
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.8. EXERCISES

2-23

1. Write down the definition of MY (t), the MGF of Y .
2. Show that the MGF of Y is (1 − 2t)−d/2 , either directly starting from the
definition or by stating and applying an appropriate result. In either case be
sure to explain the assumption(s) you are using.
3. Hence, what is the distribution of Y ? Explain briefly.
4. From the MGF find E(Y ).
5. From the MGF find Var(Y ).
Exercise 2.7
Let X1 and X2 have independent χ2 distributions with degrees of freedom d1 and d2 ,
respectively. Show that X1 + X2 has a χ2d1 +d2 distribution.
Exercise 2.8
Let Y1 , . . .∑
, Yn be independent random variables with mean µ and variance σ 2 , and
1
let Ȳ = n ni=1 Yi .
1. Show that the covariance between Ȳ and Yi − Ȳ is zero.
2. Assume also that Y1 , . . . , Yn are normally distributed. Are Ȳ and Yi − Ȳ independent? Why?
Exercise 2.9
Let Y1 , . . . , Yn be independent random variables, each with mean µ and variance σ 2 .
The values of µ and σ 2 are both unknown. Consider the sum of squares
X=

n
∑

(Yi − Ȳ ) =

i=1

2

n
∑

Yi2 − nȲ 2 .

i=1

1. Show that E(X) = (n − 1)σ 2 .
2. Hence give an estimator of σ 2 based on X that has expectation σ 2 .
Exercise 2.10
In this exercise we calculate further confidence intervals for the study in Example 2.1.
Recall that Schlaich et al. (1998) collected data on adjusted forced expiratory volume
for n = 34 patients with manifest osteoporosis. The data summaries for the sample
are:
ȳ = 94.3 and s = 14.7,
where the units are percentage points.
As before, we will assume that the n = 34 data values are a random sample from a
N (µ, σ 2 ) distribution, and that the objective is to estimate µ.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-24

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

1. Compute 90% and 99% confidence intervals for µ based on Student’s t distribution. What are their widths?
2. Before Student derived the t distribution, common practice was to carry out
calculations as above using the standard normal distribution rather than the t
distribution.
(a) Use R to plot the PDF of the standard normal for values in the range
[−4, 4]. You can set up such a grid of values at spacing of 0.01 using
x <- seq(-4, 4, by = 0.01)
When you use plot with x on the x-axis and the corresponding values of
the normal PDF on the y-axis, include the argument type = "l" to tell
R to join the coordinates as lines to create a curve.
(b) Add the PDF of the t distribution (with appropriate degrees of freedom)
to your plot. Using lines rather than plot will add to the current plot
rather than generating a new one. To distinguish the two curves, the
argument lty = 2 will make the new curve from dashed lines.
(c) Comment on how well the standard normal approximates the t distribution
here.
3. Recompute the 90% and 99% confidence intervals in part 1 but use the standard
normal rather than the t distribution. What are their widths?
4. How much wider are the confidence intervals using the t distribution relative to
those using the standard normal? (“Relative” here is a ratio of widths.)
5. Look again at your plots of the standard normal and t PDFs. Why is there more
more discrepancy in the confidence interval from the t distribution relative to
the normal distribution as the confidence level increases?
Exercise 2.11
Example 2.1 analyzed data collected by Schlaich et al. (1998) on lung function in
patients with manifest osteoporosis. The investigators also collected data on a second
sample of n = 51 patients without manifest osteoporosis. The second sample is a
“control” group for comparison. The definition of the measure FEV1% we will use
for the control group is the same as in Example 2.1 and is again called y.
The control sample gives the following data summaries:
ȳ = 96.1 and s = 14.4,
where s is the sample standard deviation. We will again assume the data are a
random sample from a normal distribution and that interest centres on estimation of
the mean of the distribution.
1. Based on the above description, write down a formal probability model for the
way that the control-sample data, y1 , . . . , y51 , arose. Be sure to specify:
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.8. EXERCISES

2-25

(a) the random variable(s);
(b) the distribution of the random variable(s);
(c) a description of any parameters of the distribution;
(d) any other assumption(s) about the random variable(s).
2. Assuming the probability model holds, calculate:
(a) an estimate of the mean of the distribution;
(b) an estimate of the standard deviation of the sample mean over repeated
samples;
(c) a 95% confidence interval for the mean of the assumed distribution.
3. Again assuming the probability model is correct, are there any approximations
in the confidence-interval calculation? Briefly explain why or why not.
4. Compare the confidence intervals in Example 2.1 and in this exercise, and comment briefly.
Exercise 2.12
[Parts 1–5 appeared on Quiz #1, 2010-11, Term 2] Suppose a random sample of
size n = 2 is drawn from a N (µ, σ 2 ) distribution to estimate µ when σ 2 is unknown.
There is a big impact on the 95% confidence interval for µ from using the t distribution
instead of the standard normal.
> qnorm (0.975)
[1] 1.959964
> qt (0.975 , df = 1)
[1] 12.7062

Thus, a confidence interval for µ based on the t distribution will be much, much
wider. Why? And why does the t distribution have 1 degree of freedom here (and
not 2 from n = 2)? The exercise sheds some light on these questions.
Let Y1 and Y2 be independent random variables sampled from a N (µ, σ 2 ) distribution.
For such a sample of size n = 2 it is easily shown that the sample variance,
n
1 ∑
(Yi − Ȳ )2 ,
S =
n − 1 i=1
2

simplifies to

(
2

S =

Y1 − Y2
√
2

)2
.

You may use this result without proof.
√
1. Let V = (Y1 − Y2 )/ 2. Show the following properties of V . Carefully state any
result you are using (no proof of the result required) and how it is applied here.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-26

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS
(a) E(V ) = 0.
(b) Var(V ) = σ 2 .
(c) V has a normal distribution.

2. Explain why the distribution of V /σ is standard normal.
3. Hence argue that when n = 2, the distribution of S 2 /σ 2 is χ21 . (A result on the
connection between N (0, 1) and χ21 random variables may be stated and used
without proof.)
4. Using (without proof) the properties of the χ21 distribution, what are the expectation and variance of S 2 /σ 2 when n = 2?
5. Hence, what is the expectation of S 2 when n = 2?
6. What is the variance of S 2 when n = 2?
7. Use qchisq in R to find quantiles l and u such that
Pr(S 2 /σ 2 < l) = 0.025 and

Pr(S 2 /σ 2 > u) = 0.025.

Hence, l and u are lower and upper bounds on S 2 /σ 2 in the sense that
Pr(l < S 2 /σ 2 < u) = 0.95.
8. Suppose S 2 is used to estimate σ 2 from a sample of size n = 2. Comment on
the values of S 2 /σ 2 that could occur.
Exercise 2.13
[Parts 1–3 appeared on the final exam, 2010-11, Term 1.] Let Y1 , . . . , Yn be independent N (µ, σ 2 ) random variables. Their sample variance is
n
1 ∑
S =
(Yi − Ȳ )2 .
n − 1 i=1
2

This question explores the properties of S 2 /σ 2 and why the tn−1 distribution approaches the standard normal as n → ∞. Section 2.4.2 argued that S 2 /σ 2 has the
same distribution as X/(n − 1), where X ∼ χ2n−1 . You may use this result and
properties of the χ2 distribution without proof.
1. Find E(S 2 /σ 2 ).
2. Find Var(S 2 /σ 2 ).
3. Let ϵ > 0 be a fixed constant representing an arbitrarily small “error”.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.8. EXERCISES

2-27

(a) As n → ∞, what is the limiting probability
Pr(1 − ϵ < S 2 /σ 2 < 1 + ϵ)?
(b) Briefly describe how you would justify the limiting probability (a complete
proof is not required).
4. In Section 2.4.3 the sample mean was standardized by its expectation and sample variance and then expanded in equation (2.4):
√
(Ȳ − µ)/ σ 2 /n
Ȳ − µ
√
√
=
.
S 2 /n
S 2 /σ 2
It was shown that this quantity has a tn−1 distribution. As n → ∞, it is known
that the tn−1 distribution converges to N (0, 1). Use the above results to justify
this convergence.
5. Using the R function rnorm, simulate 1000 samples of size n = 10 from the
normal distribution with µ = 0 and σ = 2. For each sample, compute its
sample variance using var.
6. Construct a histogram of the 1000 sample variances. Does it have a shape that
looks like one of the distributions in Figure 2.2? If so, which?
7. Compute the sample mean and sample variance of the 1000 sample variances
using mean and var. Compare to the theoretical mean and variance of the
sample variance you found in parts 1 and 2.
Exercise 2.14
Let X have a χ2d distribution. Show that a standardized version of X has a limiting
standard normal distribution as d → ∞. Be sure to be specific about the standardization of X and to check the conditions of any result on limiting distribution that
you use.
Exercise 2.15
This exercise demonstrates the CLT via simulation.
1. In R, generate a sample of 1000 independent Unif (−1, 1) random variables.
n <- 1000
x <- runif (n, min = -1, max = 1)

Take a look at the first 10 elements of the vector x that contains the sample
using x[1:10], and make sure they look good. For example, all values should
be in [−1, 1]!
2. Use hist to draw a histogram of all the data in x. Look at help(hist) to find
out how to do this.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-28

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

3. Compute the sample mean and sample variance of the data. Compare with
the theoretical mean and variance of the Unif (−1, 1) distribution. Why do the
sample and theoretical quantities not agree exactly?
4. Generate a second, independent sample
y <- runif (n, min = -1, max = 1)

and then compute the sums z[1] = x[1] + y[1], z[2] = x[2] + y[2], etc.
using
z <- x + y

Note that R will apply the sum operator element-wise to the vectors. Take
a look at the first few elements of x, y, z to make sure the summation has
worked correctly. Thus, z contains 1000 values, where each element is generated
as the sum of a sample of two independent Unif (−1, 1) random variables.
5. Draw a histogram of the sample data in z. Does the histogram look more
normal than a single sample from the uniform distribution?
6. Repeat Steps 4–5 to generate a total of 5 independent samples, and compute
z as the sum across all 5 vectors. Does the histogram of z have a shape that
looks fairly normal?
7. Why do the z values not appear to be from a standard normal distribution?
What would we have to do to the z values to standardize them?
Exercise 2.16
Throughout this question, whenever you use a general result, make sure you state it
clearly and check its conditions (if any).
1. Let U ∼ Unif (−1, 1), i.e., U has a uniform distribution with parameters a = −1
and b = 1 in Table 1.4.
(a) From the definition of expectation, find E(U ).
(b) From the definition of variance, find Var(U ).
(c) From the definition of the moment generating function, find MU (t).
(d) From the moment generating function, find E(U ) and Var(U ). (As in
Example 1.29, you may find it easier to expand the exponential functions,
collect leading terms, then differentiate.)
2. Let U1 , . . . , Un be independent Unif (−1, 1) random variables. Consider the random variable
n
∑
Y =
Ui .
i=1

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.8. EXERCISES

2-29

(a) What is E(Y )?
(b) What is Var(Y )?
(c) What is the MGF of Y ?
3. Now standardize Y to find a new random variable, Z, with mean 0 and variance
1.
(a) What is Z?
(b) What is the MGF of Z?
(c) What is the MGF of Z as n → ∞? One approach is to expand the
exponential functions and collect terms before taking the limit. Note also
that (1 + a/n)n → ea as n → ∞.
(d) What is the distribution of Z?
(e) You have just proved a special case of a more general theorem. What is
it?
Exercise 2.17
Example 2.2 worked through the normal approximation to Xn , a Bin (n, π) random
variable. It was shown that
Xn − nπ
Z=√
nπ(1 − π)
is approximately N (0, 1) for large n. Here, Xn is a discrete random variable, whereas
Z is continuous. How can a random variable with a discrete PMF be approximated
by another with a continuous PDF?
Exercise 2.18
[This exercise appeared on Quiz #1, 2011-12, Term 1 without Parts 3 and 5. The quiz
included the fact that the R function qnorm(0.975) returns 1.959964.] Let Y1 , . . . , Yn
be independent random variables, each with mean µ and variance σ 2 . Note that we
are not necessarily assuming any distribution for the Yi yet.
∑
Consider using Ȳ = n1 ni=1 Yi to estimate µ.
1. Show that E(Ȳ ) = µ.
2. Show that Var(Ȳ ) = σ 2 /n.
3. What does Chebyshev’s
inequality give for the probability Pr(|Ȳ − µ| > ϵ),
√
where ϵ = 1.96 σ 2 /n?
4. What does the CLT say about the distribution of Ȳ as n → ∞?
5. Give√an approximation based on the CLT to Pr(|Ȳ − µ| > ϵ), where ϵ =
1.96 σ 2 /n. Explain briefly.
6. Suppose Y1 , . . . , Yn also have a normal distribution.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-30

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS
(a) What is the distribution of Ȳ ? Explain briefly.
√
(b) What is Pr(|Ȳ − µ| > ϵ), where ϵ = 1.96 σ 2 /n? Explain briefly.
(c) Is the calculated probability a large-sample approximation?
briefly.

Explain

Exercise 2.19
Example 2.2 argued that a standardized version of a binomial random variable has
a limiting standard normal distribution as n → ∞. Outline the key steps in the
argument, pointing to results that would be used, without tedious algebraic detail.
For instance, you might start with
Step 1. Let Xn ∼ Bin (n, π). Its MGF, MXn (t), can be obtained from
Table 1.3.
In other words, there is no need to derive MXn (t) for this first step. Indeed, there is no
need even to give an explicit expression for MXn (t) as you will not be manipulating
it algebraically in subsequent steps. On the other hand, you will need to define
carefully and mathematically various terms like MXn (t) as you go along, just to make
your argument clear.
Exercise 2.20
This exercise explores the shape of the Poisson distribution, via simulation and via a
limiting-distribution argument.
1. Using rpois in R, generate a random sample of 1000 values from a
Pois (µ = 0.35) distribution and plot the values using hist. Does the empirical distribution have a roughly normal shape?
2. Repeat part 1 but sample from a Pois (µ = 25) distribution.
3. What do the two simulations suggest about the condition(s) for the normal
distribution to be a good approximation to the Poisson distribution.
√
4. Let Y ∼ Pois (µ). The standardized variable Z = (Y − µ)/ µ has mean 0 and
variance 1. The MGF of Z can be written as
(
))
( 2
1
t
+O √
.
MZ (t) = exp
2
µ
( √ )
The notation O 1/ µ says that, for any t, the sum of all terms after the t2
term becomes negligible for sufficiently large µ. Also, see the last part of this
question for the derivation of the expansion here.
What is the limiting distribution of Z as µ → ∞?

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.9. APPENDIX: PROOF OF LEMMA 2.2

2-31

5. In Worksheet 1.3 we ended up with a random variable Y with a Pois (µ) distribution (approximately). Based on the data in Worksheet 3.1, µ appears to be
fairly small, of the order µ ≃ 0.35. Comment on whether the result in part 4 of
this exercise justifies further approximating the distribution of Y by a normal
distribution.
6. In Worksheet 3.1 we assume the data are a realization of Y1 , . . . , Yn ,∑where
n = 74 and the Yi are IID Pois (µ). Exercise 1.20 showed that Xn = ni=1 Yi
has a Pois (nµ) distribution. As n = 74, we clearly have nµ ≫ µ. Comment on
whether a normal distribution might be a good approximation to the distribution of Xn and hence the sample mean, Ȳ = X/n.
7. Derive the expansion in part 4.
(a) Write down the MGF of Y .

√
(b) The standardized variable Z = (Y − µ)/ µ has mean 0 and variance 1.
What is the MGF of Z?
(c) Using
expansion
of the exp function, show that the exponent
(
(a series
)
√ )
µ exp t/ µ − 1 appearing in MZ (t) can be written as
(
)
t2
1
√
µt + + O √
.
2
µ
(d) Hence collect terms and obtain the expansion of MZ (t).

2.9

Appendix: Proof of Lemma 2.2

From Section 2.3.2 we already established that the PDF of T = Z/
Lemma 2.2 is given by
∫ ∞
fT (t) =
fT |W (t | w)fW (w) dw,

√
Xd /d in

0

where W = Xd /d. It was also argued that fT |W (t | w) is the N (0, 1/w) PDF. Hence
1

w2
2
fT |W (t | w) = √ e−(w/2)t
2π
by substituting y = t, µ = 0, and σ 2 = 1/w in the normal PDF of Table 1.4.
Furthermore, W = Xd /d is a simple linear transformation of the χ2d random variable
Xd . From the χ2d PDF in Table 1.4 transformed according to (1.3), the PDF of W is
1
fW (w) = d d/2 ( d ) (dw)d/2−1 e−dw/2 .
2 Γ 2
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-32

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

(The factor d comes from the derivative of the linear inverse transformation Xd =
dW .)
Combining the two PDFs, the required integral is
∫ ∞

1

w2
1
2
√ e−(w/2)t × d d/2 ( d ) (dw)d/2−1 e−dw/2 dw,
fT (t) =
2 Γ 2
2π
0
∫
∞
dd/2
2
(
)
=√
w(d+1)/2−1 e−w(d+t )/2 dw.
d
d/2
2π2 Γ 2 0
Up to constants, the integrand is the gamma PDF in Table 1.4 with
ν )= (d + 1)/2
( d+1
2
and λ = (d + t )/2. Inserting the required constants Γ(ν) = Γ 2 and λν =
((d + t2 )/2)(d+1)/2 gives
( )
∫ ∞
Γ d+1
dd/2
1
2
(d)
fT (t) = √
λ(λw)ν−1 e−λw dw.
2
(d+1)/2
d/2
((d
+
t
)/2)
Γ(ν)
2π2 Γ 2
0
The integrand is now a Gamma (ν, λ) PDF, which integrates to 1, leaving only the
constants outside the integral. Those constants
can( be
) simplified
( ) using
( ) properties
( )
√
π = Γ 12 and Γ 21 Γ d2 /Γ d+1
of (the )gamma function in Section 1.5.3:
=
2
1 d
B 2 , 2 , where B(·, ·) is the beta function. Hence,
dd/2
1
fT (t) = ( 1 d ) (d+1)/2
= (1 d) √
2
(d+1)/2
((d + t )/2)
B 2, 2 2
B 2, 2
d

)− d+1
(
2
t2
,
1+
d

which is the PDF of the td distribution in Table 1.4.

2.10

Appendix: Proof of the Central Limit Theorem

The Central limit theorem (CLT) in Theorem 2.2 is proved in the following steps.
1. Rewrite Zn in terms of Yi − µ.
2. Check that Zn in the theorem is standardized.
3. Find the MGF of Yi − µ.
∑
4. Find the MGF of Sn = ni=1 (Yi − µ)
5. Find the MGF of Zn from that of Sn .
6. Show the limiting MGF of Zn converges to that of the standard normal as
n → ∞.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2.10. APPENDIX: PROOF OF THE CENTRAL LIMIT THEOREM

2-33

1. Rewrite Zn in terms of Yi − µ
Write

Xn − nµ
√
Zn =
=
σ n

∑n

i=1 Yi − nµ

√

σ n

∑n
=

i=1 (Yi − µ)

√
σ n

.

(2.11)

2. Check that Zn is standardized
We then note that
E(Yi − µ) = E(Yi ) − µ = µ − µ = 0
and
Var(Yi − µ) = Var(Yi ) = σ 2 ,
based on the expectation and variance of a linear function of a random variable.
It then follows that
( n
)
n
∑
∑
E
(Yi − µ) =
E(Yi − µ) = 0,
i=1

and
Var

( n
∑

i=1

)
(Yi − µ)

i=1

=

n
∑
i=1

Var(Yi − µ) =

n
∑

Var(Yi ) = nσ 2 .

i=1

These expressions also use results for the expectation and variance of a sum of
random variables. For the variance calculation only, independence of Y1 , . . . , Yn
is also required. Hence, immediately E(Zn ) = 0, and
( n
)
(
)2
∑
1
1
√
Var(Zn ) =
Var
(Yi − µ) = 2 nσ 2 = 1,
σ n
σ n
i=1
which again use results for a linear function of a random variable. Thus, Zn
has mean 0 and variance 1 and is indeed a standardized random variable.
3. Find the MGF of Yi − µ
In the theorem, Y1 , . . . , Yn have identical distributions, so they share a common
MGF, MY (t). Recall that MY (t) is, by definition, E(etY ), and expanding the
exponential function gives
(
)
(tY )2 (tY )3
tY
MY (t) = E(e ) = E 1 + tY +
+
+ ··· .
2!
3!
Similarly, Yi − µ in (2.11) has MGF
(
)
(t(Y − µ))2 (t(Y − µ))3
t(Y −µ)
MY −µ (t) = E(e
) = E 1 + t(Y − µ) +
+
+ ··· .
2!
3!

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

2-34

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS
Treating this as the expectation of a linear combination of random variables,
we have
MY −µ (t) = 1 + tE(Y − µ) +

t2
t3
E(Y − µ)2 + E(Y − µ)3 + · · · .
2!
3!

The expression simplifies by noting that E(Y − µ) = 0 and E(Y − µ)2 =
Var(Y ) = σ 2 , whereupon
MY −µ (t) = 1 + t0 +

t2 2 t3
t2
t3
σ + E(Y − µ)3 + · · · = 1 + σ 2 + E(Y − µ)3 + · · · .
2!
3!
2!
3!

∑
4. Find the MGF of Sn = ni=1 (Yi − µ)
∑
Write Sn = ni=1 (Yi − µ), which has MGF
(
)n
t2 2 t3
n
3
MSn (t) = (MY −µ (t)) = 1 + σ + E(Y − µ) + · · ·
,
2!
3!
using Lemma 1.3 on the MGF of a sum of independent random variables.
5. Find the MGF of Zn from that of Sn
Write

1
Zn = √ Sn ,
σ n

whereupon
(
MZn (t) = MSn

)n
) (
(
)2 2 (
)3
1
σ
1
E(Y − µ)3
1
√ t = 1+
√ t
√ t
+
+ ···
,
2!
3!
σ n
σ n
σ n

using Lemma 1.2 on the MGF of a linear function of random random variables.
This simplifies to
(
(
))n
1 t2
1
+O
MZn (t) = 1 +
,
n2
n3/2
( 1 )
where O n3/2
represents further terms that are decreasing at rate 1/n3/2 or
faster.
6. Find the limiting MGF of Zn as n → ∞
As n → ∞,

(
MZn (t) =

1 t2
+O
1+
n2

(

1
n3/2

))n
1 2

→ e2t ,

from the result that (1 + x/n)n → ex as n → ∞. This is the MGF of a N (0, 1)
random variable, and the proof is complete.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-1

Chapter 3
Statistical Estimation
Statistical methods often involve estimation of unknown parameters in probability
models, as in Example 2.3 where the parameter π of the binomial distribution was
estimated. Furthermore, the example showed how the properties of the estimation
method lead to a probabilistic bound on the margin of error.
This chapter introduces statistical estimation more generally. It starts with some
philosophy relating to the frequentist view: that statistical properties like bias and
variance are defined by considering how estimates and other quantities would change
by chance over repeated random samples from the probability model. This paves the
way for the general method of maximum likelihood (Chapter 4) and analysis of its
properties from this frequentist perspective.

3.1

Statistical Models: The Role of Probability

A probability calculation typically starts with a probability model (a distribution)
for some random variable(s), Y , and calculates quantities like Pr(Y ≥ c) or E(Y ).
These calculations say something about the values that Y generates. Mathematically,
the manipulations involved may be lengthy and involve much special knowledge,
especially of integration and summation, but in one sense they are easy. Given a
well-defined probability model and a well-defined task like “find Pr(Y ≥ c)” there
is only one answer. This is called deductive logic. It is important to note that to
carry out such calculations numerically, the values of the model’s parameters must be
known.
Statistical inference also starts with a probability model but essentially uses it in a
reverse process: We start with data (i.e., observed values y) from some distribution
and then try to infer properties of the distribution. If the form of the distribution is
“known” (e.g., the Poisson) the values of its parameter (e.g., µ) or parameters will
usually have to be estimated from the data. In practice, one might not even know
the form of the distribution, and the data will be used to infer or at least check the
distribution. This inductive process is much less well defined and generates much
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-2

CHAPTER 3. STATISTICAL ESTIMATION

work for statisticians!
Thus, common steps in statistical inference are:
1. Choose a probability model (e.g., the binomial for discrete values or the normal
for continuous values). Often, the context will suggest a distribution from
first principles, but in complex problems specifying a probability distribution is
usually difficult.
2. Estimate the parameter(s) of the distribution (e.g., π for the binomial or µ and
σ 2 for the normal) from a sample of y values. This gives a fitted distribution,
fitted to the data.
3. Check that the fitted distribution is in reasonable agreement with the data.
Again, particularly for complex problems, this is not easy.
4. Use the probability model to answer questions of scientific interest, make predictions, etc.

3.2

The Frequentist Philosophy

This chapter concentrates on the second of the steps outlined above: estimating the
parameters of a given (chosen) distribution. In general, let θ be a parameter of interest
(e.g., π for the binomial distribution). We use some method to generate an estimate,
θ̂, from the data.
When we compute a numeric value for θ̂ this is an estimate. For instance, consider
Example 2.3 where the parameter π of interest was the proportion in a large population agreeing with the statement that Trudeau was the best Prime Minister. The
binomial distribution was the probability model, and π had the estimate π̂ = 0.36.
In general, an estimate of a parameter is based on one or more statistics, functions of
the data like the sample mean or sample proportion. Without knowing the true value
of the parameter we cannot say how good an estimate is. Furthermore, a number has
no statistical properties. It is just a number.
In the frequentist philosophy of statistics, we consider the values of θ̂ that would
have occurred in repeated random samples from the assumed probability distribution
for the data. (The term “frequentist” here parallels the frequentist interpretation of
probability, where probabilities are defined by long-run relative frequencies in repetitions of an experiment like rolling a die.) Often, this is a hypothetical argument as
there is only one sample of data values. Nonetheless, if we consider other samples
that might have occurred, each sample would generate its own value of θ̂, and now
θ̂ has a sampling distribution, i.e., it is a random variable. The random variable is
called an estimator of θ and will be denoted by θ̃. The statistical properties of the
random variable θ̃, explored in the next subsection, can be used to make statements
about how accurate θ̃ is in estimating θ over repeated random samples of data.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.2. THE FREQUENTIST PHILOSOPHY

3-3

Faults
Frequency
(y) observed expected
0
1
1.5
1
5
3.7
2
3
4.4
3
4
3.6
4
2
2.2
5
2
1.0
>5
0
0.6
17
17.0
Table 3.1: Observed numbers of faults on data lines of length about 170 km, and the
expected numbers under a Poisson probability model
Many textbooks and articles use the notation θ̂ both for an estimate, i.e., a number for
a specific data set, and for the estimator. The context has to guide the reader whether
this refers to an estimate or an estimator. In our text, however, the distinction
between θ̃ and θ̂ is just like that between a random variable, Y , and one of its values, y,
and hopefully helps the reader better understand the frequentist concept of properties
over hypothetical repeated samples. The θ̃ versus θ̂ notation is borrowed from my
colleagues Professors MacKay and Oldford, with whom I taught at the University of
Waterloo.
Example 3.1 (Faults on data lines: estimating the Poisson mean)
The number of faults (y) over a period of time was collected for a sample of 17
data-transmission lines, all of length about 170 km. The observed frequencies are
in Table 3.1. For instance, there are 5 lines with exactly 1 fault. The data are
displayed as a histogram in Figure 3.1(a).
Suppose the number of faults per line in the population of all lines is represented
by a Poisson distribution, i.e., Pois (µ), which has PMF
fY (y) =

e−µ µy
y!

(y = 0, 1, . . . , ∞; µ > 0).

(There are good engineering reasons for using the Poisson distribution here.) The
parameter µ is the expectation or mean of the Pois (µ) distribution, which we
need to estimate to assess the reliability of the system. As µ is the mean of the
assumed distribution, we use the sample mean, ȳ, as an obvious estimate of µ
from the data. The observed sample mean is
0×1+1×5+2×3+3×4+4×2+5×2
41
1 ∑
yi =
=
= 2.41
ȳ =
17 i=1
17
17
17

for the faults data. We write µ̂ = 2.41 for the estimate of µ here.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-4

CHAPTER 3. STATISTICAL ESTIMATION

(b) Poisson sample (µ = 2.41, y = 2.06)
8
6
Frequency
4
2
0

0

2

Frequency
4

6

8

(a) Faults data (y = 2.41)

0

2

4
Faults (y)

6

8

0

4
y

6

8

6
Frequency
4
2
0

0

2

Frequency
4

6

8

(d) Poisson sample (µ = 2.41, y = 3)

8

(c) Poisson sample (µ = 2.41, y = 2.59)

2

0

2

4
y

6

8

0

2

4
y

6

8

Figure 3.1: (a) Histogram of the faults data in Table 3.1. (b), (c), and (d) Histograms
of three random samples of size n = 17 from a Poisson distribution with the parameter
µ set to 2.41, which is the value of ȳ observed for the faults data. The sample mean,
ȳ, is also given for each random sample.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.2. THE FREQUENTIST PHILOSOPHY

3-5

How good is this estimate? This question is difficult to answer for the specific
sample, but we can look at properties over random samples. Such properties
are defined mathematically in Section 3.3. For now, we can informally compare
the sample data with random samples of size n = 17 drawn from a Pois (µ)
distribution. The value of µ is unknown—the objective here is to estimate it—
but we proceed by setting µ to 2.41, the estimate from the data. This gives an
idea of how much the data and hence µ̂ vary from one sample to another just by
chance if µ is about 2.41. The histograms in Figures 3.1(b), (c), and (d) show
three such random samples created via rpois in R. Two features of the plots are
worth noting.
• There is a fair amount of difference in shape in the histograms of Figures 3.1(b), (c), and (d) because the sample size is small. The histogram of
the faults data in Figure 3.1(a) does not stand out compared to the Poisson
samples.
• The sample mean, ȳ, is reported for the three samples from a Pois (µ = 2.41)
distribution. It ranges from 2.06 to 3.00. Thus, just by chance, the sample
mean varies from one random sample to another. Such sampling variation
across repeat random samples is the basis for treating the sample mean as
a random variable, Ȳ . The variation in Ȳ across samples defines the uncertainty attached to an estimate in this chapter: the frequentist paradigm.
Although Figure 3.1 demonstrates considerable variation in the sample mean here
just due to chance, the data are good enough to narrow down the range of possible
values of µ in a statistical sense. Figure 3.2 repeats Figure 3.1, but now µ = 5 when
generating three random samples from the Pois (µ) distribution. The histogram of
the data in Figure 3.2(a) now stands out: The other three histograms are shifted
to the right. Furthermore, the sample means from the Pois (µ = 5) distributions
still vary but take values much larger than ȳ = 2.41 for the faults data. We really
need more random samples to say much more, but it looks like the data rule out
µ = 5 via this statistical sampling argument.
Is the Poisson distribution a reasonable probability model here? If we set µ to the
estimate 2.41, the Poisson PMF is
fY (y) =

e−2.41 (2.41)y
.
y!

Thus, in a sample of size 17 we would expect 17fY (0) = 17×0.0898 = 1.5 values of
0, 17fY (1) = 17 × 0.216 = 3.7 values of 1, etc. These expected frequencies are also
given in Table 3.1. The agreement between observed and expected frequencies
appears good. Goodness of fit between data and a hypothesized distribution is
taken up more formally in Section 8.4.
♢♢♢

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-6

CHAPTER 3. STATISTICAL ESTIMATION

(b) Poisson sample (µ = 5, y = 4.76)
5
4
Frequency
2
3
1
0

0

1

Frequency
2
3

4

5

(a) Faults data (y = 2.41)

0

2

4

6
Faults (y)

8

10

12

0

4

6
y

8

10

12

4
Frequency
2
3
1
0

0

1

Frequency
2
3

4

5

(d) Poisson sample (µ = 5, y = 5.76)

5

(c) Poisson sample (µ = 5, y = 5.47)

2

0

2

4

6
y

8

10

12

0

2

4

6
y

8

10

12

Figure 3.2: (a) Histogram of the faults data in Table 3.1. (b), (c), and (d) Histograms
of three random samples of size n = 17 from a Poisson distribution with the parameter
µ set to 5, which is much larger than the ȳ = 2.41 observed for the faults data. The
sample mean, ȳ, is also given for each random sample.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.3. PROPERTIES OF AN ESTIMATOR

3.3

3-7

Properties of an Estimator

Let θ̃ be an estimator of the parameter θ. How good is θ̃: How much error does
it have in estimating θ? We can answer this question, at least probabilistically, by
examining the statistical properties of θ̃ and hence its error, θ̃ − θ. All the properties,
like expectation and variance of θ̃, are with respect to its sampling distribution over
repeated random samples.

3.3.1

Bias

The bias of θ̃ as an estimator of θ follows from its expectation.
Definition 3.1 (Bias)
The bias of the estimator θ̃ of a parameter θ is
Bias(θ̃) = E(θ̃) − θ.
Rewriting the bias as
Bias(θ̃) = E(θ̃) − θ = E(θ̃ − θ),
we see that the bias is the mean of the error distribution.
If E(θ̃) = θ, then the bias is zero, and we say that θ̃ is unbiased. Unbiasedness means
that the sampling distribution of θ̃ is centred on the true value, θ, in the sense that
the error might be positive or negative from one sample to another, but these errors
cancel out on average. Thus, unbiasedness is desirable.

3.3.2

Variance

The variance of θ̃ as estimator of θ is defined just like the variance of any random
variable (Definition 1.3). Small values of Var(θ̃) are desirable in the sense that the
estimator has small variability over random samples.

3.3.3

Mean squared error

A measure of accuracy summarizing the distribution of the error θ̃ − θ is the mean
squared error (MSE), the expectation of the squared error.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-8

CHAPTER 3. STATISTICAL ESTIMATION
Definition 3.2 (Mean squared error (MSE))
The mean squared error of θ̃ as an estimator of θ is
MSE(θ̃) = E(θ̃ − θ)2 .
It can be decomposed as
MSE(θ̃) = Var(θ̃) + Bias2 (θ̃).

The MSE combines the bias and variance properties of θ̃ in a single measure. The
decomposition of the MSE into its two components can be seen by writing:
(
)2
(
)2
MSE(θ̃) =E θ̃ − θ = E θ̃ − E(θ̃) + E(θ̃) − θ
(
)2
=E θ̃ − E(θ̃) + Bias(θ̃)
(from the definition of Bias(θ̃)
(
)
=E (θ̃ − E(θ̃))2 + 2(θ̃ − E(θ̃)) Bias(θ̃) + Bias2 (θ̃)
(
)2
(
)
=E θ̃ − E(θ̃) + 2E θ̃ − E(θ̃) Bias(θ̃) + Bias2 (θ̃)
(from the expectation of a linear combination of random variables
and noting that E(θ̃) and hence Bias(θ̃) are constants)
=Var(θ̃) + 2 × 0 × Bias(θ̃) + Bias2 (θ̃)

)
(
(from the definition of variance and E θ̃ − E(θ̃) = E(θ̃) − E(θ̃) = 0)

=Var(θ̃) + Bias2 (θ̃).
Thus, mean squared error penalizes an estimator both for having bias (i.e., the wrong
expectation or mean) and for its variance. As MSE decreases, the accuracy of θ̃ in
estimating θ increases in this statistical sense.
If θ̃ is unbiased, then MSE(θ̃) = Var(θ̃). This is often the case, exactly or approximately, hence the emphasis on variance calculations in statistical inference about
parameters.
Example 3.2 (Faults on data lines: properties of the estimator of µ)
Example 3.1 used µ̂ = ȳ = 2.41 as an obvious estimate of the Poisson parameter,
µ, for the faults data with n = 17. To assess the accuracy of this estimate we can
think in terms of repeated random samples of the data and the estimator µ̃. How
far can µ̃ stray from µ in a probabilistic sense just by chance sampling variation?
The sample mean Ȳ is the mean of 17 IID Poisson random variables here. From
the properties in Table 1.3, a Poisson random variable Y has mean µ and variance
µ. The following properties of Ȳ are shown more generally in Section 2.4.1.
∑
• Unbiasedness. As Ȳ = 17
i=1 Yi /17 has the same expectation as E(Yi ) = µ,
we have E(Ȳ ) = µ and µ̃ is an unbiased estimator of µ for any value of n.
• Variance. As Ȳ has variance Var(Yi )/17, we have Var(µ̃) = µ/17.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.3. PROPERTIES OF AN ESTIMATOR

3-9

• Estimated standard deviation or standard error. Because Var(µ̃) =
µ/17 depends on µ, in practice it has to be estimated. An obvious estimate
d
is Var(µ̃)
= µ̂/17 = 2.41/17
√ = 0.142. Equivalently, the estimated standard
b
deviation of µ̃ is sd(µ̃)
= 0.142 = 0.38. An estimated standard deviation
is often called a standard error, which we write as se(µ̃) here. Note that
only a random variable, here µ̃, can have a variance or standard deviation,
b
whether it is estimated or not. Thus, sd(µ̃)
= se(µ̃) = 0.38 makes statistical
b
sense, but sd(µ̂) or se(µ̂) do not.
Authors will often write a looser statement like, “an estimate µ̂ = 2.41 with
a standard error of 0.38,” but we need to interpret “with” in the sense of a
property of µ̃ not µ̂.
• Mean squared error. Because µ̃ is an unbiased estimator of µ, the MSE
of µ̃ is also µ/17.
To summarize, while the estimate µ̂ = 2.41 is a number with no statistical properties, the corresponding estimator µ̃ is unbiased over repeated samples of size
n = 17. Hence, the MSE of the estimator is entirely due to its variance, namely
µ/17. The estimator has an estimated variance of 0.142 or equivalently an estimated standard deviation of 0.38.
♢♢♢
Several facts specific to the Poisson distribution were used in Example 3.2 to obtain
statistical properties of the estimator of µ. Chapter 4 takes a general approach, the
method of maximum likelihood, for estimation and quantification of the uncertainty
of estimators. It relies less on knowledge of specific properties and can be extended
to applications where exact properties are unavailable.

3.3.4

Practical perspective

Estimation bias is usually of little practical consequence, as it is typically zero or small
relative to the standard deviation of an estimator. The following example questions
whether a familiar unbiasedness argument is compelling.
Example 3.3 (Sample variance: divisor of n − 1 or n?)
Let Y1 , . . . , Yn have constant mean µ and constant variance σ 2 . A vexing question
to countless students of statistics is why not define their sample variance with a
divisor of n, i.e.,
1
σe2 = X,
n
where
n
∑
X=
(Yi − Ȳ )2 ,
i=1

instead of the familiar S 2 = X/(n − 1)?
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-10

CHAPTER 3. STATISTICAL ESTIMATION

A standard answer is that σe2 is biased, whereas S 2 is not (Exercise 2.9). The bias
turns out to be small relative to sd(σe2 ), however, in the important special case that
the Yi are also normally distributed. Then, via the arguments of Section 2.4.2,
E(X) = (n − 1)σ 2

and

Var(X) = 2(n − 1)σ 4 ,

whereupon
1
1
Bias(σe2 ) = E(σe2 ) − σ 2 = (n − 1)σ 2 − σ 2 = − σ 2
n
n
and

1
2(n − 1) 4
Var(X)
=
σ .
n2
n2
We see that for any n ≥ 2, the bias of σe2 is smaller in magnitude than its standard
deviation:
Bias(σe2 )
−σ 2 /n
1
=√
= −√
.
2(n − 1)σ 2 /n
2(n − 1)
sd(σe2 )
Var(σe2 ) =

Of course, no bias is better than a “small” bias, but Exercise 3.4 shows that σe2
has a smaller standard deviation and smaller MSE than S 2 and is more accurate
overall. The most compelling case for the use of S 2 with divisor n − 1 is that,
for IID normal random variables, X has a χ2n−1 distribution with n − 1 degrees
of freedom, and hence S 2 fits the mathematical requirements of the t distribution
(Section 2.4.3).
♢♢♢
Usually, the MSE of an estimator either equals its variance (unbiased estimation) or
is not much larger than the variance (small squared bias). Hence, we will see that
when a confidence interval is calculated to quantify error it is nearly always based on
the estimator’s standard deviation only.
Estimation bias may be ignorable, but there are many other possible source of bias in
an empirical study. They include sampling from a population other than the target
one or bias in the measurement system producing data. Moreover, other sources are
difficult to study in a quantitative way by analysis of the data (and hence will not be
pursued much in this text). Best practice is to mitigate sources of bias proactively
by careful study design and objective measurement.

3.3.5

Consistency

Definition 3.3 (Consistency)
The estimator θ̃n of a parameter θ based on a sample of size n is consistent
for estimating θ if
Pr(|θ̃n − θ| < ϵ) → 1 as n → ∞
for any fixed error, ϵ > 0.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.3. PROPERTIES OF AN ESTIMATOR

3-11

Consistency of θ̃n is a special case of convergence in probability of a random variable
(θ̃n here) to a constant (θ).
Consistency requires that both the bias and variance of θ̃ go to zero as n → ∞.
Hence, a necessary and sufficient condition is that the mean squared error goes to
zero.
Many estimators are of the form of a sample mean, which includes the sample proportion, or a simple function of the sample mean. If the objective is to estimate the mean
of the underlying distribution, and the sample consists of independent observations,
then consistency of such estimators is easily established as a special case of the Weak
Law of Large Numbers (WLLN).
Theorem 3.1 (Weak law of large numbers (WLLN))
Let
n
1∑
Ȳn =
Yi ,
n i=1
where Y1 , . . . , Yn are n independent random variables, each with mean µ and
variance σ 2 (both of which must exist). Then, for any ϵ > 0,
Pr(|Ȳn − µ| < ϵ) → 1 as n → ∞.
The law of large numbers is not about large observations! It is concerned with a large
sample size, n. Also, the random variables are not the individual elements of the
sample. Rather, as the sample size, n, increases, there is a sequence of sample means,
Ȳn , computed from more and more elements.
In the WLLN, ϵ can be made arbitrarily small as long as it is positive. Thus, the
theorem says that the distribution of Ȳn is more and more concentrated around an
arbitrarily small neighbourhood of µ as the sample size grows. Thus, Ȳn is a consistent
estimator of µ.
The proof of the WLLN is straightforward. From (1.9) we have
)
( n
n
n
∑
∑
1
1
1∑
Yi =
E(Yi ) =
µ = µ.
E(Ȳn ) = E
n i=1
n
n
i=1
i=1
Similarly, we use (1.10) to get Var(Ȳn ). Because we are assuming the Yi are independent, all the covariance terms, Cov(Yi , Yj ) for i ̸= j, in (1.10) are zero. Thus,
)
( n
n
n
∑
∑
1
1 2 σ2
1∑
Yi =
Var(Ȳn ) = Var
Var(Y
)
=
σ = .
i
2
2
n i=1
n
n
n
i=1
i=1
Clearly, Var(Ȳn ) → 0 as n → ∞, and the result follows by putting t = ϵ in Chebyshev’s
inequality (Theorem 1.1).
Example 3.4 (Opinion polls: weak law of large numbers)
Each voter in the population of eligible voters intends to vote for the Statistics
for Everybody Party (y = 1) or will not (y = 0) in the next federal election.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-12

CHAPTER 3. STATISTICAL ESTIMATION

To estimate the population proportion, π, intending to vote for Statistics for
Everybody, a random sample of n eligible voters is taken.
Let Yi be the voting intention for person i in the sample; it is a Bernoulli random
variable with Pr(Yi = 1) = π, i.e., Bern (π). Note that E(Yi ) = π and Var(Yi ) =
π(1 − π). We further assume that Y1 , . . . , Yn are independent random variables
(which would be approximately true if the population size is large relative to the
sample size, which it usually is).
Consider estimating π using the sample mean,
1∑
Yi ,
n i=1
n

Ȳn =

where the subscript n on Ȳn emphasizes that we are investigating the impact of
increasing the sample size. (The sample mean is also a sample proportion here:
As Yi take values 0 and 1 only, Ȳ is the proportion of 1’s in the sample.) Then,
from Exercise 1.17,
E(Ȳn ) = π,
and

π(1 − π)
.
n
The variance clearly tends to zero as n increases. Thus, by Chebyshev’s inequality
(Theorem 1.1) the estimator Ȳ is in an arbitrarily small neighbourhood of the true
value π with probability approaching 1 as the sample size n grows, and Ȳn is a
consistent estimator of π.
Var(Ȳn ) =

We could argue the same result directly from the WLLN. The conditions of the
WLLN are easily verified: Y1 , . . . , Yn are independent and the mean and variance
of Yi both exist. Hence, via the WLLN, the sample mean of Y1 , . . . , Yn is a
consistent estimator of E(Yi ) = π.
♢♢♢
Consistency of θ̃ is a natural requirement: an estimator that does not converge to θ
for an infinite sample size should be questioned.

3.3.6

Relative Error

Implicitly, the measures of error used so far have related to absolute error. For example,
((
)2 )
Var(θ̃) = E
θ̃ − θ
,
where positive and negative errors θ̃ − θ are treated the same due to squaring. Similarly, in MSE and its squared bias component, the sign of the bias is immaterial.
For some applications, relative error,
θ̃
θ̃ − θ
= − 1,
θ
θ
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.3. PROPERTIES OF AN ESTIMATOR
π
nabs
nrel

0.01
0.02
44
88
110 000 54 445

0.05
0.10
212
400
21 112 10 000

3-13
0.20
712
4445

0.30 0.40 0.50
934 1067 1112
2593 1667 1112

Table 3.2: Sample size to estimate the binomial parameter π: nabs achieves sd(π̃) ≤
0.015 and nrel achieves sd(π̃)/π ≤ 0.03
and summary measures based on it are more compelling, however. The following
sample-size calculation illustrates that the different definitions of error can have important consequences.
Example 3.5 (Binomial distribution: sample size determination)
A common question is, “What should the sample size be?” For instance, the
opinion poll of Example 2.3 had n = 1000. Why are the sample sizes of opinion
polls typically of order one thousand?
Example 2.3 boils down to estimation of π in a Bin (n, π) probability model. Suppose the requirement is to determine n such that sd(π̃) ≤ 0.015. That requirement
is often stated as 1.5 percentage points, to emphasize that it is an absolute number
of “points” on the percentage scale. Rearrangement of
√
π(1 − π)
0.015 = sd(π̃) =
n
yields
n=

π(1 − π)
,
(0.015)2

as shown in the nabs row of Table 3.2. The required sample size depends on
the true value π. It is maximized when π = 0.5 and decreases as π decreases.
Similarly nabs decreases as π increases above 0.5 (not shown) in a symmetric way.
Thus, a sample size nabs = 1112 will give sd(π̃) ≤ 0.015 for any π, and a 95%
confidence interval using the standard error and a normal approximation will be
no wider that ±z0.0975 se(π̃) = ±1.96 × 0.015 = ±0.0294, or about plus or minus 3
percentage points. As nabs does not change much for, say, 0.2 < π < 0.8, a sample
size based on the worst case, π = 0.5, is often employed when π is anticipated in
such a range.
The previous arguement becomes less relevant when π is small, however. Table 3.2
gives nabs = 44 for π = 0.01, for instance, but the requirement sd(π̃) ≤ 0.015 probably needs tightening: the standard deviation is larger than the true value. More
natural is to control the standard deviation to be small relative to the true value,
e.g., sd(π̃) ≤ 0.03π. Applying the rule for the variance (hence standard deviation)
of a linear function of a random variable, the new requirement is equivalent to
( )
(
)
(
)
π̃
π̃
π̃ − π
sd(π̃)
= sd
= sd
− 1 = sd
,
0.03 =
π
π
π
π
i.e., the standard deviation of the relative error.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-14

CHAPTER 3. STATISTICAL ESTIMATION

Solving

√
0.03π = sd(π̃) =

for n yields
n=

π(1 − π)
,
n

(1 − π)
,
(0.03)2 π

as shown in the nrel row of Table 3.2. For π = 0.5, there is no impact on sample size
as the absolute and relative requirements are chosen to be equivalent at π = 0.5.
But nrel increases rapidly as π approaches zero: even for a moderately small
π = 0.05 we need a sample size of nrel = 21 112, increasing to 110 000 at π = 0.01.
Relaxing the requirement so that sd(π̃) ≤ 0.1π when π = 0.01 still needs nrel =
9900. Estimating a small probability with good relative accuracy requires a huge
sample size.
♢♢♢

3.4

Comparing Estimators

Bias and variance properties allow comparison of candidate estimators when the
choice of estimator is not obvious.
An interesting case is the Laplace (double-exponential) distribution. As noted in
Section 1.5.4, the distribution is symmetric around the location parameter µ, which
is therefore both the mean and median. Should the sample mean or the sample
median from a random sample be used as its estimator? (The sample median is the
“middle” value in the data.) It turns out that both estimators are unbiased, but
the sample median has the smaller variance for n ≥ 3 (Sarhan, 1954). These results
are demonstrated numerically in Exercise 3.5. The sample median is naturally more
robust to unusual outlying observations that can arise due to the Laplace PDF’s fat
tails, and this intuition is borne out by the theoretical properties.

3.5

Getting It Done in R

In Example 3.1 random samples were drawn from the Poisson distributions. R has
functions to generate random numbers for all the distributions listed in Table 1.9.
They have names starting with r.
Thus, R has functions with names starting with d for the PDF or PMF, p for the CDF,
q for a quantile, and r for generating random numbers. Table 3.3 lists these functions
for the normal distribution, for instance. The function rnorm has an argument n for
the sample size. Hence, rnorm(n = 10) would generate a sample of size n = 10 from
the standard normal.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.6. LEARNING OUTCOMES
Purpose
PDF
CDF
Quantile
Random number

3-15
R function
dnorm(y, mean = 0, sd = 1)
pnorm(y, mean = 0, sd = 1)
qnorm(p, mean = 0, sd = 1)
rnorm(n, mean = 0, sd = 1)

Table 3.3: R functions to return the PDF, CDF, quantile, or random numbers for the
normal distribution

3.6

Learning Outcomes

On completion of this chapter you should be able to carry out the following tasks.
1. Explain the difference between an estimate and an estimator of a parameter
and why the distinction is important to define statistical properties.
2. Derive the following properties for a specific estimator: its bias (which might be
zero); its variance; its mean squared error; and whether or not it is consistent.
3. Use the WLLN (Theorem 3.1 in Section 3.3.5), and check the conditions, to
show the sample mean is a consistent estimator of the mean of the underlying
distribution.
4. Explain your reasoning. When using a result such as the expectation or variance
of a linear combination of random variables to derive a property of an estimator,
briefly state the result you are using. If the result depends on an assumption
such as statistical independence of random variables, remind the reader that
you are using the assumption.

3.7

Exercises

Exercise 3.1
Frequentist inference considers variation across hypothetical repeated random samples. In some cases the design of a study involves a step with a probabilistic mechanism to select a sample of data. Briefly describe the probabilistic mechanism and the
set of possible random samples for the following studies:
1. The opinion poll in Example 2.3; and
2. The clinical trial in Example 1.15.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-16

CHAPTER 3. STATISTICAL ESTIMATION

Exercise 3.2
Suppose we obtain n independent observations, Y1 , . . . , Yn , from a Poisson probability
model to estimate the Poisson parameter µ. Consider the estimator µ̃ = Ȳ .
1. Show µ̃ is unbiased.
2. Find Var(µ̃) (an exact formula for the variance).
3. Is µ̃ a consistent estimator of µ?
g
4. Consider Var(µ̃)
= µ̃/n as an estimator of Var(µ̃).
g
(a) Show that Var(µ̃)
is an unbiased estimator of Var(µ̃).
g
(b) What is the variance of Var(µ̃)?
g
(c) Is Var(µ̃)
a consistent estimator of Var(µ̃)?
Exercise 3.3
Let Y1 , . . . , Yn be independent N (µ, σ 2 ) random variables. Their sample variance is
n
1 ∑
S =
(Yi − Ȳ )2 .
n − 1 i=1
2

Treat S 2 as an estimator, i.e., a random variable. Is it a consistent estimator of σ 2 ?
Exercise 3.4
[Final exam, 2011-12, Term 1] Let Y1 , . . . , Yn be independent normal random variables,
each with mean µ and variance σ 2 . We want to estimate σ 2 from such a sample of
size n ≥ 2 when µ is also unknown.
∑
This question investigates the exact properties of σe2 = X/n, where X = ni=1 (Yi −
Ȳ )2 . You may use without proof: (1) the result that X/σ 2 has a χ2n−1 distribution;
and (2) statistical properties of the χ2 distribution.
1. Show that the expectation of σe2 is σ 2 (n − 1)/n.
2. Show that the variance of σe2 is 2(n − 1)σ 4 /n2 .
3. Hence, give and simplify an expression that summarizes the accuracy of σe2 as
an estimator of σ 2 .
4. Is σe2 a consistent estimator of σ 2 ? Explain briefly.
5. The sample variance with divisor n−1, namely S 2 = X/(n−1), has E(S 2 ) = σ 2
and Var(S 2 ) = 2σ 4 /(n − 1). Give one advantage and one disadvantage of S 2
relative to σe2 as an estimator of σ 2 .
6. Explain which estimator of σ 2 is the more accurate, S 2 or σe2 .
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3.7. EXERCISES

3-17

# Number of repeat samples
n.reps <- 10000
# Vectors to store the sample means and medians
sample .mean
<- rep (0, times = n.reps)
sample . median <- rep (0, times = n.reps)
# Generate repeat samples
library ( rmutil )
for (k in 1:n.reps) {
# Random sample k
a. sample <- rlaplace (...)
# Save mean and median
sample .mean[k]
<- mean(a. sample )
sample . median [k] <- median (a. sample )
}

Figure 3.3: R code for Exercise 3.5
Exercise 3.5
A random variable Y with a Laplace (double-exponential) distribution has PDF
f (y | µ, ϕ) =

1 − |y−µ|
e ϕ
2ϕ

(−∞ < y < ∞; −∞ < µ < ∞; ϕ > 0)

(see Table 1.4). As the Laplace distribution is symmetric, µ is the expected value
(mean) and median. Hence, in this exercise we compare as estimators of µ the sample
mean and the sample median from a random sample, Y1 , . . . , Yn .
We will also need the fact that 2ϕ2 is the variance of the Laplace distribution.
1. Consider first the sample mean as an estimator, i.e., µ̃ = Ȳ . Its properties can
be found theoretically.
(a) What is the expected value of µ̃? Is it unbiased as an estimator of µ?
(b) Give a formula for sd(µ̃). What is the numerical value of sd(µ̃) if n = 25
and ϕ = 10, say?
2. Now consider the sample median as an estimator, i.e., µ̃ = median(Y1 , . . . , Yn ).
Note that µ̃ is now a different estimator. It would be a little more difficult to
establish its theoretical properties, and we resort to numerical simulation. Simulation has a learning bonus, however. We will be generating repeat samples,
the principle underlying the frequentist philosophy of statistical inference.
(a) Use rlaplace in library(rmutil) to generate a random sample of size
n = 25 from the Laplace distribution with µ = 1 and ϕ = 10. Repeat
for 10 000 samples as in the R code of Figure 3.3. You need to give values
to the parameters passed to rlaplace. The code also stores the sample
means to check a theoretical property of Ȳ here.
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

3-18

CHAPTER 3. STATISTICAL ESTIMATION
After running your code, you have an empirical distribution of the sample
median (and mean) from many repeat samples. Because the number of
repeat samples is fairly large, the empirical distribution is a good approximation to the true distribution of the sample median. We will estimate
the properties of the sample median from this empirical distribution.
(b) Apply mean to the 10 000 values of the sample median from the random
samples to find the (approximate) expected value of the sample median.
Does it appear that µ̃ = median(Y1 , . . . , Yn ) is an unbiased estimator?
(c) Apply sd to the 10 000 values of the sample median to find the (approximate) standard deviation of the sample median.

3. Check the theoretical property in part 1b by applying sd to the 10 000 values
of the sample mean.
4. Which estimator appears to be a more accurate estimator of the parameter µ
of the Laplace distribution, the sample mean or the sample median?

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-1

Chapter 10
Solutions

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-2

CHAPTER 10. SOLUTIONS

Solution 1.1
1. 0.001102, 0.004888, and 0.007978, respectively
2. 0.01102, 0.04888, and 0.07978, respectively
3. For the picture, think of the midpoint rule (also known as the rectangle method)
for approximating a definite integral.
Solution 1.4
1. The Poisson distribution has PMF
fY (y) =

e−µ µy
y!

(y = 0, 1, . . . , ∞; µ > 0).

Therefore, from Definition 1.1, the expectation is
∞
∑
e−µ µy
E(Y ) =
y
.
y!
y=0

The sum can be computed by noting there is no contribution from y = 0,
cancelling y in the numerator and denominator of the summand, and then
going back to a sum starting at y = 0:
E(Y ) =

∞
∞
∞
∑
∑
e−µ µy ∑ e−µ µy
µy−1
y
y
=
= e−µ µ
y!
y!
(y − 1)!
y=1
y=1
y=0

= e−µ µ

∞
∑
µy
y=0

y!

= e−µ µeµ = µ.

2. The simplest version of Definition 1.3 to use here is Var(Y ) = E(Y 2 ) − (E(Y ))2 .
It requires computation of E(Y 2 ), which proceeds in a similar way to E(Y ):
2

E(Y ) =

∞
∑

y

−µ y
µ
2e

y=0

=µ

(∞
∑
y=0

y!

=

∞
∑

y

2e

y=1

yfY (y) +

∞
∑

∞
∞
∑
∑
µ
e−µ µy−1
e−µ µy
=µ
y
=µ
(y + 1)
y!
(y − 1)!
y!
y=1
y=0
)

−µ y

fY (y)

= µ(E(Y ) + 1) = µ(µ + 1),

y=0

using E(Y ) from part 1 and noting that the PMF sums to 1.
Hence,
Var(Y ) = E(Y 2 ) − (E(Y ))2 = µ(µ + 1) − µ2 = µ.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-3
Solution 1.5
4. (1 − π)y
Solution 1.6
1. 1 − π
3. Geom1 (π)
Solution 1.7
1. The exponential distribution has PDF
fY (y) = λe−λy
Therefore, from Definition 1.1,

(0 < y < ∞; λ > 0).
∫ ∞

E(Y ) =

yλe−λy dy.

0

We can carry out the integration in several ways.
Integration by parts gives
∫ ∞
∫ ∞
∫ ∞
de−λy
−λy ∞
−λy
dy = −ye
E(Y ) =
yλe
dy =
(−y)
−
(−1)e−λy dy
0
dy
0
0
(
) 0
∫ ∞
∞
1
1
1
= (0 − 0) +
e−λy dy = − e−λy = 0 − −
= .
λ
λ
λ
0
0
Alternatively,
∫ ∞
∫ ∞ −λy
∫ ∞
d
de
−λy
E(Y ) =
yλe
dy = −λ
dy = −λ
e−λy dy
dλ
dλ 0
0
0
)
(
(
(
)
∞)
d
d 1
1
1 −λy
d
1
= −λ
= −λ − 2
− e
= −λ
− (0 − 1) = −λ
dλ
λ
dλ
λ
dλ λ
λ
0
1
= .
λ
2. Again using integration by parts,
∫ ∞
∫ ∞
de−λy
2
2
−λy
dy
E(Y ) =
y λe
dy =
(−y 2 )
dy
0
0
∫ ∞
∫ ∞
2 −λy ∞
−λy
= −y e
−
(−2y)e
dy = (0 − 0) + 2
ye−λy dy
0
0
0
∫
2 ∞
2
2
1
2
=
yλe−λy dy = E(Y ) =
= 2,
λ 0
λ
λλ
λ
with E(Y ) taken from part 1.
Hence,
2
Var(Y ) = E(Y ) − (E(Y )) = 2 −
λ
2

2

( )2
1
1
= 2.
λ
λ

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-4

CHAPTER 10. SOLUTIONS

Solution 1.8
1. Think of the integrand in Γ(1) as the PDF of one of the continuous distributions
in Table 1.4, which must integrate to 1.
2. Substitute x2 = 2y in the integrand and introduce further constants to make
the integrand one of the continuous distributions in Table 1.4.
Solution 1.9
The transformation here is Z = g(Y ) = 1/Y . Hence Y = g −1 (Z) = 1/Z, and
dg −1 (z)
1
= − 2.
dz
z
We also know that Y has PDF
fY (y) =

1
λ(λy)ν−1 e−λy
Γ(ν)

(0 < y < ∞; ν > 0; λ > 0).

Applying the result in (1.3),
( )ν−1
1
1 1
λ
dg −1 (z)
−1
fY (g (z)) = 2 fY (1/z) = 2
fZ (z) =
λ
e−λ/z
dz
z
z Γ(ν)
z
( )ν
1 1 λ
=
e−λ/z (0 < y < ∞; ν > 0; λ > 0).
Γ(ν) z z
Solution 1.10
1. e−λy from the CDF in Example 1.4
2. e−λy for y > 0
3. Expon (λ)
Solution 1.11
Because of the symmetry of the normal PDF around µ,
Pr(Z < µ) = 0.5.
As exp(·) is a monotonically increasing transformation,
Pr(eZ < eµ ) = 0.5,
and hence from the definition of the median, Y = eZ has median eµ .

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-5
Solution 1.12
1. 0.7 and 0.6, respectively
2. 0.21 and 0.24, respectively
3. 0.18
4. E(B) = 1.3 and Var(B) = 0.81
Solution 1.13
As Y = min(X, k) is a function of the random variable X, compute the expectation
of a function of a random variable:
∫ ∞
E(Y ) =
min(x, k)λe−λx dx.
0

Then break the integral into two parts:
∫ k
∫ ∞
∫ k
∫ ∞
−λx
−λx
−λx
E(Y ) =
xλe
dx +
kλe
dx = λ
xe
dx + k
λe−λx dx. (10.1)
0

Using the result

k

0

(

∫
ax

ax

xe dx = e

1
x
− 2
a a

k

)
,

with a = −λ, the first integral in (10.1) is
)k
)
(
(
∫ k
1
1
1
x
k
−λx
−λx
−λk
xλe
dx = e
− 2
=e
− 2 + 2.
−λ λ
−λ λ
λ
0
0
The second integral in (10.1) is immediate from the exponential distribution’s survival
function:
∫ ∞
λe−λx = Pr(X > k) = e−λk .
k

Putting these results together,
(
(
)
)
k
1
1
1 − e−λk
1
e−λk
−λk
E(Y ) = λ e
− 2 + 2 + ke−λk = −
+ =
.
−λ λ
λ
λ
λ
λ
Solution 1.14
3. Use the two previous results rather than the definition of expectation.
4. The proof should use fX,Y (x, y), the joint PDF of X and Y , without making
any assumptions about the joint distribution such as independence.
5. Use the previous results rather than the definition of expectation.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-6

CHAPTER 10. SOLUTIONS

Solution 1.15
There is no need to manipulate integrals or sums. Instead work with expectations
and use the results of Exercise 1.14.
Solution 1.16
3. Use the two previous results rather than the definition of variance.
5. Use the previous results and the result of Exercise 1.15 rather than the definition
of variance.
Solution 1.19
1. Using the result for the expectation of a linear combination of random variables,
E(Y ) = E(B1 + · · · + Bn ) = E(B1 ) + · · · + E(Bn ) = nπ.
2. Using the result for the variance of a linear combination of random variables,
Var(Y ) = Var(B1 + · · · + Bn ) = Var(B1 ) + · · · + Var(Bn ) = nπ(1 − π),
because the Bi are independent and hence all covariance terms are zero.
3. Using the result for the MGF of a sum of independent random variables,
MY (t) =

n
∏

MBi (t) = (1 − π + πet )n .

i=1

4. Bin (n, π), because the MGF identifies the distribution.
Solution 1.20
∑
2. exp ( ni=1 µi (et − 1))
3. Poisson. Be sure to give the value of the Poisson parameter.
Solution 1.22
4. NegBin (n, π)
Solution 1.23
1. Follow the steps of Example 1.24.
2. Note that Y is a linear function of Z.
(a) Use the results on expectation and variance of a linear function of a random
variable.
(b) Apply Lemma 1.2.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-7
Solution 1.25
1. λ/(λ − bt)
2. Expon (λ/b)
Solution 1.26
1. E(Y )
2

2. E(Y ) = eµ+σ /2 ; note that eµ is the median of Y (Exercise 1.11).
Solution 2.1
Write Z as a linear function of Y , i.e., Z = a + bY , then apply the method of
Example 1.30 to establish the distribution of Z.
Solution 2.2
1. The MGF of Yi is exp (µt + 12 σ 2 t2 ) (−∞ < t < +∞).
2. Since we have a sum of independent random variables Yi here, we can use the
result on the MGF of a sum of independent random variables:
MX (t) =

n
∏

MYi (t).

i=1

Then, substituting the MGF from part 1:
(
)
(
)
n
n
∏
∏
1 22
1 22
MX (t) =
MYi (t) =
exp µt + σ t = exp nµt + nσ t
2
2
i=1
i=1
(−∞ < t < +∞).
3. In general, if the MGF of Y is MY (t), then Z = a + bY has MGF MZ (t) =
exp (at)MY (bt). As Ȳ = X/n, and we already have MX (t) from part 1, we have
(
)
1 σ2 2
MȲ (t) = MX (t/n) = exp µt +
t
(−∞ < t < +∞).
2 n
4. Apply the general result in part 3 again, this time to the linear function
√
√
Ȳ − µ
n
−µ n
√ =
+
Z=
Ȳ .
σ
σ
σ/ n
From MȲ (t) in part 3, we have
( √ )
(√ )
−µ n
n
MZ (t) = exp
t MȲ
t
σ
σ
( √
)
( √ )
µ n
1 σ 2 nt2
−µ n
t exp
t+
= exp(t2 /2)
= exp
2
σ
σ
2 n σ
(−∞ < t < +∞).
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-8

CHAPTER 10. SOLUTIONS
This is the MGF of a normal distribution (see part 1) with µ = 0 and σ 2 = 1,
and the MGF uniquely identifies a distribution. Thus, Z has a standard normal
distribution.

Solution 2.3
The χ21 PDF is obtained by putting d = 1 in the χ2d PDF given in Table 1.4. Rearrange
the integral in the definition of the MGF so that it includes the integral of a gamma
PDF.
Solution 2.5
The χ2d PDF is given in Table 1.4. Rearrange the integral so that it includes the
integral of a gamma PDF.
Solution 2.6
2. Do not start with an assumed PDF for Y , or you will have a circular argument
in part 3. No integration is required.
3. Find this MGF in Table 1.4 or use the result of Exercise 2.5.
4. d
5. 2d
Solution 2.7
Use the result of Exercise 2.6.
Solution 2.8
2. Yes
Solution 2.10
1. The widths are 8.53 percentage points for 90% confidence and 13.78 percentage
points for 99% confidence.
3. The widths are 8.29 percentage points for 90% confidence and 12.99 percentage
points for 99% confidence.
4. About 2.9% wider for 90% confidence and about 6.1% wider for 99% confidence.
Solution 2.11
2. (c) [92.0, 100.2]

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-9
Solution 2.12
4. 1 and 2, respectively
5. σ 2
6. 2σ 4
7. l = 0.000982 and u = 5.02
Solution 2.13
1. 1
2. 2/(n − 1)
3. (a) 1
(b) Find a relevant theorem or lemma and describe briefly how it applies to
the random variable S 2 /σ 2 .
4. Consider the properties of S 2 /σ 2 as n → ∞.
7. Don’t forget that parts 1 and 2 relate to the distribution of S 2 divided by σ 2 .
Solution 2.14
Use the fact that a random variable with a χ2d distribution arises as the sum of squares
of d independent standard normal random variables, and apply the CLT.
Solution 2.16
1. (a) 0
(b) 1/3
(c) (et − e−t )/(2t)
(d) 0 and 1/3
2. (a) 0
(b) n/3
( t −t )n
(c) e −e
2t
√

3. (a) Z = n3 Y
( √
) )n
( (√ )
exp t 3/n −exp −t 3/n
√
(b)
2t

3/n

t2 /2

(c) e

(d) Standard normal

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-10

CHAPTER 10. SOLUTIONS

Solution 2.17
Find an example where the normal approximation to the binomial is used. Argue
that the use is compatible with Definition 2.1 on convergence in distribution. In
particular, does Definition 2.1 mention a PMF or a PDF?
Solution 2.18
3. Less than or equal to 0.26
5. 0.05
6. (a) N (µ, σ 2 /n)
(b) 0.05
(c) No, it is an exact property of the normal distribution.
Solution 3.2
1. Applying the result for the expectation of a linear combination of random variables to Ȳ gives E(µ̃) = µ.
2. Applying the result for the variance of a linear combination of independent
random variables to Ȳ gives Var(µ̃) = µ/n.
3. Yes, it is unbiased, and its variance goes to zero as n → ∞.
4. (a) Using the result for the expectation of a linear function of a random variable,
g
E(Var(µ̃))
= E(µ̃/n) = E(µ̃)/n = µ/n = Var(µ̃).
(b) Using the result for the variance of a linear function of a random variable,
g
Var(Var(µ̃))
= Var(µ̃/n) = Var(µ̃)/n2 = µ/n/n2 = µ/n3 .
(c) Yes, it is unbiased, and its variance goes to zero as n → ∞.
Solution 3.3
Yes. Use the results of Exercise 2.13.
Solution 3.4
3. The bias is −σ 2 /n. Hence,
( )
( )
( ) ( σ 2 )2 2(n − 1)σ 4
(2n − 1)σ 4
2
2
2
e
e
+
MSE σ = Bias σ + Var σe2 = −
=
.
n
n2
n2
4. Yes. The MSE goes to zero as n → ∞.

© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-11
5. S 2 is an unbiased estimator of σ 2 , because E(S 2 ) = σ 2 , whereas σe2 is biased.
On the other hand, Var(S 2 ) = 2σ 4 /(n − 1) is greater than
(
)
( ) 2(n − 1)σ 4
1
1
4
2
e
Var σ =
= 2σ
−
n2
n n2
for all n ≥ 2.
6. Compare the two MSEs:
MSE(S 2 ) = Var(S 2 ) =
and

2
σ4
n−1

( ) 2n − 1
σ 4 (from part 3)
MSE σe2 =
2
)
(n
2
1
σ4.
=
−
n n2

( )
Clearly, MSE σe2 < MSE(S 2 ) for all n ≥ 2, and σe2 is more accurate in the
sense of MSE.
Solution 3.5
1. (a) Using the result on the expectation of a linear combination of random
variables,
(
)
Y1 + Y2 + · · · + Yn
1
E(µ̃) = E
= (E(Y1 ) + E(Y2 ) + · · · + E(Yn ))
n
n
1
= (nµ) = µ.
n
Hence, µ̃ = Ȳ is an unbiased estimator of µ.
(b) Using the result on the variance of a linear combination of random variables
and noting that all Cov(Yi , Yj ) terms are zero because the Yi are assumed
independent,
)
(
Y1 + Y2 + · · · + Yn
Var(µ̃) = Var
n
1
= 2 (Var(Y1 ) + Var(Y2 ) + · · · + Var(Yn ))
n
) 2ϕ2
1 (
.
= 2 2nϕ2 =
n
n
Therefore,
√
2ϕ2
sd(µ̃) =
.
n
For the given numbers,
√
√
2ϕ2
200
sd(µ̃) =
=
= 2.828.
n
25
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

10-12

CHAPTER 10. SOLUTIONS

2. (b) One execution of the code produces 1.012 for the sample mean of the
10 000 sample medians. You will obtain a slightly different value from
your (different) random samples. An unbiased estimator has an expected
value of 1. Based on the simulation of the sampling distribution, the
estimate of the expected value, 1.012, is close to 1. Hence, it appears that
the bias is very small and could be zero.
(c) An estimate of the true standard deviation of the sample median is 2.31.
Again your estimate will be slightly different.
3. The estimate of the true standard deviation of the sample mean is 2.82, which
agrees well with the theoretical calculation.
4. The sample mean is unbiased and the sample median appears to be unbiased or
have negligible bias. However, the sample median has a much smaller standard
deviation in the simulation. Hence, the sample median appears to be more
accurate.
Solution 4.1
1. −40µ + 10 ln(µ)
4. 0.25
5. 0.0791
6. 0.25 ± 0.155
7. 0.38. Adapt the argument in Section 4.6. In particular, the tail probability
α = 0.05 will be entirely in one tail, instead of divided as it was in Figure 4.9.
8. 0.68
9. 0 faults has an expected frequency of 31.2, etc; yes
Solution 4.2
1. The likelihood function is
fY1 ,...,Y298 (y1 , . . . , y298 | µ) =

298
∏
i=1

fYi (yi | µ) =

298 −µ yi
∏
e µ

yi !

i=1

∑298

e−298µ µ i=1 yi
=
,
∏298
i=1 yi !

since Y1 , . . . , Y298 are independent random variables.
2. The likelihood function is viewed as a function of µ.
3. The log likelihood is
( 298 )
∑
ln fY1 ,...,Y298 (y1 , . . . , y298 | µ) = −298µ +
yi ln(µ) + c,
i=1
© Copyright William J. Welch 2009–2019. All rights reserved.
Not to be copied, used, or revised without explicit written permission from the copyright owner.

2019.8.14

