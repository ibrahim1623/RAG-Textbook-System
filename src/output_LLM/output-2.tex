Here is the provided text converted into LaTeX code:

```latex
\documentclass{book}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}

\begin{document}

© Copyright William J. Welch 2009--2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

\section{EXERCISES} 

\subsection{Exercise 1.13}

For a specific type of property insurance claim, an actuary models the customer’s loss by the random variable $X \sim \text{Expon}(\lambda)$. But the particular policy has a limit $k$ on the amount that the insurance company has to pay. Thus, when a claim is made the company pays out $Y = \min(X, k)$. What is $E(Y)$?

\subsection{Exercise 1.14}

Let $X$ and $Y$ be continuous random variables with finite expectations, and let $a$, $b$, and $c$ be finite constants. From the definition of expectation, prove the following results.
\begin{enumerate}
    \item $E(a + X) = a + E(X)$.
    \item $E(bX) = bE(X)$.
    \item $E(a + bX) = a + bE(X)$.
    \item $E(X + Y) = E(X) + E(Y)$.
    \item $E(a + bX + cY) = a + bE(X) + cE(Y)$.
    \item If $X$ and $Y$ are discrete random variables, how are these proofs changed?
\end{enumerate}

\subsection{Exercise 1.15}

Let $X$ and $Y$ be random variables with finite covariance, and let $a$ and $b$ be finite constants. From the definition of covariance, prove the result
\[
\text{Cov}(a + bY, c + dZ) = bd \text{Cov}(Y, Z)
\]
in (1.11).

\subsection{Exercise 1.16}

Let $X$ and $Y$ be random variables with finite variances, and let $a$, $b$, and $c$ be finite constants. Starting from the definition of variance, i.e., $\text{Var}(X) = E(X^2) - (E(X))^2$, prove the following results. (Hint: The definition of variance is in terms of expectations; use the results of Exercise 1.14.)
\begin{enumerate}
    \item $\text{Var}(a + X) = \text{Var}(X)$.
    \item $\text{Var}(bX) = b^2 \text{Var}(X)$.
    \item $\text{Var}(a + bX) = b^2 \text{Var}(X)$.
    \item $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y)$.
    \item $\text{Var}(a + bX + cY) = b^2 \text{Var}(X) + c^2 \text{Var}(Y) + 2bc\text{Cov}(X, Y)$.  (This is a special case of the more general result in Section 1.7.7.)
\end{enumerate}

© Copyright William J. Welch 2009--2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14


\subsection{Exercise 1.17}
Let $Y_1, \ldots, Y_n$ be independent random variables, each taking the values 0 or 1 with probabilities $1 - \pi$ and $\pi$, respectively. Here $\pi$, the probability that $Y = 1$, is an unknown parameter to be estimated. 
\begin{enumerate}
    \item Show that $E(Y_i) = \pi$ and $\text{Var}(Y_i) = \pi(1 - \pi)$. 
    \item Consider the estimator $\tilde{\pi} = \frac{1}{n}\sum_{i=1}^n Y_i$ of $\pi$. (This is simply the proportion of 1’s amongst $Y_1, \ldots, Y_n$. It is a random variable because the $Y_i$ are random.)
    \begin{enumerate}
        \item Show that $E(\tilde{\pi}) = \pi$, i.e., $\tilde{\pi}$ is an unbiased estimator of $\pi$. 
        \item Show that $\text{Var}(\tilde{\pi}) = \frac{\pi(1 - \pi)}{n}$.
    \end{enumerate}
\end{enumerate}

\subsection{Exercise 1.18}
[Quiz \#1, 2009-10, Term 1] Let $B$ be a Bernoulli random variable taking values $b = 0, 1$. Its PMF is given by $f_B(0) = \Pr(B = 0) = 1 - \pi$ and $f_B(1) = \Pr(B = 1) = \pi$. Thus, $B \sim \text{Bern}(\pi)$. Show each of the following results. For full marks you need to be explicit about the mathematical definition of the quantity involved ($E(\cdot)$, $\text{Var}(\cdot)$ or MGF) and how the definition is used for this specific problem. 
\begin{enumerate}
    \item Show $E(B) = \pi$. 
    \item Find $E(10B)$. 
    \item Show $\text{Var}(B) = \pi(1 - \pi)$. 
    \item Show that the moment generating function (MGF) of $B$ is $M_B(t) = 1 - \pi + \pi e^t$. 
    \item Check that the MGF exists for $t$ in an open neighborhood of zero. 
    \item Use the MGF to find $E(B)$.
\end{enumerate}

\subsection{Exercise 1.19}
[Quiz \#1, 2009-10, Term 1] Let $Y = B_1 + \ldots + B_n$, where the random variables $B_1, \ldots, B_n$ are independent and each has a $\text{Bern}(\pi)$ distribution. You may use the results in Exercise 1.18 that $B_i$ has mean $\pi$, variance $\pi(1 - \pi)$, and MGF $1 - \pi + \pi e^t$. Also, $n$ is some fixed number. 
\begin{enumerate}
    \item Find $E(Y)$. 
    \item Find $\text{Var}(Y)$. 
    \item Find the moment generating function of $Y$. 
    \item Hence, what is the distribution of $Y$? 
\end{enumerate}

© Copyright William J. Welch 2009--2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

\subsection{Exercise 1.20}
Let $Y \sim \text{Pois}(\mu)$. Thus, the PMF of $Y$ is
\[
f_Y(y) = \frac{e^{-\mu} \mu^y}{y!} \quad (y = 0, 1, \ldots, \infty; \mu > 0).
\]
\begin{enumerate}
    \item Show that $Y$ has the MGF
    \[
    M_Y(t) = \frac{e^{\mu(e^t - 1)}}{t}.
    \]
    \item Let $Y_1, \ldots, Y_n$ be independent Poisson random variables, where $Y_i$ has mean $\mu_i$, i.e., $Y_i \sim \text{Pois}(\mu_i)$. Thus, the random variables may have different means and are not necessarily identically distributed. What is the MGF of $\sum_{i=1}^n Y_i$? 
    \item Hence, what is the distribution of $\sum_{i=1}^n Y_i$?
\end{enumerate}

\subsection{Exercise 1.21}
Let $Y \sim \text{Unif}(a, b)$. Use the expansion of its MGF in Example 1.29 to show the following properties:
\begin{enumerate}
    \item $E(Y) = \frac{(a + b)}{2}$;
    \item $\text{Var}(Y) = \frac{(b - a)^2}{12}$.
\end{enumerate}

\subsection{Exercise 1.22}
Let $Y \sim \text{Geom1}(\pi)$. Thus, the PMF of $Y$ is
\[
f_Y(y) = (1 - \pi)^{y - 1} \pi \quad (y = 1, 2, \ldots, \infty; 0 < \pi < 1).
\]
\begin{enumerate}
    \item Show that $Y$ has the MGF
    \[
    M_Y(t) = \frac{\pi e^t}{1 - (1 - \pi)e^t}.
    \]
    \item From the MGF show that 
    \[
    E(Y) = \frac{1}{\pi}
    \]
    and 
    \[
    \text{Var}(Y) = \frac{1 - \pi}{\pi^2}.
    \]
    \item Let $Y_1, \ldots, Y_n$ be IID $\text{Geom1}(\pi)$ random variables. What is the distribution of $\sum_{i=1}^n Y_i$? What is the MGF of $Y_1 + \ldots + Y_n$?
\end{enumerate}

© Copyright William J. Welch 2009--2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

\subsection{Exercise 1.23}
Let $Y \sim N(\mu, \sigma^2)$, i.e., the normal distribution with mean $\mu$ and variance $\sigma^2$. This exercise shows in two ways that the MGF of $Y$ is 
\[
M_Y(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}.
\]
\begin{enumerate}
    \item Apply the definition of the MGF in Definition 1.7 directly to the $N(\mu, \sigma^2)$ PDF to find the MGF of $Y$.
    \item Let $Z$ have a standard normal distribution, i.e., $N(0, 1)$. Its MGF is 
    \[
    M_Z(t) = e^{\frac{1}{2} t^2} \quad \text{(see Example 1.24)}.
    \]
    Now let $Y = \mu + \sigma Z$.
    \begin{enumerate}
        \item Verify that $E(Y) = \mu$ and $\text{Var}(Y) = \sigma^2$ as required. 
        \item Find the MGF of $Y$ from the MGF of $Z$.
    \end{enumerate}
\end{enumerate}

\subsection{Exercise 1.24}
Let $Y \sim N(\mu, \sigma^2)$. Starting from the MGF of $Y$, i.e., 
\[
M_Y(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2},
\]
this exercise verifies the first two moments.
\begin{enumerate}
    \item Use the MGF to show that $E(Y) = \mu$.
    \item Use the MGF to show that $E(Y^2) = \mu^2 + \sigma^2$, and hence that $\text{Var}(Y) = \sigma^2$.
\end{enumerate}

\subsection{Exercise 1.25}
Let $Y \sim \text{Expon}(\lambda)$. Consider multiplying $Y$ by a constant to give a new random variable, $Z = bY$, where $b > 0$.
\begin{enumerate}
    \item Table 1.4 says the MGF of $Y$ is $\frac{\lambda}{\lambda - t}$. What is the MGF of $Z$? 
    \item What is the distribution of $Z$? 
    \item Apply the same argument to the gamma distribution to show that if $Y \sim \text{Gamma}(\nu, \lambda)$, then $Z = bY \sim \text{Gamma}(\nu, \lambda/b)$.
\end{enumerate}

\subsection{Exercise 1.26}
A random variable $Y$, taking positive values, is said to have a log-normal distribution if $Z = \ln(Y)$ has a $N(\mu, \sigma^2)$ distribution. This exercise finds the expectation of $Y$ from the MGF of $Z$.
\begin{enumerate}
    \item The definition of the MGF of $Z$ is $E(e^{tZ})$. What expression do we get if we put $t = 1$ in this definition? 
    \item Look up the MGF of $Z$ (see Table 1.4 or Exercise 1.23), and put $t = 1$ in it. Hence, what is $E(Y)$?
\end{enumerate}

© Copyright William J. Welch 2009--2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

% The continuation of the document should follow the same structure.
\end{document}
```

This LaTeX document reproduces the content while accurately converting mathematical symbols and other elements into LaTeX format. Adjustments can be made for the layout depending on your specific needs (e.g., chapter headings, sectioning, etc.) as necessary.