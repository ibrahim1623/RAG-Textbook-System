```markdown
Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-28

# CHAPTER 1. PROBABILITY TOOLS

Grade  
\( g \)  
55.5  
69.0  
72.0  
85.5  

\( f_G(g) \)  
0.1  
0.1  
0.2  
0.6  

**Table 1.7:** Probability mass function for the course grade of a randomly chosen student in a given STAT 305 section  
takes the value 55.5 if \( Y \) is 60 and \( Z \) is 50, with joint probability 0.1 from Table 1.6. From \( f_G(g) \) it is easily verified that \( E(G) = 78.15 \). Computing the expectation of  
\( G \) by first computing \( f_G(g) \) would be a very tedious numerical problem, however,  
if many random variables were being combined to form \( G \). Alternatively, applying (1.9) which says that the expectation of a linear combination of random variables is the linear combination of their expectations, we  
have  
\( E(G) = E(0.55Y + 0.45Z) = 0.55E(Y ) + 0.45E(Z) = 78.15 \). This calculation requires only the expectations of the marginal distributions; it  
does not require any other properties of the joint distribution. ♢♢♢

1.7.7

## Variance of a linear combination of random variables

Providing all variances exist,  
\[
Var\left(a_0 + \sum_{i=1}^{n} a_i Y_i\right) = \sum_{i=1}^{n} a_i^2 Var(Y_i) + 2\sum_{i=1}^{n}\sum_{j=i+1}^{n} a_i a_j Cov(Y_i, Y_j).
\]  
(1.10)

A simpler version of this result with \( n = 2 \) random variables is proved in Exercise 1.16. Special cases of the result include:  
\( Var(Y + Z) = Var(Y ) + Var(Z) + 2Cov(Y, Z) \)  
\( Var(Y − Z) = Var(Y ) + Var(Z) − 2Cov(Y, Z) \)  
\( Var(a + bY ) = b^2 Var(Y) \). When using the general rule or special cases in deriving another result, it is helpful  
to explain the step in the argument by a statement such as “using the result on the  
variance of a linear combination of random variables”. 

Example 1.20 (Course grade: variance of a linear combination)  
The distribution of grades in Table 1.7 gives \( Var(G) = 99.65 \). Alternatively,  
from (1.10) we find  
\[
Var(G) = (0.55)^2 Var(Y ) + (0.45)^2 Var(Z) + 2(0.55)(0.45)Cov(Y, Z) = 99.65.
\]  
© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. ♢♢♢  
2019.8.14

1.7. SEVERAL VARIABLES

1-29

Example 1.21 (Gamma distribution: mean and variance)  
The gamma distribution (see Table 1.4) has PDF  
\[
f_Y (y) = \frac{1}{\lambda (\lambda y)^{\nu-1} e^{-\lambda y}} {\Gamma(\nu)}
\]  
\( (0 < y < \infty; \nu > 0; \lambda > 0) \),  

which we write as \( \text{Gamma} (\nu, \lambda) \). It has a similar form to the exponential distribution, for which we have already found the mean and variance, and a similar  
approach could be used again. With a slight loss of generality we can find the gamma distribution’s mean and  
variance rather more simply, however. If the parameter \( \nu \) is an integer greater  
than or equal to 1 (this is the loss of generality), then a \( \text{Gamma} (\nu, \lambda) \) random  
variable \( Y \) can be generated by:  
\[
Y = Y_1 + \cdots + Y_\nu,
\]  
where the \( Y_i \) are independent \( \text{Expon} (\lambda) \) random variables. (This result will  
be proved in Example 1.31.) We know \( Y_i \) has mean \( 1/\lambda \) and variance \( 1/\lambda^2 \). Hence it immediately follows that a \( \text{Gamma} (\nu, \lambda) \) random variable has mean \( \nu/\lambda \)  
(from (1.9)) and variance \( \nu/\lambda^2 \) (from (1.10)). Note that because the \( Y_i \) are independent, all covariance terms in the variance calculation are zero. This result  
actually holds for general \( \nu > 0 \). ♢♢♢  

Independence can be an important assumption in formal statistical models and derivations. For instance, the result (1.10) on the variance of a linear combination of random variables is applied to derive the variance of a sample mean or sample proportion  
from independent observations \( Y_1, \ldots, Y_n \) (e.g., Exercise 1.17). But simple results are  
only obtained when all distinct pairs of observations are independent and hence all  
\( Cov(Y_i, Y_j) \) terms for \( i \neq j \) are all zero. If the assumption of independence is false,  
the claimed variance of the sample mean or proportion could be highly misleading. Furthermore, the assumption of independence is usually made out of necessity. To  
take account of covariance terms between any two observations in the calculation of  
the variance of a linear combination, one needs some insight into the structure of the  
covariance, insight which is often lacking. In practice, appealing to the way the data  
were collected—as a random sample or via randomization in an experiment—is the  
only feasible justification of an independence assumption. 

1.7.8

## Covariance between linear functions or combinations of random variables

There are analogous results for the covariance between linear functions of random  
variables:  
\[
Cov(a + bY, c + dZ) = bdCov(Y, Z)
\]  
(1.11)  
This is proved as Exercise 1.15. © Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-30

# CHAPTER 1. PROBABILITY TOOLS

Similarly, for linear combinations of random variables,  
\[
Cov\left( \sum_{i=1}^{n} a_i Y_i, \sum_{j=1}^{m} b_j Z_j \right) = \sum_{i=1}^{n} \sum_{j=1}^{m} a_i b_j Cov(Y_i, Z_j).
\]  

Note that in general the two linear combinations can have different numbers of random  
variables, \( n \) and \( m \), respectively. When they involve the same random variables, i.e.,  
with \( m = n \) and \( Y_i = Z_i \) for \( i = 1, \ldots, n \), we have  
\[
Cov\left( \sum_{i=1}^{n} a_i Y_i, \sum_{j=1}^{n} b_j Y_j \right) = \sum_{i=1}^{n} \sum_{j=1}^{n} a_i b_j Cov(Y_i, Y_j).
\]  

1.7.9

## Bivariate normal distribution

Two continuous random variables \( Y_1 \) and \( Y_2 \) with a bivariate normal distribution have  
joint PDF given by  
\[
f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2\pi \det(Σ)} \exp\left(-\frac{1}{2} (y - \mu)^T \Sigma^{-1} (y - \mu)\right),
\]  

where  
\[
y = \begin{pmatrix} y_1 \\ y_2 \end{pmatrix}, \quad \mu = \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \quad Σ = \begin{pmatrix} \sigma_1^2 & \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 & \sigma_2^2 \end{pmatrix}.
\]  

Here, \( \mu_1 \) and \( \mu_2 \) are the means of \( Y_1 \) and \( Y_2 \), respectively, \( \sigma_1 > 0 \) and \( \sigma_2 > 0 \) are the standard  
deviations of \( Y_1 \) and \( Y_2 \), respectively, \( -1 < \rho < 1 \) is the correlation between \( Y_1 \) and \( Y_2 \), and \( \det(Σ) \) and \( Σ^{-1} \) denote matrix determinant and inverse of \( Σ \), respectively. The off-diagonal element \( \rho \sigma_1 \sigma_2 \) in the covariance matrix \( Σ \) is the covariance between  
\( Y_1 \) and \( Y_2 \). It is zero if \( Y_1 \) and \( Y_2 \) are uncorrelated, i.e., if \( \rho = 0 \). The bivariate normal has the special property that a covariance of zero between the  
two random variables implies they are independent. 

**Lemma 1.1 (Bivariate normal: covariance of 0 implies independence)**  
If \( Y_1 \) and \( Y_2 \) have a joint bivariate normal distribution and their covariance  
(correlation) is zero, then \( Y_1 \) and \( Y_2 \) are independent normal random variables. To show the result, assume \( \rho = 0 \). Independence will follow by showing that the joint  
distribution factorizes. First, we have  
\[
\det(Σ) = \det\begin{pmatrix} \sigma_1^2 & 0 \\ 0 & \sigma_2^2 \end{pmatrix} = \sigma_1^2 \sigma_2^2,
\]

and hence  
\[
\det^2(Σ) = \sigma_1 \sigma_2.
\]  
Second,  
\[
(y - \mu)^T Σ^{-1} (y - \mu) = \left( \begin{pmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{pmatrix} \right)^T \begin{pmatrix} \frac{1}{\sigma_1^2} & 0 \\ 0 & \frac{1}{\sigma_2^2} \end{pmatrix} \begin{pmatrix} y_1 - \mu_1 \\ y_2 - \mu_2 \end{pmatrix} 
\]
\[
= \frac{(y_1 - \mu_1)^2}{\sigma_1^2} + \frac{(y_2 - \mu_2)^2}{\sigma_2^2}.
\]

© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner.  

2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-31

Substituting these two results into the joint distribution gives  
\[
f_{Y_1,Y_2}(y_1, y_2) = \frac{1}{2\pi\sigma_1\sigma_2} \exp\left(-\frac{(y_1 - \mu_1)^2}{2\sigma_1^2} - \frac{(y_2 - \mu_2)^2}{2\sigma_2^2}\right).
\]  

which is the product of a \( N(\mu_1, \sigma_1^2) \) PDF for \( Y_1 \) and a \( N(\mu_2, \sigma_2^2) \) PDF for \( Y_2 \). Hence  
\( Y_1 \) and \( Y_2 \) are independent by Definition 1.5. Note that independence implies covariance of zero for any two random variables (see  
Section 1.7.5), including the bivariate normal. But the result that covariance of zero  
implies independence does not hold in general. The same arguments apply to the multivariate normal with \( n \) variables: if all pairwise  
covariances are zero, the \( n \) random variables are mutually independent and normal. 

1.8

# Moment Generating Functions

1.8.1

## Uses of moment generating functions

The moment generating function (MGF) is a powerful tool for proving probability  
results essential for statistical methods. 
- We can sometimes find the distribution of a sum of IID random variables from  
the MGF of the underlying distribution. This is clearly useful for statistical  
properties of sample totals or sample means, which are sums. For instance:
  - Example 1.31 establishes that the sum of IID exponential random variables  
  has a gamma distribution, a result used for statistical hypothesis testing  
  in Example 7.3. 
  - Exercise 1.20 shows that a sum of independent Poisson random variables  
  has a Poisson distribution. This is again used in hypothesis testing, in  
  Exercise 7.2. 
  - The sum of IID geometric random variables has a negative-binomial distribution (Example 1.33). 
- If we know the MGF of a random variable, it is easy to write down the MGF  
of any linear function of it. This provides an easy proof that a linear function  
of a normal random variable also has a normal distribution (Example 1.30), an  
important property. 
- Using the MGF is a relatively easy way of establishing approximate normality  
of a sample mean or sample total under certain conditions (the central limit  
theorem of Theorem 2.2) and special cases like the approximation of a binomial  
distribution by a normal distribution (Example 2.2). Normal approximations  
are widely used in statistical inference. 
- The properties of the \( \chi^2 \) distribution and hence the sample variance when  
sampling from a normal distribution are readily shown using MGFs (in Section 2.4.2). 

1.8.2 

## Definition of the moment generating function

As its name suggests, the moment generating function generates the moments of a  
distribution or random variable. 

**Definition 1.6 (Moments of a random variable)**  
Let \( Y \) be a random variable. Its \( k \)-th moment for \( k = 1, 2, \ldots \) is \( E(Y^k) \), which  
exists if the expectation is finite. Thus, the first moment with \( k = 1 \) is simply \( E(Y) \). The first two moments, \( E(Y^1) \) and  
\( E(Y^2) \), give the variance from \( Var(Y ) = E(Y^2) − (E(Y))^2 \). The MGF, once found,  
can generate all the moments of a random variable, including these two. The MGF is found by computing an expectation. 

**Definition 1.7 (Moment generating function)**  
Let \( Y \) be a random variable. The moment generating function (MGF) for \( Y \) is defined as  
\[
M_Y(t) = E(e^{tY}),
\]  
if it exists for \( t \) in a neighbourhood of 0, i.e., for \( t \) in the open interval  
\(-T < t < T\), where \( T > 0 \). Note that the expectation is with respect to the distribution of \( Y \), and is just the  
expectation of a function of \( Y \), namely \( e^{tY} \). The parameter \( t \) is a dummy variable. The MGF has to exist in an interval around \( t = 0 \) because manipulations of it will  
involve the derivatives at \( t = 0 \) and Taylor series approximation at \( t = 0. 

Example 1.22 (Exponential distribution: MGF)  
Let \( Y \) be distributed \( \text{Expon} (\lambda) \). As this is a continuous random variable, we  
compute the expectation in the MGF via integration:  
\[
M_Y(t) = E(e^{tY}) = \int_0^{\infty} e^{ty}f_Y(y) dy = \int_0^{\infty} e^{ty}\lambda e^{-\lambda y} dy.
\]  
The integrand converges and the MGF exists if \( \lambda - t > 0 \). Carrying out the integration is straightforward here, but this simple example is  
an opportunity to show a method that avoids explicit integration in more difficult cases. With the condition \( \lambda - t > 0 \), we can rewrite the integral as  
\[
\int_0^{\infty} (lambda - t)e^{-(\lambda - t)y} dy.
\]  

© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-33

The integrand is now the PDF of an exponential random variable with parameter  
\( \lambda - t \), and like any PDF it must integrate to 1. Thus, the MGF of the \( \text{Expon}(\lambda) \) distribution is  
\[
M_Y(t) = \frac{\lambda}{\lambda - t} 
\]  
(1.12)  
which exists for \( t < \lambda \). We also note that \( \lambda > 0 \) (see Table 1.4), so the interval  
\( t < \lambda \) includes an open interval around \( t = 0 \), as required by Definition 1.7. ♢♢♢   

Example 1.23 (Gamma distribution: MGF)  
From Table 1.4, the PDF of \( Y \sim \text{Gamma}(\nu, \lambda) \) is  
\[
f_Y(y) = \frac{1}{\lambda (\lambda y)^{\nu-1} e^{-\lambda y}} \Gamma(\nu).
\]  
First, we apply the definition of the MGF and simplify a little:  
\[
M_Y(t) = E(e^{tY}) = \int_0^{\infty} e^{ty} f_Y(y) dy = \int_0^{\infty} e^{ty} \frac{1}{\lambda (\lambda y)^{\nu-1} e^{-\lambda y}} dy
\]  
\[
= \int_0^{\infty} \frac{e^{(t-\lambda)y}}{\lambda (1/y)^{\nu-1} dy},
\]  
which exists if \( t < \lambda \). The integrand is of the form \( y^a e^b^y \), and we note that \( a \) is  
not necessarily an integer. (From Table 1.4, the parameter \( \nu \) takes values \( \nu > 0 \)  
and hence \( a = \nu - 1 > -1 \).) There are many ways to proceed:
- Use standard methods of calculus.
- Look up a table of integrals.
- Use software such as Mathematica or Maple.
- Note that the integrand is very similar to the form of the original gamma PDF and again use the fact that a PDF integrates to 1.  

The last route turns out to be easy. All we need to do is take out a factor:  
\[
\frac{1}{\Gamma(\nu)} \int_0^{\infty} \frac{1}{\lambda} e^{-t}dy
\]
\[
= \frac{1}{(\lambda - t)^{\nu}}.\Gamma(\nu).
\]  

The integrand is now the PDF of a \( \text{Gamma}(\nu, \lambda - t) \) random variable, i.e., with  
the parameter \( \lambda \) replaced by \( \lambda - t \) everywhere, and the integral is 1. We are  
left with just the factor in front of the integral, and the MGF of a \( \text{Gamma}(\nu, \lambda) \) random variable is  
\[
M_Y(t) = \frac{\lambda^{\nu}}{(\lambda - t)^{\nu}}.
\]  
(1.13)  
It exists for \( t < \lambda \) and hence in an open interval around \( t = 0 \), because again  
\( \lambda > 0 \). ♢♢♢   

Example 1.24 (Standard normal distribution: MGF)  
Let \( Z \sim N(0, 1) \), i.e., the standard normal distribution with mean \( \mu = 0 \) and  
variance \( \sigma^2 = 1 \). Substituting these parameter values into the general normal PDF in Table 1.4 gives  
\[
f_Z(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}}.
\]  
Thus,  
\[
M_Z(t) = E(e^{tZ}) = \int_{-\infty}^{\infty} e^{tz} f_Z(z) dz = \int_{-\infty}^{\infty} e^{tz} \frac{1}{\sqrt{2 \pi}} e^{-\frac{z^2}{2}} dz .
\]  

By completing the square and using properties of the normal distribution, we show:  
\[
\int_{-\infty}^{\infty} e^{-\frac{1}{2}(z - 2tz)^2} dz = e^{t^2/2}.
\]  

The last integral is 1, because we see that the integrand is a normal PDF (with  
\( \mu = t \) and \( \sigma^2 = 1 \)). We also note that \( e^{t^2/2} \) and hence \( M_Z(t) \) exists for \( -\infty < t < \infty \). ♢♢♢  

The method used in Example 1.24 can also be applied to find the MGF of \( Y \sim N(\mu, \sigma^2) \),  
i.e., a normal random variable with arbitrary mean and variance. The MGF of \( Y \) is  
\[
M_Y(t) = e^{\mu t + \frac{1}{2} \sigma^2 t^2}.
\]  
(1.14)  
The details are left to Exercise 1.23. 

Example 1.25 (Binomial distribution: MGF)  
The binomial distribution has PMF  
\[
f_Y(y) = \binom{n}{y} \pi^y (1 - \pi)^{n - y} \quad (y = 0, 1, \ldots, n).
\]  
Because it takes discrete values, the MGF is found by summation:  
\[
M_Y(t) = E(e^{tY}) = \sum_{y=0}^{n} e^{ty} f_Y(y) = \sum_{y=0}^{n} e^{ty} \binom{n}{y} \pi^y (1 - \pi)^{n - y}.
\]  
Minor simplification is possible by collecting together the \( e^{ty} \) and \( \pi^y \) terms, and  
the task is to evaluate  
\[
M_Y(t) = \sum_{y=0}^{n} \binom{n}{y} (\pi e^t)^y (1 - \pi)^{n - y}.
\]  
For this discrete problem we next try to turn the sum into a sum of a PMF over its possible values. The expression to evaluate looks like the binomial PMF, but a binomial PMF involves complementary probabilities, \( \pi \) and \( 1 - \pi \),  
which sum to 1. In the evaluation, \( \pi e^{t} \) and \( 1 - \pi \) do not sum to 1 for \( t \neq 0 \), but this is easily fixed by dividing them by their sum to create  
\[
\pi̇ = \frac{\pi e^{t}}{1 - \pi + \pi e^t}
\]  
and  
\[
1 - \pi̇ = \frac{1 - \pi}{1 - \pi + \pi e^t}.
\]  

The divisor is cancelled by a factor outside the sum when we rewrite. Therefore, we find:  
The MGF of the binomial distribution is  
\[
M_Y(t) = (1 - \pi + \pi e^t)^n.
\]  
It exists for \( -\infty < t < \infty \). 

1.8.3

♢♢♢

## Finding moments from the MGF

As its name suggests, the MGF for a distribution generates the moments, \( E(Y^k) \). The  
first moment is \( E(Y) \), the mean of \( Y \). The second moment is \( E(Y^2) \); from it and the  
first moment, we can compute the variance, \( Var(Y) = E(Y^2) − (E(Y))^2 \). Similarly,  
skewness, etc., can be computed from higher-order moments. We will prove the result relating the MGF to the moments using a Taylor series  
expansion around \( t = 0 \). This explains the mysterious condition in the definition of  
the MGF that it needs to exist for \( t \) in an open interval around 0. Suppose \( M_Y(t) \) exists in a neighbourhood of \( t = 0 \). Then,  
\[
M_Y^{(k)}(0) = E(Y^k),
\]  
(1.16)  

where \( M_Y^{(k)}(0) \) is \( M_Y(t) \) differentiated \( k \) times and evaluated at \( t = 0 \). To show this  
we make a Taylor series expansion of \( e^{tY} \) in the definition of the MGF:  
\[
M_Y(t) = E(e^{tY}) = E\left(1 + tY + \frac{(tY)^2}{2!} + \frac{(tY)^3}{3!} + \cdots \right).
\]

Differentiating once with respect to \( t \) gives  
\[
M_Y^{(1)}(t) = E(Y) + \frac{t}{2}\ E(Y^2) + \cdots,
\]  
and evaluating at \( t = 0 \) gives \( M_Y^{(1)}(0) = E(Y) \). Similarly, differentiating twice gives  
\[
M_Y^{(2)}(t) = E(Y^2) + \frac{3t^2}{6}\ E(Y^3) + \cdots, 
\]  
and \( M_Y^{(2)}(0) = E(Y^2) \). The general result in (1.16) for \( E(Y^k) \) is just a continuation of  
this process. The proof given here makes it obvious how the Taylor-series expansion of \( e^{tY} \) generates powers of \( Y \) and hence the moments after taking expectation. 

Example 1.26 (Exponential distribution: mean and variance via the MGF)  
The first and second derivatives of the exponential distribution’s MGF in (1.12) are  
\[
M_Y^{(1)}(t) = \frac{\lambda}{(\lambda - t)^2}
\]  
and  
\[
M_Y^{(2)}(t) = \frac{2\lambda^2}{(\lambda - t)^3}.
\]  
Putting \( t = 0 \) in the first expression gives \( E(Y) = M_Y(0) = \frac{1}{\lambda} \). Similarly, \( t = 0 \)  
in the second expression gives \( E(Y^2) = M_Y(0) = \frac{2}{\lambda^2} \), and hence \( Var(Y) =  
E(Y^2) - (E(Y))^2 = \frac{1}{\lambda^2} \). ♢♢♢   

We can compute the mean and variance of the exponential distribution directly (Exercise 1.7), so what is gained by use of the MGF in Example 1.26? The direct attack  
on the mean and variance involves moderately complicated integrals. In contrast the  
integration to find the MGF of the exponential distribution was straightforward. After some minor algebra, we recognized the integral of a PDF, which we  
know must be 1. Differentiating the MGF to get the moments was also easy. So we replaced nontrivial integrations with algebra and differentiation.  
(For a discrete random variable, potentially nontrivial summations are similarly avoided.)

Example 1.27 (Gamma distribution: mean and variance via the MGF)  
First, rewrite the MGF of the gamma distribution in (1.13) as  
\[
M_Y(t) = \frac{\lambda^{\nu}}{(\lambda - t)^{\nu}}.
\]  
The first derivative of \( M_Y(t) \) is  
\[
M_Y^{(1)}(t) = \left(-\nu \frac{1}{\lambda - t}\right)^{\nu + 1}.
\]  

Evaluating at \( t = 0 \) gives  
\[
E(Y) = \frac{\nu}{\lambda}. 
\]  
The second derivative is  
\[
M_Y^{(2)}(t) = \frac{\nu(\nu - 1)}{\lambda^2} (1 + O(t)).
\]  
Thus, \( E(Y^2) = \frac{\nu(\nu + 1)}{\lambda^2} \) and hence  
\[
Var(Y) = E(Y^2) - (E(Y))^2 = \frac{\nu}{\lambda^2}. 
\]  
The same expectation and variance were found in Example 1.21, but the result  
here holds for any \( \nu > 0 \) and not just for positive integer values of \( \nu \). ♢♢♢   

Example 1.28 (Binomial distribution: mean and variance via the MGF)  
From Example 1.25 the MGF of the binomial distribution is  
\[
M_Y(t) = (1 - \pi + \pi e^t)^n. 
\]  
The first two derivatives of the MGF are  
\[
M_Y^{(1)}(t) = n\pi e^t (1 - \pi + \pi e^t)^{n-1}
\]  
and  
\[
M_Y^{(2)}(t) = n\pi e^t(1 - \pi + \pi e^t)^{n-1} + n(n - 1)\pi^2 e^{2t}(1 - \pi + \pi e^t)^{n-2}. 
\]  
Evaluating these derivatives at \( t = 0 \) gives  
\[
E(Y) = n\pi  
\]  
and  
\[
E(Y^2) = n\pi + n(n - 1)\pi^2. 
\]  
Therefore,  
\[
Var(Y) = E(Y^2) - (E(Y))^2 = n\pi + n(n - 1)\pi^2 - (n\pi)^2 = n\pi(1 - \pi). 
\]  
♢♢♢   

The argument to obtain the moments from the MGF hinges on the Taylor series  
expansion at \( t = 0 \). Thus, the MGF has to exist at \( t = 0 \), which was straightforward  
to demonstrate for the examples up to here. The next example is a little more subtle. 

Example 1.29 (Uniform distribution: existence of the MGF)  
If \( Y \sim \text{Unif}(a, b) \), Table 1.4 says its MGF is  
\[
M_Y(t) = \frac{e^{bt} - e^{at}}{(b - a)t} \quad (-\infty < t < \infty).
\]  
The appearance of \( t \) in the denominator may create some doubt about the existence, but note  
that the numerator is also 0 at \( t = 0 \). Expanding the exponential functions, however, shows that  
all is well:  
\[
\frac{(b-a)t + (b-a)\frac{(b-a)t^2}{2!} + (b-a)\frac{(b-a)^2t^3}{3!} + \ldots}{(b-a)t}.
\]  
This equals 1 at \( t = 0 \). Hence, the first two moments give the expectation and variance of \( Y \) in Table 1.4  
(Exercise 1.21). ♢♢♢

1.8.4

## MGF of a linear function or a sum

**Lemma 1.2 (MGF of a linear function of a random variable)**  
If the MGF of \( Y \) is \( M_Y(t) \), then \( Z = a + bY \) has MGF  
\[
M_Z(t) = e^{at} M_Y(bt).
\]  
The result follows from  
\[
M_Z(t) = E(e^{tZ}) = E(e^{t(a+bY)}) = e^{at} E(e^{btY}) = e^{at} M_Y(bt). 

**Lemma 1.3 (MGF of a sum of independent random variables)**  
Suppose \( Y_1, \ldots, Y_n \) are independent random variables, and \( Y_i \) has MGF \( M_{Y_i}(t) \)  
(which must exist). Then the MGF of \( X = Y_1 + \cdots + Y_n \) is  
\[
M_X(t) = \prod_{i=1}^{n} M_{Y_i}(t).
\]  
The result follows from  
\[
M_X(t) = E(e^{tX}) = E\left(e^{t\sum_{i=1}^{n} Y_i}\right) = E_{Y_1, \ldots, Y_n} \left(\prod_{i=1}^{n} e^{t Y_i}\right) = \prod_{i=1}^{n} E_{Y_i} \left(e^{t Y_i}\right) = \prod_{i=1}^{n} M_{Y_i}(t).
\]  

Here, we are using the result that the expectation of a product of independent random variables is the product of expectations. 

1.8.5 

## The MGF identifies a distribution

An important property of the MGF is that it identifies a distribution uniquely. 

**Theorem 1.3 (The MGF identifies a distribution)**  
Let \( Y \) and \( Z \) be two random variables with MGFs \( M_Y(t) \) and \( M_Z(t) \), respectively. If  
\( M_Y(t) = M_Z(t) \) for all \( t \) in an open interval of 0, then  
\( Pr(Y \leq y) = Pr(Z \leq y) \), i.e., \( Y \) and \( Z \) have the same CDF.
```