Â© Copyright William J.Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1.11. EXERCISES

1-49

Exercise 1.13
For a specific type of property insurance claim, an actuary models the customerâ€™s loss
by the random variable X âˆ¼ Expon (Î»). But the particular policy has a limit k on
the amount that the insurance company has to pay. Thus, when a claim is made the
company pays out Y = min(X, k). What is E(Y )? Exercise 1.14
Let X and Y be continuous random variables with finite expectations, and let a, b,
and c be finite constants. From the definition of expectation, prove the following
results. 1. E(a + X) = a + E(X). 2. E(bX) = bE(X). 3. E(a + bX) = a + bE(X). 4. E(X + Y ) = E(X) + E(Y ). 5. E(a + bX + cY ) = a + bE(X) + cE(Y ). 6. If X and Y are discrete random variables, how are these proofs changed? Exercise 1.15
Let X and Y be random variables with finite covariance, and let a and b be finite
constants. From the definition of covariance, prove the result
Cov(a + bY, c + dZ) = bdCov(Y, Z)
in (1.11). Exercise 1.16
Let X and Y be random variables with finite variances, and let a, b, and c be finite
constants. Starting from the definition of variance, i.e., Var(X) = E(X 2 ) âˆ’ (E(X))2 ,
prove the following results. (Hint: The definition of variance is in terms of expectations; use the results of Exercise 1.14.)
1. Var(a + X) = Var(X). 2. Var(bX) = b2 Var(X). 3. Var(a + bX) = b2 Var(X). 4. Var(X + Y ) = Var(X) + Var(Y ) + 2Cov(X, Y ). 5. Var(a + bX + cY ) = b2 Var(X) + c2 Var(Y ) + 2bcCov(X, Y ).(This is special case
of the more general result in Section 1.7.7.)

Â© Copyright William J.Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-50

CHAPTER 1. PROBABILITY TOOLS

Exercise 1.17
Let Y1 , . . . , Yn be independent random variables, each taking the values 0 or 1 with
probabilities 1 âˆ’ Ï€ and Ï€, respectively. Here Ï€, the probability that Y = 1, is an
unknown parameter to be estimated. 1. Show that E(Yi ) = Ï€ and Var(Yi ) = Ï€(1 âˆ’ Ï€). âˆ‘
2. Consider the estimator Ï€Ìƒ = n1 ni=1 Yi of Ï€. (This is simply the proportion of
1â€™s amongst Y1 , .. . , Yn . It is a random variable because the Yi are random.)
(a) Show that E(Ï€Ìƒ) = Ï€, i.e., Ï€Ìƒ is an unbiased estimator of Ï€. (b) Show that Var(Ï€Ìƒ) = Ï€(1 âˆ’ Ï€)/n. Exercise 1.18
[Quiz #1, 2009-10, Term 1] Let B be a Bernoulli random variable taking values
b = 0, 1. Its PMF is given by fB (0) = Pr(B = 0) = 1âˆ’Ï€ and fB (1) = Pr(B = 1) = Ï€. Thus, B âˆ¼ Bern (Ï€). Show each of the following results. For full marks you need to be explicit about the
mathematical definition of the quantity involved (E(), Var() or MGF) and how the
definition is used for this specific problem. 1. Show E(B) = Ï€. 2. Find E(10B ). 3. Show Var(B) = Ï€(1 âˆ’ Ï€). 4. Show that the moment generating function (MGF) of B is MB (t) = 1 âˆ’ Ï€ + Ï€et . 5. Check that the MGF exists for t in an open neighbourhood of zero. 6. Use the MGF to find E(B). Exercise 1.19
[Quiz #1, 2009-10, Term 1] Let Y = B1 + Â· Â· Â· + Bn , where the random variables
B1 , . . . , Bn are independent and each has a Bern (Ï€) distribution. You may use the
results in Exercise 1.18 that Bi has mean Ï€, variance Ï€(1 âˆ’ Ï€), and MGF 1 âˆ’ Ï€ + Ï€et . Also n is some fixed number. 1. Find E(Y ). 2. Find Var(Y ). 3. Find the moment generating function of Y . 4. Hence, what is the distribution of Y ? Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1.11. EXERCISES

1-51

Exercise 1.20
Let Y âˆ¼ Pois (Âµ). Thus, the PMF of Y is
fY (y) =

eâˆ’Âµ Âµy
y!(y = 0, 1, .. . , âˆ; Âµ > 0). 1. Show that Y has the MGF
MY (t) = eÂµ(e âˆ’1) .t

2. Let Y1 , . . . , Yn be independent Poisson random variables, where Yi has mean Âµi ,
i.e., Yi âˆ¼ Pois (Âµi ). Thus, the random variables may have different
âˆ‘nmeans and
are not necessarily identically distributed. What is the MGF of i=1 Yi ? âˆ‘
3. Hence, what is the distribution of ni=1 Yi ? Exercise 1.21
Let Y âˆ¼ Unif (a, b). Use the expansion of its MGF in Example 1.29 to show the
following properties:
1. E(Y ) = (a + b)/2; and
2. Var(Y ) = (b âˆ’ a)2 /12. Exercise 1.22
Let Y âˆ¼ Geom1 (Ï€). Thus, the PMF of Y is
fY (y) = (1 âˆ’ Ï€)yâˆ’1 Ï€

(y = 1, 2, . . . , âˆ; 0 < Ï€ < 1). 1. Show that Y has the MGF
MY (t) =

et Ï€
. 1 âˆ’ (1 âˆ’ Ï€)et

2. From the MGF show that
E(Y ) =

1
Ï€

and Var(Y ) =

1âˆ’Ï€
. Ï€2

3. âˆ‘
Let Y1 , . . . , Yn be IID Geom1 (Ï€) random variables. n
i=1 Yi ? âˆ‘
4. Hence, what is the distribution of ni=1 Yi ? What is the MGF of

Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-52

CHAPTER 1. PROBABILITY TOOLS

Exercise 1.23
Let Y âˆ¼ N (Âµ, Ïƒ 2 ), i.e., the normal distribution with mean Âµ and variance Ïƒ 2 . This
exercise shows in two ways that the MGF of Y is
1

2 2

MY (t) = eÂµt+ 2 Ïƒ t . 1. Apply the definition of the MGF in Definition 1.7 directly to the N (Âµ, Ïƒ 2 ) PDF
to find the MGF of Y . 2. Let Z have a standard normal distribution, i.e., N (0, 1). Its MGF is
1 2

MZ (t) = e 2 t
(see Example 1.24). Now let Y = Âµ + ÏƒZ. (a) Verify that E(Y ) = Âµ and Var(Y ) = Ïƒ 2 as required. (b) Find the MGF of Y from the MGF of Z. Exercise 1.24
Let Y âˆ¼ N (Âµ, Ïƒ 2 ). Starting from the MGF of Y , i.e.,
1

2 2

MY (t) = eÂµt+ 2 Ïƒ t ,
this exercise verifies the first two moments. 1. Use the MGF to show that E(Y ) = Âµ. 2. Use the MGF to show that E(Y 2 ) = Âµ2 + Ïƒ 2 , and hence that Var(Y ) = Ïƒ 2 . Exercise 1.25
Let Y âˆ¼ Expon (Î»). Consider multiplying Y by a constant to give a new random
variable, Z = bY , where b > 0. 1. Table 1.4 says the MGF of Y is Î»/(Î» âˆ’ t). What is the MGF of Z? 2. What is the distribution of Z? 3. Apply the same argument to the gamma distribution to show that if Y âˆ¼
Gamma (Î½, Î»), then Z = bY âˆ¼ Gamma (Î½, Î»/b). Exercise 1.26
A random variable, Y , taking positive values is said to have a log-normal distribution
if Z = ln(Y ) has a N (Âµ, Ïƒ 2 ) distribution. This exercise finds the expectation of Y
from the MGF of Z. 1. The definition of the MGF of Z is E(etZ ). What expression do we get if we put
t = 1 in this definition? 2. Look up the MGF of Z (see Table 1.4 or Exercise 1.23), and put t = 1 in it. Hence, what is E(Y )?Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2-1

Chapter 2
The Normal Distribution in
Statistics
2.1

Introduction

The normal distribution, sometimes called the Gaussian distribution after Gauss, is
ubiquitous in statistical analysis. It will arise in this book in several ways, including
the following. â€¢ Based on a random sample from a N (Âµ, Ïƒ 2 ) distribution, the sample mean is
often used to estimate Âµ. Exact properties of the normal distribution lead
to a confidence interval for Âµ that accounts for the uncertainty in estimation,
even if Ïƒ 2 is unknown. This analysis based on the t distribution is common in
applications, as typified by Example 2.1. â€¢ If a random sample is taken from a distribution with mean Âµ and variance Ïƒ 2 ,
but the distribution is only approximately normal, use of the t distribution to
provide a confidence interval for Âµ is often still approximately valid. â€¢ For a large sample size, the normal distribution serves as an approximation to
other distributions. For instance, the binomial distribution is a commonly used
model for applications where estimating a population proportion is the focus. The error in using the sample proportion as an estimate is again quantified in a
confidence interval, this time based on a normal approximation to the binomial
distribution. More generally, under certain conditions, sample means, proportions, and totals have approximate normal distributions for a large enough
sample size via the central limit theorem (Section 2.5.3). â€¢ The method of maximum likelihood in Chapter 4 is a powerful generic method
to estimate parameters for a wide range of probability models. An approximate
confidence interval for the parameter is often available from an approximate
normal distribution.Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2-2

2.2

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

Some Properties of the Normal Distribution

Let Y be a normal random variable with mean Âµ and variance Ïƒ 2 . We write Y âˆ¼
N (Âµ, Ïƒ 2 ). The following properties were established using MGFs in Section 1.8 or in
related exercises. â€¢ The expected value (mean) of Y is E(Y ) = Âµ, and the variance is Var(Y ) = Ïƒ 2
(Exercise 1.24). â€¢ A linear function of Y is also normal: a+bY âˆ¼ N (a + bÂµ, b2 Ïƒ 2 ) (Example 1.30). In particular,
Y âˆ’Âµ
Âµ 1
Z=
= âˆ’ + Y âˆ¼ N (0, 1)
Ïƒ
Ïƒ Ïƒ
has the standard normal distribution N (0, 1), i.e., with mean 0 and variance 1
(Exercise 2.1). Also, for n normally distributed random variables, we have the following properties. â€¢ If the n random variables are independent (but not necessarily identically distributed), Example 1.32 showed that a linear combination of them also has a
normal distribution. â€¢ If the n random variables are correlated but follow a multivariate normal distribution, a linear combination of them still has a normal distribution. The
covariances between the n variables affect only the variance of the linear combination, via (1.10). â€¢ If the n random variables follow a multivariate normal distribution and all the
pairwise covariances between them are zero, then they are mutually independent. This result is a generalization of Lemma 1.1, which said that if Y and Z
are bivariate normal with Cov(Y, Z) = 0, then Y and Z are independent. 2.3

Distributions Derived From the Normal

The normal distribution is important in itself and because other important distributions arise from it. The relationships among these distributions are summarized in
Figure 2.1. We next give some details about these distributions. 2.3.1

The Ï‡2 distribution

A Ï‡2 (â€œchi-squaredâ€) random variable is generated by the sum of squares of independent standard normal random variables.Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2.3. DISTRIBUTIONS DERIVED FROM THE NORMAL

2-3

Standard normal distribution
Z âˆ¼ N (0, 1)
Z1 , . . . , Zd âˆ¼ independent N (0, 1)

SSS
SSS
SSS
SSS
SS)

t distribution
If Z and Xd are independent
âˆš Z âˆ¼ td



5
lll
lll
l
l
lll
lll

Xd /d

Ï‡2 (Chi-squared) distribution
Z 2 âˆ¼ Ï‡21
2
Xd = Z1 + Â· Â· Â· + Zd2 âˆ¼ Ï‡2d


F distribution
Xd1 âˆ¼ Ï‡2d1
Xd2 âˆ¼ Ï‡2d2
If Xd1 and Xd2 are independent
Xd1 /d1
âˆ¼ Fd1 ,d2
Xd2 /d2

Figure 2.1: Relationships between distributions derived from the normal

Â© Copyright William J.Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS
1.2

2-4

0.0

0.2

0.4

f(y)
0.6

0.8

1.0

1 df
3 df
5 df
10 df

0

5

10
y

15

20

Figure 2.2: PDF of the Ï‡2 distribution with 1, 3, 5, and 10 degrees of freedom
Lemma 2.1 (Ï‡2 distribution)
If Z1 , . . . , Zd are independent N (0, 1) random variables, then their sum of
squares,
Xd = Z12 + Â· Â· Â· + Zd2 âˆ¼ Ï‡2d ,
has a Ï‡2 distribution with d degrees of freedom, which we write as Ï‡2d . A proof is developed in Exercise 2.6. Figure 2.2 plots the PDF of the Ï‡2d distribution for d = 1, 3, 5, and 10. With d = 1
or d = 2 degrees of freedom, the PDF is monotonic decreasing. For d â‰¥ 3, the PDF
increases then decreases. For large d the shape of the distribution is approximately
normal; this is starting to be evident for d = 10 in Figure 2.2. The limiting normality
stems from the central limit theorem (Theorem 2.2), because of the sum in Lemma 2.1. The Ï‡2 distribution is related to the gamma distribution. Let G be a
Gamma (Î½ = d/2, Î» = 1) random variable (see Table 1.4), where d > 0 is an integer. Then X = 2G is a Ï‡2d random variable. The Ï‡2 distribution arises in inference about the variance of a normal distribution in
Section 2.4.2. It is also heavily used for hypothesis testing in Chapters 7â€“9. 2.3.2

The t distribution

The following lemma is due to W.S. Gosset, who wrote under the pseudonym â€œStudentâ€. Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2-5

0.4

2.3. DISTRIBUTIONS DERIVED FROM THE NORMAL

0.0

0.1

f(y)
0.2

0.3

1 df
3 df
10 df
Normal

âˆ’6

âˆ’4

âˆ’2

0
y

2

4

6

Figure 2.3: PDF of the t distribution with 1, 3, 10, and âˆ degrees of freedom; âˆ
degrees of freedom gives the standard normal
Lemma 2.2 (t distribution (Student, 1908))
If Z âˆ¼ N (0, 1), Xd âˆ¼ Ï‡2d , and Z and Xd are independent, then
Z
âˆš
âˆ¼ td ,
Xd /d
where td is Studentâ€™s t distribution with d degrees of freedom. Gossetâ€™s motivation for the lemma was the distribution of the sample mean of n IID
N (Âµ, Ïƒ 2 ) standardized using the sample variance as an estimator of Ïƒ 2 , leading to
confidence intervals and hypothesis tests for Âµ based upon the t distribution (Section 2.4.3). Contrary to Gossetâ€™s modest dismissal of his contribution in comments
made to R.A. Fisher, the result is one of the most commonly applied in all of the
statistical sciences. Figure 2.3 plots the td PDF for d = 1, 3, and 10. The PDF has a bell shape similar to
the normal, but has wider tails than the normal. As d â†’ âˆ, the td PDF approaches
the standard normal PDF. With 1 degree of freedom, the distribution is known as
the Cauchy distribution. âˆš
An insightful way to prove Lemma 2.2 is to first write Z/ Xd /d in the lemma as a
mixture of normal random variables. Define
Z
Xd
Z
=âˆš ,
W =
and T = âˆš
d
W
Xd /d
where T is the variable of interest and W appears in its denominator. (We write T
following our convention of using upper case letters for random variables, whereas
Â© Copyright William J.Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2-6

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

many textbooks use t for the random variable and T for Hotellingâ€™s T 2 statistic.)
Their joint distribution can be written in terms of the conditional distribution of T
given W :
fT,W (t, w) = fT |W (t | w)fW (w)
(see Section 1.7.2). The key simplification here is that conditional on W taking the
value w, the variable T of interest becomes
(
)
Z
1
âˆš âˆ¼ N 0,
,
w
w
i.e., fT |W (t | w) is simply the normal PDF with variance 1/w or âˆš
standard deviation
âˆš
1/ w. Here we are using the assumed independence of Z and W in the lemma,
so that Z still has a normal distribution conditional on Wâˆš. Once we condition on
W = w, there is a constant, not random, divisor in Z/ w, i.e., a simple linear
transformation of a normal random variable, which is also normal. Similarly, W is a
trivial rescaling of a Ï‡2d random variable, and fW (w) is easily derived. The marginal
distribution of T then follows by integrating out W from the joint distribution (see
Section 1.7.1):
âˆ« âˆ
âˆ« âˆ
fT (t) =
fT,W (t, w) dw =
fT |W (t | w)fW (w) dw. 0

0

Written this way, the td PDF is an average or mixture of normal PDFs, averaged with
respect to the distribution of W . The second step is to carry out the integration. Readers interested in the formal details can find them in the Appendix of Section 2.9, but it is perhaps more instructive
to demonstrate the averaging of normal random variables graphically. For definiteness, take d = 3 degrees of freedom. Furthermore, we will approximate the continuous
values of Xd=3 and hence W by just five representative values. Figure 2.4(a) shows
the Ï‡23 PDF of X3 . The distribution is divided into five equal probabilities by the
dotted lines. These sub-intervals in the figure are represented by the numbers 0.6,
1.4, etc. For instance, the value 0.6 cuts off a probability of 0.1 to the left, and in this
sense it is in the centre of the first interval of probability 0.2. The five representative
values are obtained in R using the function qchisq. > # Quantiles of X_3 cutting off probs 0.1 , 0.3 , etc. to the left
> x3 <- qchisq (c(0.1 , 0.3 , 0.5 , 0.7 , 0.9) , df = 3)
> x3
[1] 0.5843744 1.4236522 2.3659739 3.6648708 6.2513886

As W = X3 /3 is a monotonic increasing function of X3 , we have 0.1 = Pr(X3 <
0.5843744) = Pr(X3 /3 < 0.5843744/3), etc., and the five representative values of W
are found by simple arithmetic. > # Quantiles of W cutting off probs 0.1 , 0.3 , etc. to the left
> w <- x3 / 3
> w
[1] 0.1947915 0.4745507 0.7886580 1.2216236 2.0837962
Â© Copyright William J.Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2.3. DISTRIBUTIONS DERIVED FROM THE NORMAL

(b) t distribution with 3 df as mixture of normals

f(y)
0.3
0.6

0

1.4

2.4

2

3.7

0.0

0.00

0.1

0.05

0.2

0.10

f(y)

0.15

0.4

0.5

0.20

0.6

0.25

(a) Chi squared distribution with 3 df

2-7

6.3

4

6

8

âˆ’4

âˆ’2

y

0
y

2

4

Figure 2.4: (a) Ï‡23 PDF, with the distribution divided into five subintervals of probability 0.2 each by the dashed lines. Each subinterval is represented by a single value. (b) The t3 PDF shown as a solid line is to a good approximation given by the average
of the five normal PDFs shown as dashed lines. âˆš
Finally, each representative value, w, leads to a standard deviation of 1/ w in the
normal distribution. > # Standard deviation of conditional normal
> sd.norm <- 1/ sqrt(w)
> sd.norm
[1] 2.2657659 1.4516391 1.1260448 0.9047556 0.6927434

Figure 2.4(b) shows five normal PDFs as dashed lines with these five standard deviations. Note that the conditional normal distributions have considerable variation in
their standard deviations. Visually averaging these five PDFs gives a curve that is
hard to distinguish from the t3 PDF shown by a solid line in the figure. The t distribution will arise when we make inference about the mean, Âµ, of a normal
distribution and the variance, Ïƒ 2 , also has to be estimated. 2.3.3

The F distribution

The F distribution arises from the ratio of two independent Ï‡2 distributions.Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

0.8

1.0

2-8

0.0

0.2

0.4

f(y)

0.6

(2, 2) df
(2, 10) df
(3, 2) df
(3, 10) df

0

1

2

3
y

4

5

6

Figure 2.5: PDF of the F distribution with (2, 2) (2, 10), (3, 2), and (3, 10) degrees
of freedom
Lemma 2.3 (F distribution)
If Xd1 âˆ¼ Ï‡2d1 , Xd2 âˆ¼ Ï‡2d2 , and Xd1 and Xd2 are independent, then
Xd1 /d1
âˆ¼ Fd1 ,d2 ,
Xd2 /d2
where Fd1 ,d2 is an F distribution with d1 and d2 degrees of freedom. Figure 2.5 plots the PDF of Fd1 ,d2 for various values of d1 and d2 . With d1 = 1
or 2, the PDF is monotonic decreasing. For d1 > 2 the distribution increases then
decreases. The F distribution is used to make inference when comparing two variances. It is
also very important in inference about the parameters of linear regression models
(STAT 306). 2.4

Estimating the Parameters of the Normal Distribution

Let Y1 , . . . , Yn be independent N (Âµ, Ïƒ 2 ) random variables. Such variables would arise
if each is an independent draw from the same normal distribution. Equivalent to this
probability model, a statistician might say Y1 , . . . , Yn are assumed to be a random
sample of size n from an N (Âµ, Ïƒ 2 ) distribution. Here, the statistician has in mind an
infiniteâ€”in practice, largeâ€”population of values taken to be normal.Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2.4. ESTIMATING THE PARAMETERS OF THE NORMAL

2-9

Typically, both Âµ and Ïƒ 2 are unknown, and these quantities are estimated by the
sample mean and sample variance, respectively. 2.4.1

Distribution of the sample mean (known variance)

Suppose we use the sample mean to estimate Âµ. What are its statistical properties? The sample mean is

1âˆ‘
YÌ„ =
Yi . n i=1
n

Clearly, YÌ„ is a linear combination of random variables. Hence,
n
n
1âˆ‘
1âˆ‘
E(YÌ„ ) =
E(Yi ) =
Âµ=Âµ
n i=1
n i=1

and

1 âˆ‘
1 âˆ‘ 2 Ïƒ2
Var(Yi ) = 2
Ïƒ = . Var(YÌ„ ) = 2
n i=1
n i=1
n
n

n

The distribution of YÌ„ will be exactly normal under the assumptions we are making
or often approximately normal:
â€¢ The assumptions at the beginning of Section 2.4 included normality of Y1 , .. . ,
Yn . As a linear combination of normal random variables is normal (Example 1.32), YÌ„ is also normal. We can summarize by saying
(
)
YÌ„ âˆ¼ N Âµ, Ïƒ 2 /n ,
or equivalently if we standardize YÌ„ for its mean and variance,
YÌ„ âˆ’ Âµ
âˆš
âˆ¼ N (0, 1). Ïƒ 2 /n

(2.1)

â€¢ Even if Y1 , . . . , Yn are not normal, the CLT will often apply. The left-hand side
of (2.1) is the same as the sample-mean version of the CLT in (2.10). Thus,
if the conditions of the CLT hold, the left-hand side of (2.1) will converge in
distribution to N (0, 1) as n â†’ âˆ even if the Yi are not normal. If we know Ïƒ 2 we are ready for statistical inference (confidence intervals, hypothesis
tests) for Âµ based on the normal distribution. Unfortunately, in practice, Ïƒ 2 is also
usually unknown and we need to estimate it by the sample variance. Â© Copyright William J. Welch 2009â€“2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

2-10

CHAPTER 2. THE NORMAL DISTRIBUTION IN STATISTICS

2.4.2

Distribution of the sample variance

The sample variance is
n
1 âˆ‘
1
2
(Yi âˆ’ YÌ„ )2 =
S =
n âˆ’ 1 i=1
nâˆ’1

( n
âˆ‘

)
Yi2 âˆ’ nYÌ„ 2

. (2.2)

i=1

The equivalence of the two formulas for computing S 2 is seen by multiplying out the
square in the first formula and collecting terms. It is fairly straightforward to show that dividing by n âˆ’ 1 (and not n) makes S 2 an
unbiased estimator of Ïƒ 2 , i.e.,
E(S 2 ) = Ïƒ 2 .This is proved as Exercise 2.9. The proof only requires that Y1 , . . . , Yn are independent
with mean Âµ and variance Ïƒ 2 , i.e., it does not depend on having a normal distribution. Finding the distribution of S 2 is more challenging in general but approachable when
the Yi are IID normal. In the first definition of S 2 in (2.2), write
(
)
Yi âˆ’ Âµ YÌ„ âˆ’ Âµ
Yi âˆ’ YÌ„ = Ïƒ
âˆ’
= Ïƒ(Zi âˆ’ ZÌ„),
Ïƒ
Ïƒ
i.e., standardize Yi to Zi with a standard normal distribution. Then
S2 =

n
n
1 âˆ‘
1 âˆ‘
(Yi âˆ’ YÌ„ )2 = Ïƒ 2
(Zi âˆ’ ZÌ„)2 ,
n âˆ’ 1 i=1
n âˆ’ 1 i=1

or

n
1 âˆ‘
S2
=
(Zi âˆ’ ZÌ„)2 . Ïƒ2
n âˆ’ 1 i=1
âˆ‘
Thus, the distribution of S 2 is that of ni=1 (Zi âˆ’ ZÌ„)2 up to constants. âˆ‘
The following theorem shows that the distribution of ni=1 (Zi âˆ’ ZÌ„)2 is Ï‡2nâˆ’1 , i.e., the
degrees of freedom are nâˆ’1 and not n. Each observation is â€œcorrectedâ€ for the sample
mean, i.e., Yi âˆ’ YÌ„ or Zi âˆ’ ZÌ„. This is because the mean, Âµ, of the normal distribution
is unknown and has to be estimated; the estimation of this one parameter leads to a
correction of the degrees of freedom by 1. Theorem 2.1 (Ï‡2 corrected degrees of freedom)
Let Z1 , . . . , Zn be independent N (0, 1) random variables. Then
n
âˆ‘

(Zi âˆ’ ZÌ„)2 âˆ¼ Ï‡2nâˆ’1 ,

i=1

i.e., Ï‡2 with n âˆ’ 1 degrees of freedom.