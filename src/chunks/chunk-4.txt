Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-28

CHAPTER 1. PROBABILITY TOOLS
Grade
(g)
55.5
69.0
72.0
85.5

fG (g)
0.1
0.1
0.2
0.6

Table 1.7: Probability mass function for the course grade of a randomly chosen student in a given STAT 305 section
takes the value 55.5 if Y is 60 and Z is 50, with joint probability 0.1 from Table 1.6. From fG (g) it is easily verified that E(G) = 78.15. Computing the expectation of
G by first computing fG (g) would be a very tedious numerical problem, however,
if many random variables were being combined to form G. Alternatively, applying (1.9) which says that the expectation of a linear combination of random variables is the linear combination of their expectations, we
have
E(G) = E(0.55Y + 0.45Z) = 0.55E(Y ) + 0.45E(Z) = 78.15. This calculation requires only the expectations of the marginal distributions; it
does not require any other properties of the joint distribution. ♢♢♢

1.7.7

Variance of a linear combination of random variables

Providing all variances exist,
(
)
n
n
n ∑
n
∑
∑
∑
2
Var a0 +
ai Yi =
ai Var(Yi ) + 2
ai aj Cov(Yi , Yj ). i=1

i=1

(1.10)

i=1 j=i+1

A simpler version of this result with n = 2 random variables is proved in Exercise 1.16. Special cases of the result include:
Var(Y + Z) = Var(Y ) + Var(Z) + 2Cov(Y, Z)
Var(Y − Z) = Var(Y ) + Var(Z) − 2Cov(Y, Z)
Var(a + bY ) = b2 Var(Y ). When using the general rule or special cases in deriving another result, it is helpful
to explain the step in the argument by a statement such as “using the result on the
variance of a linear combination of random variables”. Example 1.20 (Course grade: variance of a linear combination)
The distribution of grades in Table 1.7 gives Var(G) = 99.65. Alternatively,
from (1.10) we find
Var(G) = (.55)2 Var(Y ) + (.45)2 Var(Z) + 2(.55)(.45)Cov(Y, Z) = 99.65.© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. ♢♢♢
2019.8.14

1.7. SEVERAL VARIABLES

1-29

Example 1.21 (Gamma distribution: mean and variance)
The gamma distribution (see Table 1.4) has PDF
fY (y) =

1
λ(λy)ν−1 e−λy
Γ(ν)

(0 < y < ∞; ν > 0; λ > 0),

which we write as Gamma (ν, λ). It has a similar form to the exponential distribution, for which we have already found the mean and variance, and a similar
approach could be used again. With a slight loss of generality we can find the gamma distribution’s mean and
variance rather more simply, however. If the parameter ν is an integer greater
than or equal to 1 (this is the loss of generality), then a Gamma (ν, λ) random
variable Y can be generated by:
Y = Y1 + · · · + Yν ,
where the Yi are independent Expon (λ) random variables. (This result will
be proved in Example 1.31.) We know Yi has mean 1/λ and variance 1/λ2 . Hence it immediately follows that a Gamma (ν, λ) random variable has mean ν/λ
(from (1.9)) and variance ν/λ2 (from (1.10)). Note that because the Yi are independent, all covariance terms in the variance calculation are zero. This result
actually holds for general ν > 0. ♢♢♢
Independence can be an important assumption in formal statistical models and derivations. For instance, the result (1.10) on the variance of a linear combination of random variables is applied to derive the variance of a sample mean or sample proportion
from independent observations Y1 , . . . , Yn (e.g., Exercise 1.17). But simple results are
only obtained when all distinct pairs of observations are independent and hence all
Cov(Yi , Yj ) terms for i ̸= j are all zero. If the assumption of independence is false,
the claimed variance of the sample mean or proportion could be highly misleading. Furthermore, the assumption of independence is usually made out of necessity. To
take account of covariance terms between any two observations in the calculation of
the variance of a linear combination, one needs some insight into the structure of the
covariance, insight which is often lacking. In practice, appealing to the way the data
were collected—as a random sample or via randomization in an experiment—is the
only feasible justification of an independence assumption. 1.7.8

Covariance between linear functions or combinations of
random variables

There are analogous results for the covariance between linear functions of random
variables:
Cov(a + bY, c + dZ) = bdCov(Y, Z)
(1.11)
This is proved as Exercise 1.15.© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-30

CHAPTER 1. PROBABILITY TOOLS

Similarly, for linear combinations of random variables,
( n
)
m
n ∑
m
∑
∑
∑
Cov
ai Y i ,
bj Zj =
ai bj Cov(Yi , Zj ). i=1

j=1

i=1 j=1

Note that in general the two linear combinations can have different numbers of random
variables, n and m, respectively. When they involve the same random variables, i.e.,
with m = n and Yi = Zi for i = 1, . . . , n, we have
( n
)
n
n ∑
n
∑
∑
∑
Cov
ai Y i ,
bj Y j =
ai bj Cov(Yi , Yj ). i=1

1.7.9

j=1

i=1 j=1

Bivariate normal distribution

Two continuous random variables Y1 and Y2 with a bivariate normal distribution have
joint PDF given by
(
)
1
1
T −1
exp − (y − µ) Σ (y − µ) ,
fY1 ,Y2 (y1 , y2 ) =
1
2
2π det 2 (Σ)
(

where
y=

y1
y2

)

(
,

µ=

µ1
µ2

)

(
,

Σ=

ρσ1 σ2
σ12
ρσ1 σ2
σ22

)
,

µ1 and µ2 are the means of Y1 and Y2 , respectively, σ1 > 0 and σ2 > 0 are the standard
deviations of Y1 and Y2 , respectively, −1 < ρ < 1 is the correlation between Y1 and
Y2 , and det(Σ) and Σ−1 denote matrix determinant and inverse of Σ, respectively. The off-diagonal element ρσ1 σ2 in the covariance matrix Σ is the covariance between
Y1 and Y2 . It is zero if Y1 and Y2 are uncorrelated, i.e., if ρ = 0. The bivariate normal has the special property that a covariance of zero between the
two random variables implies they are independent. Lemma 1.1 (Bivariate normal: covariance of 0 implies independence)
If Y1 and Y2 have a joint bivariate normal distribution and their covariance
(correlation) is zero, then Y1 and Y2 are independent normal random variables. To show the result, assume ρ = 0. Independence will follow by showing that the joint
distribution factorizes. First, we have
( 2
)
σ1 0
det(Σ) = det
= σ12 σ22 ,
0 σ22
1

and hence det 2 (Σ) = σ1 σ2 . Second,

(

σ12 0
(y − µ) Σ (y − µ) = (y1 − µ1 , y2 − µ2 )
0 σ22
(y1 − µ1 )2 (y2 − µ2 )2
=
+
. σ12
σ22
T

−1

)−1 (

y1 − µ1
y2 − µ2

© Copyright William J.Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. )

2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-31

Substituting these two results into the joint distribution gives
(
(
))
1
1 (y1 − µ1 )2 (y2 − µ2 )2
fY1 ,Y2 (y1 , y2 ) =
exp −
+
2πσ1 σ2
2
σ12
σ22
(
)
(
)
1
1
1
1
2
2
=√
exp − 2 (y1 − µ1 ) × √
exp − 2 (y2 − µ2 ) ,
2σ1
2σ2
2πσ1
2πσ2
which is the product of a N (µ1 , σ12 ) PDF for Y1 and a N (µ2 , σ22 ) PDF for Y2 . Hence
Y1 and Y2 are independent by Definition 1.5. Note that independence implies covariance of zero for any two random variables (see
Section 1.7.5), including the bivariate normal. But the result that covariance of zero
implies independence does not hold in general. The same arguments apply to the multivariate normal with n variables: if all pairwise
covariances are zero, the n random variables are mutually independent and normal. 1.8

Moment Generating Functions

1.8.1

Uses of moment generating functions

The moment generating function (MGF) is a powerful tool for proving probability
results essential for statistical methods. • We can sometimes find the distribution of a sum of IID random variables from
the MGF of the underlying distribution. This is clearly useful for statistical
properties of sample totals or sample means, which are sums. For instance:
– Example 1.31 establishes that the sum of IID exponential random variables
has a gamma distribution, a result used for statistical hypothesis testing
in Example 7.3. – Exercise 1.20 shows that a sum of independent Poisson random variables
has a Poisson distribution. This is again used in hypothesis testing, in
Exercise 7.2. – The sum of IID geometric random variables has a negative-binomial distribution (Example 1.33). • If we know the MGF of a random variable, it is easy to write down the MGF
of any linear function of it. This provides an easy proof that a linear function
of a normal random variable also has a normal distribution (Example 1.30), an
important property. • Using the MGF is a relatively easy way of establishing approximate normality
of a sample mean or sample total under certain conditions (the central limit
theorem of Theorem 2.2) and special cases like the approximation of a binomial
© Copyright William J.Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-32

CHAPTER 1. PROBABILITY TOOLS
distribution by a normal distribution (Example 2.2). Normal approximations
are widely used in statistical inference. • The properties of the χ2 distribution and hence the sample variance when
sampling from a normal distribution are readily shown using MGFs (in Section 2.4.2). 1.8.2 Definition of the moment generating function
As its name suggests, the moment generating function generates the moments of a
distribution or random variable. Definition 1.6 (Moments of a random variable)
Let Y be a random variable. Its kth moment for k = 1, 2, . . . is E(Y k ), which
exists if the expectation is finite. Thus, the first moment with k = 1 is simply E(Y ). The first two moments, E(Y 1 ) and
E(Y 2 ), give the variance from Var(Y ) = E(Y 2 ) − (E(Y ))2 . The MGF, once found,
can generate all the moments of a random variable, including these two. The MGF is found by computing an expectation. Definition 1.7 (Moment generating function)
Let Y be a random variable. The moment generating function (MGF) for Y
is defined as
MY (t) = EY (etY ),
if it exists for t in a neighbourhood of 0, i.e., for t in the open interval
−T < t < T , where T > 0. Note that the expectation is with respect to the distribution of Y , and is just the
expectation of a function of Y , namely etY . The parameter t is a dummy variable. The MGF has to exist in an interval around t = 0 because manipulations of it will
involve the derivatives at t = 0 and Taylor series approximation at t = 0. Example 1.22 (Exponential distribution: MGF)
Let Y be distributed Expon (λ). As this is a continuous random variable, we
compute the expectation in the MGF via integration:
∫ ∞
∫ ∞
∫ ∞
tY
ty
ty
−λy
MY (t) = E(e ) =
e fY (y) dy =
e λe
dy =
λe−(λ−t)y dy. 0

0

0

The integrand converges and the MGF exists if λ − t > 0. Carrying out the integration is straightforward here, but this simple example is
an opportunity to show a method that avoids explicit integration in more difficult
cases. With the condition λ − t > 0, we can rewrite the integral as
∫ ∞
λ
(λ − t)e−(λ−t)y dy. λ−t 0
© Copyright William J.Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-33

The integrand is now the PDF of an exponential random variable with parameter
λ − t, and like any PDF it must integrate to 1. Thus, the MGF of the Expon (λ)
distribution is
λ
MY (t) =
,
(1.12)
λ−t
which exists for t < λ. We also note that λ > 0 (see Table 1.4), so the interval
t < λ includes an open interval around t = 0, as required by Definition 1.7. ♢♢♢
Example 1.23 (Gamma distribution: MGF)
From Table 1.4, the PDF of Y ∼ Gamma (ν, λ) is
fY (y) =

1
λ(λy)ν−1 e−λy . Γ(ν)

First, we apply the definition of the MGF and simplify a little:
∫ ∞
∫ ∞
1
tY
ty
λ(λy)ν−1 e−λy dy
MY (t) = E(e ) =
e fY (y) dy =
ety
Γ(ν)
0
0
∫ ∞
1
=
λ(λy)ν−1 e−(λ−t)y dy,
Γ(ν)
0
which exists if t < λ. The integrand is of the form y a eby , and we note that a is
not necessarily an integer. (From Table 1.4, the parameter ν takes values ν > 0
and hence a = ν − 1 > −1.) There are many ways to proceed:
• Use standard methods of calculus. • Look up a table of integrals. • Use software such as Mathematica or Maple. • Note that the integrand is very similar to the form of the original gamma
PDF and again use the fact that a PDF integrates to 1. The last route turns out to be easy. All we need to do is take out a factor:
(
)ν ∫ ∞
∫ ∞
1
λ
1
ν−1 −(λ−t)y
λ(λy) e
dy =
(λ − t)((λ − t)y)ν−1 e−(λ−t)y dy. Γ(ν)
λ−t
Γ(ν)
0
0
The integrand is now the PDF of a Gamma (ν, λ − t) random variable, i.e., with
the parameter λ replaced by λ − t everywhere, and the integral is 1. We are
left with just the factor in front of the integral, and the MGF of a Gamma (ν, λ)
random variable is
(
)ν
λ
MY (t) =
. (1.13)
λ−t
It exists for t < λ and hence in an open interval around t = 0, because again
λ > 0.♢♢♢
© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-34

CHAPTER 1. PROBABILITY TOOLS

Example 1.24 (Standard normal distribution: MGF)
Let Z ∼ N (0, 1), i.e., the standard normal distribution with mean µ = 0 and
variance σ 2 = 1. Substituting these parameter values into the general normal
PDF in Table 1.4 gives
1 2
1
fZ (z) = √ e− 2 z . 2π
Thus,
∫ ∞
∫ ∞
∫ ∞
1 2
1
− 12 z 2
tz
tz 1
√ e− 2 (z −2tz) dz
dz =
MZ (t) =
e fZ (z) dz =
e √ e
2π
2π
−∞
−∞
∫ ∞
∫−∞
∞
1
1 2
1
1 − 1 ((z−t)2 −t2 )
2
√ e 2
√ e− 2 (z−t) dz
=
dz = e 2 t
2π
2π
−∞
−∞
1 2

= e2t . The last integral is 1, because we see that the integrand is a normal PDF (with
1 2
µ = t and σ 2 = 1). We also note that e 2 t and hence MZ (t) exist for −∞ < t <
∞. ♢♢♢
The method used in Example 1.24 can also be applied to find the MGF of Y ∼
N (µ, σ 2 ), i.e., a normal random variable with arbitrary mean and variance. The
MGF of Y is
1 2 2
(1.14)
MY (t) = eµt+ 2 σ t . The details are left to Exercise 1.23. Example 1.25 (Binomial distribution: MGF)
The binomial distribution has PMF
( )
n y
fY (y) =
π (1 − π)n−y (y = 0, 1, ... , n). y
Because it takes discrete values, the MGF is found by summation:
( )
n y
MY (t) = E(e ) =
e fY (y) =
e
π (1 − π)n−y . y
y=0
y=0
tY

n
∑

ty

n
∑

ty

Minor simplification is possible by collecting together the ety and π y terms, and
the task is to evaluate
n ( )
∑
n
MY (t) =
(πet )y (1 − π)n−y . (1.15)
y
y=0
For this discrete problem we next try to turn the sum into a sum of a PMF over
its possible values. The expression to evaluate in (1.15) looks like the binomial
PMF, but a binomial PMF involves complementary probabilities, π and 1 − π,
© Copyright William J.Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-35

which sum to 1. In (1.15) πet and 1 − π do not sum to 1 for t ̸= 0, but this is
easily fixed by dividing them by their sum to create
π̇ =

πet
1 − π + πet

and 1 − π̇ =

1−π
. 1 − π + πet

The divisor is cancelled by a factor outside the sum when we rewrite (1.15) as
n ( )
∑
n y
t n
MY (t) = (1 − π + πe )
π̇ (1 − π̇)n−y . y
y=0
Here the sum is over the possible values of a Bin (n, π̇) random variable, and the
sum must be 1. Thus, the MGF of the binomial distribution is
MY (t) = (1 − π + πet )n . It exists for −∞ < t < ∞. 1.8.3

♢♢♢

Finding moments from the MGF

As its name suggests, the MGF for a distribution generates the moments, E(Y k ). The
first moment is E(Y ), the mean of Y . The second moment is E(Y 2 ); from it and the
first moment, we can compute the variance, Var(Y ) = E(Y 2 ) − (E(Y ))2 . Similarly,
skewness, etc., can be computed from higher-order moments. We will prove the result relating the MGF to the moments using a Taylor series
expansion around t = 0. This explains the mysterious condition in the definition of
the MGF that it needs to exist for t in an open interval around 0. Suppose, MY (t) exists in a neighbourhood of t = 0. Then,
(k)

MY (0) = E(Y k ),

(1.16)

(k)

where MY (0) is MY (t) differentiated k times and evaluated at t = 0. To show this
we make a Taylor series expansion of etY in the definition of the MGF:
)
(
t2 Y 2 t3 Y 3
tY
+
+ ···
(1.17)
MY (t) = E(e ) = E 1 + tY +
2! 3! t2
t3
2
= 1 + tE(Y ) + E(Y ) + E(Y 3 ) + · · · . (1.18)
2! 3! Differentiating once with respect to t gives
(1)

MY (t) = E(Y ) +

2t
3t2
E(Y 2 ) +
E(Y 3 ) + · · · ,
2! 3! and evaluating at t = 0 gives M (1) (0) = E(Y ). Similarly, differentiating twice gives
(2)

MY (t) = E(Y 2 ) +

6tY 3
+ ··· ,
3!© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-36

CHAPTER 1. PROBABILITY TOOLS
(2)

and MY (0) = E(Y 2 ). The general result in (1.16) for E(Y k ) is just a continuation of
this process. The proof given here makes it obvious how the Taylor-series expansion
of etY generates powers of Y and hence the moments after taking expectation. Example 1.26 (Exponential distribution: mean and variance via the MGF)
The first and second derivatives of the exponential distribution’s MGF in (1.12)
are
(
)2
λ
1
(1)
MY (t) =
λ λ−t
and
(2)
MY (t) =

2
λ2

(

λ
λ−t

)3
. (1)

Putting t = 0 in the first expression gives E(Y ) = MY (0) = 1/λ. Similarly, t = 0
(2)
in the second expression gives E(Y 2 ) = MY (0) = 2/λ2 , and hence Var(Y ) =
E(Y 2 ) − (E(Y ))2 = 1/λ2 . ♢♢♢
We can compute the mean and variance of the exponential distribution directly (Exercise 1.7), so what is gained by use of the MGF in Example 1.26? The direct attack
on the mean and variance involves moderately complicated integrals. In contrast the
integration to find the MGF of the exponential distribution was straightforward. After some minor algebra, we recognized the integral of a PDF, which we know must be
1. Differentiating the MGF to get the moments was also easy. So we replaced nontrivial integrations with algebra and differentiation. (For a discrete random variable,
potentially nontrivial summations are similarly avoided.)
Example 1.27 (Gamma distribution: mean and variance via the MGF)
First, rewrite the MGF of the gamma distribution in (1.13) as
(
)ν (
)ν
λ
1
MY (t) =
=
. λ−t
1 − t/λ
(Application of the chain rule is then a little easier.)
The first derivative of MY (t) is
(
(1)
MY (t) = −ν

1
1 − t/λ

)ν+1 (
)
(
)ν+1
1
1
ν
−
=
,
λ
λ 1 − t/λ

which equals ν/λ at t = 0. Therefore,
E(Y ) =

ν
. λ

The second derivative is
ν
(2)
MY (t) = (−(ν + 1))
λ

(

1
1 − t/λ

)ν+2 (

1
−
λ

)

ν(ν + 1)
=
λ2

(

1
1 − t/λ

© Copyright William J.Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. )ν+2
,
2019.8.14

1.8. MOMENT GENERATING FUNCTIONS

1-37

which evaluates to ν(ν + 1)/λ2 at t = 0. This is E(Y 2 ). Therefore,
ν
ν(ν + 1) ( ν )2
Var(Y ) = E(Y 2 ) − (E(Y ))2 =
−
= 2. 2
λ
λ
λ
The same expectation and variance were found in Example 1.21, but the result
here holds for any ν > 0 and not just for positive integer values of ν. ♢♢♢
Example 1.28 (Binomial distribution: mean and variance via the MGF)
From Example 1.25 the MGF of the binomial distribution is
MY (t) = (1 − π + πet )n . The first two derivatives of the MGF are
(1)

MY (t) = nπet (1 − π + πet )n−1
and
(2)

MY (t) = nπet (1 − π + πet )n−1 + nπet (n − 1)πet (1 − π + πet )n−2 . Evaluating these derivatives at t = 0 gives
E(Y ) = nπ
and
E(Y 2 ) = nπ + n(n − 1)π 2 . Therefore,
Var(Y ) = E(Y 2 ) − (E(Y ))2 = nπ + n(n − 1)π 2 − (nπ)2 = nπ(1 − π). ♢♢♢

The argument to obtain the moments from the MGF hinges on the Taylor series
expansion (1.18) at t = 0. Thus, the MGF has to exist at t = 0, which was straightforward to demonstrate for the examples up to here. The next example is a little
more subtle. Example 1.29 (Uniform distribution: existence of the MGF)
If Y ∼ Unif (a, b), Table 1.4 says its MGF is
MY (t) =

ebt − eat
(b − a)t

(−∞ < t < ∞),

i.e., the table claims the MGF exists for all t including t = 0. The appearance of t in the denominator may create some doubt about the existence, but note that the numerator is also 0 at t = 0. Expanding the exponential
functions, however, shows that all is well, however:
1 + bt + (bt)2 /(2!) + (bt)3 /(3!) + · · · − (1 + at + (at)2 /(2!) + (at)3 /(3!) + · · · )
(b − a)t
2
2 2
3
3 3
(b − a)t + (b − a )t /2 + (b − a )t /6 + · · ·
=
(b − a)t
=1 + (a + b)t/2 + (a2 + b2 + ab)t2 /6 + · · · ,

MY (t) =

© Copyright William J. Welch 2009–2019. All rights reserved. Not to be copied, used, or revised without explicit written permission from the copyright owner. 2019.8.14

1-38

CHAPTER 1. PROBABILITY TOOLS

which equals 1 at t = 0. (Simplifying the t3 term uses (b3 − a3 ) = (b − a)(a2 +
b2 + ab).)
Hence, the first two moments give the expectation and variance of Y in Table 1.4
(Exercise 1.21). ♢♢♢

1.8.4

MGF of a linear function or a sum

Lemma 1.2 (MGF of a linear function of a random variable)
If the MGF of Y is MY (t), then Z = a + bY has MGF
MZ (t) = eat MY (bt). The result follows from
MZ (t) = EZ (etZ ) = EY (et(a+bY ) ) = eat EY (ebtY ) = eat MY (bt). We next derive a result useful for a sum of independent random variables. Lemma 1.3 (MGF of a sum of independent random variables)
Suppose Y1 , . . . , Yn are independent random variables, and Yi has MGF MYi (t)
(which must exist). Then the MGF of X = Y1 + · · · + Yn is
MX (t) =

n
∏

MYi (t). i=1

The result follows from
tX

t

MX (t) = EX (e ) = EY1 ,...,Yn (e

∑n

i=1 Yi

) = EY1 ,...,Yn

( n
∏
i=1

=

n
∏

)
e

tYi

=

n
∏

EYi (etYi )

i=1

MYi (t). i=1

Here, we are using the result that the expectation of a product of independent random
variables is the product of expectations. 1.8.5

The MGF identifies a distribution

An important property of the MGF is that it identifies a distribution uniquely. Theorem 1.3 (The MGF identifies a distribution)
Let Y and Z be two random variables with MGFs MY (t) and MZ (t), respectively. If MY (t) = MZ (t) for all t in an open interval of 0, then
Pr(Y ≤ y) = Pr(Z ≤ y), i.e., Y and Z have the same CDF.