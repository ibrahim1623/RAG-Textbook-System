Here is the provided text converted to LaTeX format, effectively marking the equations:

```latex
\section{1.1.1}

\subsection{Probability mass function and probability density function}

The distribution of \( Y \) over its possible values is denoted by \( f_Y(y) \). For a discrete random variable, \( f_Y(y) \) can be interpreted as \( \Pr(Y = y) \), the probability that \( Y \) takes the value \( y \), and \( f_Y(y) \) is called a probability mass function (PMF). The mass function is positive and sums to 1 over the possible \( y \) values.

\subsubsection*{Example 1.1 (Poisson PMF)}
If \( Y \) has a Poisson distribution, it has possible values \( y = 0, 1, \ldots, \infty \) and PMF

\[
f_Y(y) = \frac{e^{-\mu} \mu^y}{y!}.
\]

The Poisson distribution is actually a family of distributions depending on the value of the parameter \( \mu > 0 \), and we will use the notation \( \text{Pois}(\mu) \) to denote the family. (The properties of the Poisson and other commonly used distributions will be summarized in Sections 1.4 and 1.5.) In practice, the value of \( \mu \) is usually unknown for a specific application, and much of our statistical work will be about how to estimate the values of parameters like \( \mu \) from a sample of data.

The Poisson PMF sums to 1, as required:

\[
\sum_{y=0}^{\infty} f_Y(y) = \sum_{y=0}^{\infty} \frac{e^{-\mu} \mu^y}{y!} = e^{-\mu} \sum_{y=0}^{\infty} \frac{\mu^y}{y!} = e^{-\mu} e^{\mu} = 1,
\]

because of the series representation \( 1 + \frac{\mu}{1!} + \frac{\mu^2}{2!} + \cdots \) for \( e^{\mu} \).

\(\blacksquare\)

\subsection*{(The end of an example is marked by a \(\blacksquare\) symbol.)}

For a continuous random variable, \( f_Y(y) \) is called a probability density function (PDF), and \( f_Y(y) \) cannot be interpreted as a probability. It is, however, proportional to the probability that \( Y \) falls in a small interval around \( y \) (Exercise 1.1). The density function is positive and integrates to 1 over the range of possible \( y \) values.

\subsubsection*{Example 1.2 (Exponential distribution: PDF)}
If \( Y \) has an exponential distribution, it has possible values \( 0 < y < \infty \) and PDF

\[
f_Y(y) = \lambda e^{-\lambda y} \quad (0 < y < \infty; \, \lambda > 0).
\]

The distribution, denoted \( \text{Expon}(\lambda) \), depends on the parameter \( \lambda > 0 \). The exponential PDF integrates to 1, as required:

\[
\int_0^{\infty} f_Y(y) \, dy = \int_0^{\infty} \lambda e^{-\lambda y} \, dy = -e^{-\lambda y} \bigg|_{y=0}^{y=\infty} = 0 - (-1) = 1.
\]

\(\blacksquare\)

A distribution is symmetric if there exists \( \mu \) such that its PMF or PDF can be written

\[
f_Y(\mu - x) = f_Y(\mu + x),
\]

for values \( x \) that generate all possible values \( y \).

\section{1.1. DISCRETE AND CONTINUOUS RANDOM VARIABLES}

\subsubsection*{Example 1.3 (Normal distribution: symmetry)}
A random variable with a normal distribution has PDF

\[
f_Y(y) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(y - \mu)^2}{2\sigma^2}}
\]

over possible values \( -\infty < y < \infty \), for given constants \( \mu \) and \( \sigma^2 > 0 \). The PDF satisfies

\[
f_Y(\mu - x) = f_Y(\mu + x) \quad (0 \leq x < \infty)
\]

and hence is symmetric around \( \mu \) for any value of \( \sigma^2 \).
```

This LaTeX representation maintains the structure of the original text while correctly formatting mathematical equations and symbols.